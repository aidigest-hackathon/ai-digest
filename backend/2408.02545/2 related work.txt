2 Related Work
There are numerous open-source tools related to
the different aspects of RAG, namely inference,
training and evaluation. LlamaIndex (Liu, 2022),
LangChain (Chase, 2022) and Haystack (Pietsch
et al., 2019) are well known libraries for composing
RAG pipelines; however they are not focused on
evaluation and their training capability is under-
developed.
Hoshi et al. (2023) proposes a framework for
developing RAG-based LLMs; while our process-
ing may be similar in the sense of being comprised
of custom individual steps, they do not introduce
any form of training. Khattab et al. (2023, 2022)
presents a different approach, where LLM prompt-
ing is represented as a programming language, to
be optimized and compiled; a rather unique and
general approach that could benefit RAG but has
a high level of complexity due to the abstractions
introduced. Saad-Falcon et al. (2024) focuses more
on the evaluation aspect, by creating synthetic data
and training an LLM critic to evaluate the RAG sys-
tem. Hsia et al. (2024) studies aspects of retrieval
on the performance of RAG; our RAG Foundry li-
brary is general and enables experimentation on all
aspects of RAG: retrieval, text-processing, prompt
design, model selection, inference and evaluations.
Recently, a concurrent work by Jin et al. (2024)
proposes a RAG building framework, including
some RAG implementations and datasets; we fo-
cus on extensibility, letting users define custom
types of pipelines with custom components. Rau
et al. (2024) presents a framework, sharing a
similar design-principle of extensibility-through-
configuration as ours; their library imposes a spe-
cific workflow structure (retriever, ranker, LLM)
while our library is more general and does not im-
poses any specific paradigm.