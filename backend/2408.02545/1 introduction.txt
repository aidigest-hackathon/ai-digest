1 Introduction
Large Language Models (LLMs) have emerged as
a transformative force in the field of AI, demon-
strating an impressive ability to perform a wide
range of tasks that traditionally required human in-
telligence (Brown et al., 2020; Kojima et al., 2022).
Despite their impressive capabilities, LLMs have
inherent limitations. These models can produce
plausible-sounding but incorrect or nonsensical an-
swers, struggle with factual accuracy, lack access
to up-to-date information after their training cutoff
and struggle in attending to relevant information in
large contexts (Huang et al., 2023; Liu et al., 2023).
Data
TrainingLoRA
Inference
LoadersAugmentation
SelectorsRetrieversSamplersPromptersCachingAPI
EvaluationEMF1FaithfulnessRelevancyAnswer ProcessorROUGE
Figure 1: An overview of the RAG F OUNDRY frame-
work: the Data Augmentation module persists RAG
interactions into a dedicated dataset, which is then used
for training, inference and evaluation.
Retrieval-Augmented Generation (RAG) enhances
LLMs performance by integrating external infor-
mation using retrieval mechanisms. Combining re-
trieval that leverages vast knowledge-bases outside
theknowledge of the model, effectively addresses
knowledge limitations, can reduce hallucinations,
improve the relevance of generated content, pro-
vide interpretability and could be vastly more cost-
efficient (Lewis et al., 2021; Mallen et al., 2022;
Gao et al., 2023; Asai et al., 2023; Borgeaud et al.,
2021; Peng et al., 2023; de Jong et al., 2023). Fur-
thermore, recent research indicates that fine-tuning
LLMs for RAG can achieve state-of-the-art perfor-
mance, surpassing that of larger, proprietary mod-
els (Yu et al., 2024b; Liu et al., 2024).
However, the implementation of RAG systems
is inherently complex and requires a series of
intricate decisions that can significantly impact
the performance of the system. This process de-arXiv:2408.02545v1  [cs.CL]  5 Aug 2024mands a thorough understanding of the data and
use case, and often, solutions do not generalize
well to other domains (Barnett et al., 2024; Bala-
guer et al., 2024). Some key RAG design decisions
include text embedding, indexing parameters, re-
trieval algorithms, query building, and prompt de-
sign, among other considerations beyond the LLM
configuration (Wang et al., 2024). Another issue is
reproducibility: achieving consistent and compara-
ble results across runs, datasets and tasks. Varia-
tions in training data, pre-processing steps, model
configurations, and hardware can lead to discrep-
ancies in performance, making it challenging for
researchers and practitioners to replicate findings
and build upon previous work. Additionally, evalu-
ating RAG systems presents a challenge due to the
dual reliance on retrieval accuracy and generative
quality. These systems require a sophisticated eval-
uation suite that accounts for the interplay among
the retrieved information, the formalization of data,
and the generated output (Chen et al., 2023; Yu
et al., 2024a; Es et al., 2024).
We introduce RAG F OUNDRY , an open-source
python framework for developing sophisticated
retrieval-augmented LLMs for RAG use-cases. The
library supports researchers and practitioners in the
nuanced task of enhancing the capabilities of LLMs
in RAG use cases. It is highly customizable, fa-
cilitating rapid prototyping and experimentation
across all aspects of RAG, including data selec-
tion, aggregation and filtering, retrieval, text pro-
cessing, document ranking, few-shot generation,
prompt design using templates, fine-tuning, infer-
ence, and evaluation. To cater to the specific needs
of researchers, we designed the framework to func-
tion as an end-to-end experimentation environment.
The backbone of the library consists of four dis-
tinct modules: data creation, training, inference,
and evaluation. Each module is encapsulated and
controlled by a configuration file, ensuring compat-
ibility between the output of one module and the
input of the next. This modular approach allows
each step to be isolated and independently experi-
mented with, enabling the production of multiple
outputs and the concurrent execution of numerous
experiments. Evaluation can be conducted on the
generated outputs as well as on any feature within
the data, including retrieval, ranking, and reason-
ing.
To illustrate the utility of the framework, we
conducted experiments involving retrieval, fine-
tuning, chain-of-thought (CoT) reasoning (Wuet al., 2023) and a negative distractor-documents
technique (Zhang et al., 2024). We compared
two widely accepted baseline models using vari-
ous enhancement methods across three knowledge-
intensive question-answering tasks, demonstrating
the effectiveness of RAG F OUNDRY .