{
    "research_paper": [
        "3 Arctic-TILT\nOur starting point is the TILT encoder-decoder\nmodel, built upon T5 (Raffel et al., 2020) by (1)\nextending its sequential positional bias with an at-\ntention bias based on relative horizontal and ver-\ntical distances between each pair of tokens, and\n(2) adding contextualized image embeddings that\ncover the token image region semantics in the con-\ntext of its entire visual neighborhood (Powalski\net al., 2021).\nWe further enhance TILT\u2019s performance and re-\nmove limitations. Specifically, we propose novel\nmodality fusion, introduce attention sparsity, en-\nhance training recipe, and optimize training andinference (Table 1). Improved variant of the model\nis referred to as the Arctic-TILT.\n3.1 Fusion of Text and Vision\nThe TILT model\u2019s unique approach to combin-\ning visual and textual semantics involves summing\nword embeddings and RoI-pooled representations\nof the word\u2019s bounding box with a variant of the U-\nNet network used as an image encoder. Visual and\ntextual features are integrated once, immediately\nafter embedding both inputs.\nWe begin by replacing the post-embedding sum-\nmation of text and vision modalities from the origi-\nnal TILT with our proposed mechanism, integrated\nwithin each transformer block.\nFusion by Tensor Product. In contrast to Powal-\nski et al. (2021), we opt for the fusion of modal-\nities inspired by tensor product representations\n(Smolensky, 1990; Schmidhuber, 1993) and their\nHadamard approximation (Schlag et al., 2019).\nSpecifically, given the text and image embeddings\nt, i\u2208Rd, we calculate the fused embedding with:\nFuse( t, i) =O(V(t+i)\u2299(1+Rt)) +t\nwhere V,R, andOareRd\u00d7dtrainable parameters.\nIn practice, we use a variant of this mechanism with\nadditional layer norm and dropout, as depicted in\nFigure 2 and Listing 1.\nModule placement. Having described the mech-\nanism of integrating two modalities together, the\nquestion arises on the positioning of the fusion\nmodule within the transformer block.\nWe found that placing the fusion module after\nFFNs is most beneficial as the fusion results are di-\nrectly fed to the next layer (Figure 3). Additionally,\nby applying fusion after every encoder layer, we\nmitigate the vanishing gradient effect and enable\nthe model to focus on different visual features as\nits comprehension of the document improves.\n3.2 Long Context Support\nConcerning the product-oriented nature of our\nwork, it is essential to cover a significant fraction\nof real-world documents of potentially arbitrary\nlengths while operating within limited resources.\nThe outlined optimizations are guided by the\nneed to handle as much context as possible on\nwidely available A10 and L4 GPUs equipped with\n24GB vRAM. We assume a single-GPU setup and\nmeasure the impact of applied techniques and ar-\nchitectural changes on the maximum context length\nContextualized\nVision\nMulti-Head\nAttention1D + 2D\nBiasesFeed\nForward\nTextual\nSemanticsF\nNxDecoderFigure 3: The Arctic-TILT encoder block combines Con-\ntextualized Vision from U-Net and Textual Semantics\nfrom input embeddings through Fusion (F) operation.\nTheMulti-Head Attention is augmented with 1D and\n2D positional biases to capture spatial and sequential\narrangement. This procedure is repeated in each layer\n(Nx), allowing to process integrated information further.\nused during the finetuning and inference under this\nmemory budget.\nChunked processing. To address the quadratic\ncomplexity of self-attention computation in the\nencoder, we employ a variant of fusion-in-\ndecoder (de Jong et al., 2023), also known as\nblockwise encoding (Pietruszka et al., 2022) or\nSLED (Ivgi et al., 2022) with zero chunk padding.\nThis method restricts the encoder attention ma-\ntrix to a neighborhood around its diagonal with\na bounded width. Without padding, this results\nin a block diagonal matrix, reducing the number\nof non-zero attention weights to a linear quantity\nrelative to the input sequence length.\nSpecifically, for a fixed core chunk length c, and\noverlap size o, being hyperparameters of our model,\nprefix of length land total input size C=n\u00b7c,\nwe build a sequence of chunks in the following\nmanner: chunk number 1is filled with ltokens of\nprefix, and with tokens 0, .., c\u2212lof the input. Say\nchunk number ialready used input tokens of up to\nt. Chunk number i+1starts with ltokens of prefix,\nfollowed with tokens t\u2212o+ 1, t\u2212o+ 2, ..., t\u2212\no+c\u2212lof the input. In practice, we found a core\nchunk length of 1024 and no overlap to performInput  lengthTarget\nlengthChunk  lengthEncoder(A) TIL T (B) Arctic-TIL T\nFusion\nDecoderFigure 4: An illustration of sparse attention matrices\nassuming a two-layer encoder and decoder. The origi-\nnal TILT (A) consumes the complete input at once, in\ncontrast to Arctic-TILT (B) with blockwise attention\nrobustly across most tasks (see Appendix C).\nThe resulting sequences are passed through the\nencoder separately. The encoder outputs are then\nrecombined by removing the prefix tokens embed-\ndings from all but the first chunk and concatenating\nthe results. The resulting encoder output is passed\nto the decoder (see Figure 4).\nSince the underlying attention implementation is\nalready memory efficient, this technique improves\nthe computational efficiency in training and infer-\nence, resulting in a 10-fold increase in input length\nduring training and inference.\nNested stack checkpointing. Implementing gra-\ndient checkpointing over the entire 24-layer en-\ncoder stack reduces the memory required for acti-\nvations. Only the last layer\u2019s activations are stored,\nwhich are necessary for the decoder. Consequently,\nmemory savings are significant here, as the require-\nment for processing 1M tokens reduced from 96GB\nto merely 4GB for the encoder part, albeit at the\ncost of an additional forward pass through the en-\ncoder. This improvement allowed to quadruple the\ninput length for training.Random chunks. While our modifications effec-\ntively handle the encoder\u2019s memory limitations, the\nlength of concatenated chunk embeddings can still\ncause the decoder cross-attention to exceed avail-\nable memory. Technically, the model can handle\n230k tokens during the training, which was further\naddressed with a simple method that allows for ex-\ntending the length of the document while also posi-\ntively impacting the scores. We randomly discard\nchunks, effectively exposing the model to the differ-\nent parts of longer documents across epochs. The\nfirst chunk, typically containing the initial pages, is\nalways preserved to provide essential context.\nIn addition to primary techniques, we employ\nseveral other optimizations. Specifically, we use\nmixed precision training with bfloat16 while turn-\ning off weight caching to save RAM, leading to\na 2\u00d7improvement in inference input length. Sec-\nondly, by recomputing projections for each decoder\nlayer instead of using the key-value cache, we ex-\ntend the maximum inference context to 389k to-\nkens. Next, we optimize training by offloading the\ndecoder\u2019s activations needed for backpropagation\nfrom GPU to CPU, minimizing peak memory usage\nof the GPU by increasing processing time. Finally,\nimplementing memory-efficient attention reduces\nthe memory overhead of the attention mechanism\n(Rabe and Staats, 2022).\nUltimately, our optimizations culminate in signifi-\ncant memory usage improvements, allowing us to\neffectively train and deploy Arctic-TILT for doc-\numents up to 500 pages1on a single 24GB GPU.\nThe step-by-step summary is studied in Table 2.\n3.3 Pretraining and Finetuning\nThe training process began with a self-supervised\npretraining phase using the pretrained T5 large\nmodel (Raffel et al., 2020). Following the introduc-\ntion of TILT architecture changes, which included\nU-Net (Ronneberger et al., 2015) and 2D biases,\nas well as text-vision post-fusion, the model un-\nderwent further self-supervised pretraining for a\ntotal of 900k steps based on documents from the\nCCpdf (Turski et al., 2022) and OCR-IDL (Biten\net al., 2022). These two large-scale, publicly avail-\nable PDF resources come with OCR results from\nTesseract and Amazon Textract, respectively.\n1Specifically, 390k input tokens with an output of 128\ntokens, corresponding to 780 tokens per page on average.Inference Training\nVanilla TILT 9k 4k\n+ attention sparsity 87k 41k\n+ mixed precision 179k 51k\n+ memory efficient attention 183k 56k\nInference-only optimizations\n+ no cross-attention KV cache 389k\nTraining-only optimizations\n+ nested checkpointing 230k\n+ CPU offloading 256k\n+ random chunks 389k\nTable 2: Max input length (tokens) consumed during\ntraining and inference given single 24GB GPU. Tested\nfor documents up to 500 pages (389k tokens).\nFinally, the model was fine-tuned on QA and\nKIE datasets. In this phase, we increase the number\nof supervised datasets to 17, compared to TILT\u2019s\noriginal choice of four. The datasets chosen rep-\nresent critical aspects of DU tasks, including, but\nnot limited to, forms, financial reports, charts, in-\nvoices, insurance documents, contracts, and legal\ndocuments (detailed in Appendix B).",
        "5 Summary\nWe have introduced the Arctic-TILT model, which\naddresses TILT\u2019s limitations in handling multi-\nmodal input, suboptimal training procedure, and\nmaximum context length. By analyzing the results\nand considering the cost-efficiency of the designed\nsolution, we provided practical insights into design-\ning capable, lightweight models for the industry. In\nparticular, we:\n\u2022established state-of-the-art performance on seven\nbenchmarks demanding text, vision, and layout\ncomprehension;\n\u2022demonstrated that within the industrial applica-\ntions setting and while keeping the parameter\ncount below 1B, one could achieve performance\nbetter or comparable to vastly larger LLMs and\nLVLMs;\n\u2022presented a novel modality fusion mechanism\ninspired by tensor product representations, and\nhave shown how effectively apply it across the\ntransformer encoder;\n\u2022demonstrated how, with well-designed attention\nsparsity patterns and numerous other optimiza-\ntions, consume extensive input sequences dur-\ning training and inference, given a single cost-\nefficient GPU, while maintaining competitive ac-\ncuracy of the model;\n\u2022provided insights that can be applied to design\nfuture generations of multimodal models, partic-\nularly for visually-rich document processing.\nOur work illustrates that strategic design and opti-\nmization can rival the capabilities of larger, more\nresource-intensive models.\nAcknowledgements\nWe express our sincere gratitude to Tomasz Dwo-\njak and Daniel Campos for their feedback on the\nmanuscript and suggestions that have greatly en-\nhanced the quality of this work. We also extend our\nthanks to \u0141ukasz S\u0142abinski, Micha\u0142 Gdak, Tomasz\nthe average target length of evaluation datasets mentioned in\nSection 4.1.Stanis\u0142awek, Nikolai Scholz, and Vivek Raghu-\nnathan, whose support and guidance as managers\nhave been helpful throughout this research. Finally,\nwe thank Rafa\u0142 Kobiela for his assistance with the\ncloud infrastructure.",
        "The vast portion of workloads employing\nLLMs involves answering questions grounded\non PDF or scan content. We introduce the\nArctic-TILT achieving accuracy on par with\nmodels 1000 \u00d7its size on these use cases. It can\nbe fine-tuned and deployed on a single 24GB\nGPU, lowering operational costs while process-\ning Visually Rich Documents with up to 400k\ntokens. The model establishes state-of-the-art\nresults on seven diverse Document Understand-\ning benchmarks, as well as provides reliable\nconfidence scores and quick inference, which\nare essential for processing files in large-scale\nor time-sensitive enterprise environments.\nSNOWFLAKE CORPORATE GOOGLE SLIDES THEME 2024 v1.1\n\u00a9 2024 Snowflake Inc. All Rights ReservedAnswer: How many attorneys are listed for the plaintiffs?Fill out the template with the amount, date, and IBAN.2Answer QuestionsExtract Key InformationSummarize ContentFinetune ModelArctic-TILTArctic-TILTSoftwareHardware Semi-Conductors\nGet crucial terms of the non-disclosure agreement.Learn new scenarios or increase accuracy on existing ones.SNOWFLAKE CORPORATE GOOGLE SLIDES THEME 2024 v1.1\n\u00a9 2024 Snowflake Inc. All Rights ReservedAnswer: How many attorneys are listed for the plaintiffs?Fill out the template with the amount, date, and IBAN.2Answer QuestionsExtract Key InformationSummarize ContentFinetune ModelArctic-TILTArctic-TILTSoftwareHardware Semi-Conductors\nGet crucial terms of the non-disclosure agreement.Learn new scenarios or increase accuracy on existing ones.\nSNOWFLAKE CORPORATE GOOGLE SLIDES THEME 2024 v1.1\n\u00a9 2024 Snowflake Inc. All Rights ReservedAnswer: How many attorneys are listed for the plaintiffs?Fill out the template with the amount, date, and IBAN.2Answer QuestionsExtract Key InformationSummarize ContentFinetune ModelArctic-TILTArctic-TILTSoftware\nGet crucial terms of the non-disclosure agreement.Learn new scenarios or increase accuracy on existing ones.Arctic-TILT24GB GPU500 PAGES\nFigure 1: Arctic-TILT consumes long, richly formatted\nPDFs given a single, cost-efficient GPU and can produce\ntheir summary, answer questions, and extract values,\noutperforming vastly heavier LLMs and LVLMs.\n\u2217See Appendix E for contributions.1 Introduction\nGeneral-purpose LLMs and their multi-modal\ncounterparts provide a crucial advantage in pro-\ncess automation: they can be applied immediately,\neliminating the expensive and time-consuming ef-\nforts of creating dedicated system architecture\nand model development. Though they are suit-\nable choices for prototyping and building proof-\nof-concept solutions, once the case is validated,\nit becomes essential to consider the demands of\nreal-world deployments, such as cost-efficiency (Fu\net al., 2024; Ong et al., 2024), fine-tunability (Liu\net al., 2022), and ensuring accurate confidence cali-\nbration (Van Landeghem, 2024).\nWe consider these issues in the context of Docu-\nment Understanding (DU), where it is commonly\nrequired to integrate textual, layout and graphical\nclues to obtain the required information and intro-\nduce the Arctic-TILT, designed to address the needs\nof broad-use deployments, cost efficiency, and do-\nmain adaptations for a fraction of the cost of the\nleading models. The proposed solution achieves\nstate-of-the-art accuracy on business and long doc-\nument benchmarks of MP-DocVQA (Tito et al.,\n2023), DUDE (Van Landeghem et al., 2023), Kleis-\nter NDA and Charity (Stanislawek et al., 2021),\nArXiv-Lay and PubMed-Lay (Nguyen et al., 2023),\nand remains competitive with orders of magnitude\nlarger models on other document VQA datasets.",
        "4 Experiments\nWe start by examining diverse benchmarks in the\nDocument Understanding field, focusing on these\ncloser to enterprise applications due to the domain\nof the included documents and the presence of\nmulti-page inputs that systems encounter in real-\nworld applications (see Table 3).\nHyperparameters used during finetuning were\nsubject to optimization outlined in Appendix C.\n4.1 Document Visual QA and KIE\nRegarding the model\u2019s intended use for processing\nunstructured documents, Document Visual Ques-\ntion Answering and Key Information Extraction\ntasks appear best suited for assessing performance.\nMulti-page. Arctic-TILT excels when input is a\nlong, business document. In the case of DUDE\n(Van Landeghem et al., 2023) consisting of PDFs\nof up to 25 pages, sourced across industries, we\noutperform GPT-4 Vision Turbo by 4 points and\nGemini 1.5 Pro by 12 points. Qualitatively, Arctic-\nTILT not only outperforms GPT-4Vt in handling\nnon-answerable questions but also exceeds state-of-\nthe-art models in list and abstractive analysis tasks,\nan example showing complex data handling skills.\nSimilarly, we establish state-of-the-art perfor-\nmance on MP-DocVQA (Tito et al., 2023) dataset\nTable 1PageArctic-TILT (MP)GRAM (MP)Arctic-TILT (SlideVQA)Arctic-TILT (MMLong)32K + OCR1 - 577,178,257,226,66 - 1072,974,455,321,411 - 1569,769,652,618,916 - 2072,677,250,322,221 - 3016,531 - 4014,241 - 5018,4-5013,3MethodANLSAccuracyPage 0Page 1TILT \ufb01netuned optuna 30,812250,78700,86390,7967GRAM0,803219,98410,83800,7854\n1-526.66-1021.411-1518.916-2022.221-3016.531-4014.241-5018.450-1000013.3Page\n020406080\n1-56-1011-1516-\n72,669,772,977,1MP-DocVQA\n020406080\n1-56-1011-1516-\n50,352,655,357,2SlideVQA\n0102030\n1-56-1011-1516-2021-3031-4041-5051-\n13,318,414,216,522,218,921,426,6MMLongBench-Doc\n2Figure 5: Scores of Arctic-TILT on MP-DocVQA, Slide-\nVQA, and MMLongBench-Doc, depending on the evi-\ndence location (buckets of five pages).\nconsisting of questions posed to industrial docu-\nments of up to 20 pages, outperforming GRAM\n(Blau et al., 2024) by 1 point. Further, we were\nable to surpass the long-standing score of LAM-\nBERT (Garncarek et al., 2021) on Kleister Charity\n(Stanislawek et al., 2021), with documents reach-\ning above 300 pages, by 4 points. Similarly, we\noutperform ERNIE-Layout (Peng et al., 2022) on\nKleister NDA by 6 points.\nConcerning SlideVQA (Tanaka et al., 2023) that\nis based on visually rich presentations of up to 20\nslides, we obtain 2 points less than GPT-4 Vision.\nOn recently introduced MMLongBench-Doc (Ma\net al., 2024) that evaluates zero-shot performance\non documents as long as 400 pages, we outperform\nvastly larger LLMs: Mixtral 8x7B by 8 points,\nQWen-Plus by 6 points, and LVLMs: Claude-3\nOpus by 9 points, InternVL by 11 points. Better\nperformance was attained by models such as Gem-\nini 1.5 Pro and GPT-4 Vision Omnia, which are\nbelieved to have hundreds of times more parame-\nters. Whereas it was the only task considered in\nzero-shot setup, please note Section 4.3 studies how\nthe performance of our model improves compared\nto GPT-4o given several annotated documents.\nFinally, given that three of datasets considered\nunder this category contain labeled positions of\nanswers within documents, we can investigate how\nthe model\u2019s performance changes depending on the\nevidence location. The results shown in Figure 5\nindicate the primacy bias , with the highest scores\nachieved when relevant information appears at theDataset Industrial Multipage State-of-the-Art Arctic-TILT\nMP-DocVQA \u2713 \u2713 GRAM 80.3 81.2\nKleister Charity \u2713 \u2713 LAMBERT 83.6 88.1\nKleister NDA \u2713 \u2713 ERNIE-Layout 88.1 94.3\nDUDE \u2713/\u2717 \u2713 GPT-4Vt + OCR 53.9 58.1\nMMLongBench-Doc\u2020\u2713/\u2717 \u2713 GPT-4o 42.8 25.8\nSlideVQA \u2717 \u2713 GPT-4Vt + OCR 57.3 55.1\nArXiv-Lay \u2717 \u2713 BigBird-Pegasus+Layout 41.2 44.4\nPubMed-Lay \u2717 \u2713 BigBird-Pegasus+Layout 42.1 44.8\nDocVQA \u2713 \u2717 InternVL 2.0 Pro 95.1 90.2\nVQA-CD \u2713 \u2717 QALayout 42.5 90.7\nInfographicsVQA \u2717 \u2717 InternVL 2.0 Pro 86.8 57.0\nTable 3: Arctic-TILT compared to the previous state-of-the-art. Our model remains competitive despite having\nless than 1B parameters and excels when input is a long, business document. We use the original metrics for each\ndataset, i.e., F1 for Kleisters, Accuracy for MMLongBench-Doc, EM for SlideVQA, ROUGE-L for ArXiv-Lay and\nPubMed-Lay, and ANLS for the remaining tasks;\u2020denotes zero-shot evaluation.\nbeginning of the input (Liu et al., 2024).\nSingle-page. In benchmarks involving single-\npage excerpts from multi-page documents or stan-\ndalone images with limited input length, our\nmodel shows promising results. While Arcitc-\nTILT improved by 2 points over TILT on DocVQA\n(Mathew et al., 2021b) and outperformed GPT-\n4V , it particularly excels in the newly introduced\nVQA-CD dataset, which includes invoices and pur-\nchase orders, establishing state-of-the-art results\n(Souleiman Mahamoud et al., 2022).\nAlthough there is still a gap compared to\n102B InternVL 2.0 Pro\u2019s performance (Chen\net al., 2024), especially in non-business Infograph-\nicsVQA (Mathew et al., 2021a), our achievements\nhighlight significant advancements in handling\nmulti-modal inputs.\n4.2 Layout-Aware Summarization\nTo supplement VQA and KIE results, we examine\nhow Arctic-TILT exploits layout information and\ncaptures long-range dependencies in the LoRaLay\ncollection of summarization tasks where, in con-\ntrast to the majority of similar datasets, input is\nnot plain text but a scientific document with rich\nstructure (Nguyen et al., 2023).\nResults presented in Table 3 show that even\nthough, in contrast to the previous SOTA, we had\nno pretraining objective explicitly designed for the\nsummarization task, we could outperform the best\nmodel by a few points on both ArXiv-Lay and\nPubMed-Lay.\n0102030\n1-56-1011-1516-2021-3031-4041-5051-\n13,318,414,216,522,218,921,426,65075100\n0-shot5-10-15-20-25-shot\n5075100\n0-shot5-10-15-20-25-shot\nTable 1PageArctic-TILT (MP)GRAM (MP)Arctic-TILT (SlideVQA)Arctic-TILT (MMLong)32K + OCR1 - 577,178,257,226,66 - 1072,974,455,321,411 - 1569,769,652,618,916 - 2072,677,250,322,221 - 3016,531 - 4014,241 - 5018,4-5013,3MethodANLSAccuracyPage 0Page 1Page 2Page 3Page 4TILT \ufb01netuned optuna 30,812250,78700,86390,79670,75510,73120,7105GRAM0,803219,98410,83800,78540,75280,79080,7452\n1-526.66-1021.411-1518.916-2022.221-3016.531-4014.241-5018.450-1000013.3\nPayment StubsGhega Patents0-shot52,437,9paystubs:5-92,276,70-shot: 52.410-93,282,85-shot: 92.215-93,586,110-shot: 93.220-93,490,115-shot: 93.525-shot94,989,720-shot: 93.430-shot95,290,525-shot: 94.930-shot: 95.2ghega:0-shot: 37.95-shot: 76.710-shot: 82.815-shot: 86.120-shot: 90.125-shot: 89.730-shot: 90.5Page\n020406080\n1-56-1011-1516-\n72,669,772,977,1MP-DocVQA\n020406080\n1-56-1011-1516-\n50,352,655,357,2SlideVQA\nMMLongBench-DocPayment Stubs\nGhega PatentsGPT-4o\nGPT-4o2Figure 6: Improvement of Arctic-TILT zero-shot accu-\nracy given fine-tuning on up to 25 annotated documents.\nZero-shot performance of GPT-4o for comparison.\n4.3 Adapting to Novel Use Cases\nSome optimizations introduced in Arctic-TILT aim\nto improve training performance under the minimal\nmemory regime. These capabilities enable further\nimprovement of the model in a production environ-\nment, especially when encountering out-of-domain\nexamples or novel use cases, and appear vital in\nline with previous works, which have shown that\nsmaller LLMs can outperform larger, prompted\nmodels assuming we allow fine-tuning (Zhao et al.,0.0\nCon\ufb01dence\n0.0\n0.5\n0.5\n1.0\n1.0\nModel accuracy (mean score)\nModel\n95% CI\nIdealFigure 7: Arctic-TILT calibration.\n2024; Bucher and Martini, 2024).\nWe study how the zero-shot accuracy of Arctic-\nTILT increases, given fine-tuning on up to 25 an-\nnotated documents from holdout datasets. In par-\nticular, we rely on Ghega patents (Medvet et al.,\n2011) and a private dataset of payment stubs and\ncompare the model\u2019s performance to GPT-4o (refer\nto Appendix D for dataset\u2019s details).\nResults shown in Figure 6 demonstrate that\nArctic-TILT quickly approaches the accuracy of\nGPT-4o with as few as five annotated examples\nand outperform it given slightly more. These find-\nings support the argument for employing special-\nized, \u2018smaller\u2019 LLMs over a single general-purpose\nmodel in production, emphasizing the solution\u2019s\ncost-effectiveness and adaptability.\n4.4 Confidence Calibration\nFollowing the postulate of Van Landeghem et al.\n(2023), we evaluate the Expected Calibration Error\n(ECE) and Area Under the Risk-Coverage Curve\n(AURC) on the DUDE dataset. Each answer\u2019s con-\nfidence is computed from a list of per-token scores.\nIn contrast to some previous works, we take the\nminimum score in the list rather than the geometric\nmean as we found it empirically superior.\nObtained results show exceptional calibration\nand confidence assessment, achieving a state-of-\nthe-art ECE of 7.6, significantly improving upon\nthe previous best of 19.0. This suggests a closer\nalignment between model confidence and accuracy.\nAdditionally, our AURC of 25.3, which surpasses\nthe previous best of 44.0, demonstrates that our\nmodel can effectively discriminate between correct\nTable 1InputPhi-3 MiniTILT (KV)TILT4000253,43,46500415,35,37,735849056603771600010313,113,16400041251,951,95120003298414,5775InputPhi-3 MiniTILT (KV)TILT4000273,63,66500415,35,31600010413,613,66400041353,653,6512000329913571\n10100100010000\n     \n775\n52\n13\n5\n3\n415\n52\n13\n5\n3\n3\u00a0298\n412\n103\n41\n25\nPhi-3 Mini\nArctic-TILT (KV)\n(no KV)0102030\n1-56-1011-1516-2021-3031-4041-5051-\n13,318,414,216,522,218,921,426,6020406080\n1-56-1011-1516-\n72,669,772,977,1MP-DocVQA\n020406080\n1-56-1011-1516-\n50,352,655,357,2SlideVQA\nMMLongBench-Doc\n10100100010000\n4k6.5k16k64k512k\n13\u00a0571\n54\n14\n5\n4\n54\n14\n5\n4\n3\u00a0299\n413\n104\n41\n27VQA / KIE\nSummarization\nExpert (~90%)10100100010000\n4k6.5k16k64k512k\nPhi-3 MiniArctic-TILT8x less\u2028TFLOPs\n6.5k\n1Figure 8: Arctic-TILT\u2019s computational efficiency\n(TFLOPs, lower is better) compared to Phi-3 Mini on\nVQA/KIE given inputs ranging from 4k to 512k tokens.\nand incorrect predictions. It also shows our model\u2019s\nability to appropriately assign low-confidence to\npredictions demanding additional human review.\nTo explore the landscape beyond a single dataset,\nwe provide results on 18k data points sampled from\nfourteen private and public datasets in Figure 7.\nThe analysis confirms low ECE and indicates that\nthe Arctic-TILT confidence score is well calibrated\nas the accuracy (mean score) follows the diagonal\ny=xon the calibration plot.\n4.5 Computational Efficiency\nThe imperative for businesses to rapidly and effi-\nciently process substantial document volumes calls\nfor models that maximize throughput while also\nmaximizing operational efficiency.\nTo address this aspect of the model, we analyze\nthe inference floating point operations per second\n(TFLOP) required for Arctic-TILT compared to\nPhi-3 Mini (Abdin et al., 2024), an example of\na decoder-only model featuring 3.8B parameters\nand optimized by resorting to the attention sliding\nwindow. The latter was selected as a well-known\nreference model concerning the limited memory\nand compute regime we aim at, though it is not\ncapable of achieving satisfactory accuracy on Doc-\nument Understanding tasks.\nResults presented in Figure 8 indicate that Arctic-\nTILT consistently demands lower TFLOP across\nall context lengths for our primary use case of\nVQA/KIE,2reflecting its smaller parameter size.\n2We assume the output of 8 tokens, which is longer thanImportantly, concerning the input of 6.5k tokens,\nthe mean input length for VQA/KIE tasks consid-\nered before, we require 8 \u00d7less operations.",
        "2 Related Works\nThe appearance of LLMs is a result of trends that\nhave also been strongly visible in the DU field in\nrecent years.\nTraditionally, tasks like table or information ex-\ntraction from Visually Rich Documents were bro-arXiv:2408.04632v1  [cs.CL]  8 Aug 2024TILT Arctic-TILT\nVision Encoding and its Fusion with Text\nsum of text & image fusion by tensor product\nfirst layer only every encoder layer\nPretraining and Finetuning\n400k steps of adaptation 900k steps of adaptation\nSFT on 4 datasets SFT on 17 datasets\nTransformer\ndense attention, vanilla sparse attention, SLED\nmax 9k tokens max 400k tokens\nbasic optimization heavy optimization\nTable 1: Comparison of TILT and Arctic-TILT.\nken down into separate steps, such as form recogni-\ntion, field detection, and value extraction (Prabhu\net al., 2021; Medvet et al., 2011; Rusi\u00f1ol et al.,\n2013; Peanho et al., 2012; Tian et al., 2016; Le\net al., 2019; Baek et al., 2019; Holt and Chisholm,\n2018; Wang et al., 2021; Carbonell et al., 2019).\nEach of these steps typically required distinct mod-\nels or heuristics and processing pipelines that were\nlater approached in a more end-to-end manner em-\nploying graph-based approaches (Liu et al., 2019;\nHwang et al., 2021; Yu et al., 2021; Wang et al.,\n2024, inter alia ).\nUltimately, the DU field has converged on formu-\nlating tasks in a unified text-to-text format due to\nits robustness in handling various problems, which\nLLMs align well with due to their generic input-\noutput format (Mathew et al., 2021b,a; Borchmann\net al., 2021). Although this approach appears ele-\ngant and its ease of application makes it appealing\nfor industrial-scale implementation, treating docu-\nments as pure text is often insufficient, particularly\nwhere layout-intensive aspects dominate. Hence,\nthere has been a recent surge in extending LLMs\nwith visual (Li et al., 2023; Wu et al., 2023), lay-\nout modality (Fujitake, 2024), or both (Mao et al.,\n2024; Li et al., 2024; Tang et al., 2023) to better\ncapture the nuances of document structures and\nimprove performance on layout-intensive tasks.\nA separate line of work approaches DU prob-\nlems using vision-only models (Kim et al., 2021,\n2022; Lee et al., 2023; Beyer et al., 2024), assum-\ning one can address the problem without specific\narchitectural biases. However, models with textual\ninput outperform them with a notable example of\nGPT-4 Vision that benefits from the availability of\nOCR-recognized text (Borchmann, 2024).\nDespite these advancements and the significant\nbenefits the scale of LLMs offers, we provide argu-\nStackProjection O\nSum\nText\nembeddingImage\nembeddingMultiplication\nSumProjection V Projection RSumFused\nembedding\nLayerNormDropoutFigure 2: Arctic-TILT modality fusion. It can be seen as\nattention with role vector (Schlag et al., 2019) simplified\nconcerning we calculate it over a pair of aligned text\nand image tokens.\nments for smaller, problem-specific models, simi-\nlarly to Fu et al. (2024); Zhao et al. (2024) and fo-\ncus on cost-efficient deployment (Ong et al., 2024).",
        "3 Action-Based Conversation Dataset\nIn this section, we describe the task setting of\nABCD by following along with the example di-\nalog shown in Figure 1.\n3.1 Customer\nDuring data collection, customers are given a sim-\nple prompt (such as \u201cYou want to keep your sub-\nscription another year.\u201d) instead of step-by-step\ninstructions, which re\ufb02ects how real-world cus-\ntomers innately understand their own issue, but\nonly have a rough idea of how to resolve said is-\nsue. Accordingly, customers within ABCD remain\noblivious towards what values apply to which ac-\ntions, nor are they aware that actions exist in \ufb01rst\nplace. This ambiguity forces the agent and cus-\ntomer to collaboratively uncover the correct latent\nintent through back and forth communication, nat-\nurally leading to longer dialogues.3.2 Customer Service Agent\nFollowing the standard dialog setup, the agent\nstarts by parsing the dialogue history to capture the\ncustomer intent, which in Figure 1 is a subscrip-\ntion extension. ABCD then diverges as the next\nstep involves interpreting the Agent Guidelines, a\ndocument representing the internal policies of a\ncompany in the online retail domain (See Table 1).\nUsing the guidelines, the trained agent should \ufb01nd\nthe one unique sub\ufb02ow corresponding to the cus-\ntomer intent. Each sub\ufb02ow in turn is de\ufb01ned by\nexactly one unique sequence of actions.\nWhile identifying a sub\ufb02ow may seem straight-\nforward, information asymmetry prevents the cus-\ntomers from directly revealing the name of their\nintent. For example, a customer might inquire\nabout the status of their recent purchase, but an\nagent has over a dozen different sub\ufb02ows related\nto order statuses, so selecting the right one sud-\ndenly becomes highly non-trivial.\nIn our case, the agent eventually \ufb01gures out\nthe correct sub\ufb02ow and begins to execute actions,\nwhich consists of recording values given by the\ncustomer, namely the customer\u2019s full name or ac-\ncount ID in order to [Pull up Account] . As the\nthird action, the guidelines instruct the agent to\nask for the customer\u2019s membership level. After the\ncustomer supplies this information, the agent en-\nters the \u201cguest\u201d value into the agent dashboard by\nclicking the [Membership] button. Buttons have\nvariable slots that may or may not need to be \ufb01lled,\ndepending on the context (See Table 1 for a full\nlist). Dialogue success demands that agents exe-\ncute a chain of such actions in the right order with\nthe right values, while simultaneously engaging\nthe customer in natural language conversation.There are three reasons that make carrying out\na series of actions more dif\ufb01cult than the task lets\non. To start, the permitted actions in a given state\nare determined not only by Agent Guidelines, but\nalso by the user\u2019s desire, which may be in con\ufb02ict.\nFor example, the customer in Figure 1 wanted to\nextend their subscription, but the guidelines pre-\nvented the agent from doing so. Secondly, actions\nmust be completed in order. This procedural re-\nquirement comes from the realization that com-\npleting actions out of order (or with missing steps)\ndo not make sense in many real-world scenarios.\nFor example, it is critical to [Verify Identity] be-\nfore resetting someone\u2019s password, not after. Fi-\nnally, actions themselves induce stochastic out-\ncomes, preventing agents from memorizing pat-\nterns of sub\ufb02ow resolution. As an example, [Ask\nthe Oracle] often determines if a customer com-\nplaint was valid. In the case of a company error,\nthe agent is compelled to immediately resolve the\nissue, whereas a misunderstanding made by the\ncustomer warrants a different set of responses.",
        "2 Related Work\nTraditional Dialogue Datasets In recent years,\ndialogue datasets have grown in size from hun-\ndreds of conversations to the tens of thou-\nsands (Henderson et al., 2014; Budzianowskiet al., 2018; Peskov et al., 2019). Unlike open-\ndomain chatbots often built for entertainment,\ntask-oriented dialogue systems trained on such\ndatasets are intended for solving user issues. The\nresolution of these issues implicitly requires tak-\ning actions, where an action is a non-utterance\ndecision that depends on both user and system\ninputs. Despite the tremendous number of dia-\nlogues, examples in previous benchmarks \ufb01xate\non the single knowledge base (KB) lookup action\nwhere the agent searches for an item that matches\nthe user\u2019s desires and is available in the KB. By\nsticking to this sole interaction, conversations can\nbe generated through rules (Weston et al., 2016),\nparaphrased from templates (Byrne et al., 2019)\nor taken from static text scenarios (Zhang et al.,\n2018), leading to dialogues that are predominantly\nhomogeneous in nature.\nMany datasets have scaled to more domains\nas well (Eric et al., 2017; Budzianowski et al.,\n2018; Peskov et al., 2019) Since each new domain\nintroduces a KB lookup requiring different slot-\nvalues, the number of unique actions grows as a\nlinear function of the number of domains covered.\nRather than expanding wider, ABCD instead fo-\ncuses deeper by increasing the count and diversity\nof actions within a single domain.\nExploring Other Avenues Multiple aspects are\nexplored by conversational datasets attempting to\nmimic reality. Rashkin et al. (2019) studies the\nability of a dialogue model to handle empathy,\nwhile Zhou et al. (2018) focuses on common-\nsense reasoning. Another approach is to aug-\nment dialogues with multi-modality including au-\ndio (Castro et al., 2019) or visual (Das et al.,\n2017a) components. Other researchers have ex-\nplored grounding conversations with external data\nsources such as personas (Zhang et al., 2018), on-\nline reviews (Ghazvininejad et al., 2018) or large\nknowledge bases (Dinan et al., 2019). Intricate\ndialogues can also appear when studying collab-\noration (He et al., 2017; Kim et al., 2019) or nego-\ntiation (Lewis et al., 2017; He et al., 2018) which\nstrongly encourage interaction with the other par-\nticipant. In comparison, ABCD aims to make di-\nalogue more realistic by considering distinct con-\nstraints from policies.\nDialogues with Policies Procedural actions fol-\nlowing strict guidelines naturally emerge in dia-\nlogue research geared towards real-world appli-Sub\ufb02owsrecover-username,1recover-password,1reset-2fa,1status-service-added,2status-service-removed,2status-\nshipping-question,2status-credit-missing,2manage-change-address,2manage-change-name,2manage-change-\nphone,2manage-payment-method,2status-mystery-fee,3status-delivery-time,3status-payment-method,3status-\nquantity,3manage-upgrade,3manage-downgrade,3manage-create,3manage-cancel,3refund-initiate,4refund-\nupdate,4refund-status,4return-stain,4return-color,4return-size,4bad-price-competitor,5bad-price-yesterday,5\nout-of-stock-general,5out-of-stock-one-item,5promo-code-invalid,5promo-code-out-of-date,5mistimed-billing-\nalready-returned,5mistimed-billing-never-bought,5status,6manage,6missing,6cost,6boots,7shirt,7jeans,7jacket,7\npricing,8membership,8timing,8policy,8status-active,9status-due-amount,9status-due-date,9manage-pay-bill,9\nmanage-extension,9manage-dispute-bill,9credit-card,10shopping-cart,10search-results,10slow-speed10\nActionsverify-identity, ask-the-oracle, validate-purchase, make-password, promo-code, subscription-status, offer-refund,\nmake-purchase, record-reason, enter-details, shipping-status, update-order, pull-up-account, update-account, send-\nlink, notify-team, membership, search-faq, try-again, log-out-in, instructions, search-jeans, search-shirt, search-\nboots, search-jacket, search-pricing, search-membership, search-timing, search-policy, select-faq\nTable 1: Full ontology of Agent Guidelines decomposable into high-level \ufb02ows describing the overall category\nand sub\ufb02ows de\ufb01ning a unique set of intents. All actions are also shown. Upper script numeral indicates the \ufb02ow\nthat the sub\ufb02ow belongs to. 1: account access, 2: manage account, 3: order issue, 4: product defect, 5: purchase\ndispute, 6: shipping issue, 7: single item query, 8: storewide query, 9: subscription inquiry, 10: troubleshoot site\ncations. Hybrid Code Networks encode busi-\nness logic through masking templates since vari-\nous behaviors become nonsensical in certain sit-\nuations (Williams et al., 2017). Research from\nMoiseeva et al. (2020) studies multi-purpose vir-\ntual assistants that attempt to distinguish among\nthirteen explicit actions. The closest prior work\nto ABCD is the Schema Guided Dialogue (SGD)\ndataset, which contains dozens of API calls that\ncan be interpreted as individual actions send-\ning commands to a SQL engine (Rastogi et al.,\n2020b). The functionality of these actions is occa-\nsionally restricted to re\ufb02ect constraints of real-life\nservices. The action restrictions within ABCD are\nmade explicit by the Agent Guidelines manual.",
        "Existing goal-oriented dialogue datasets focus\nmainly on identifying slots and values. How-\never, customer support interactions in reality\noften involve agents following multi-step pro-\ncedures derived from explicitly-de\ufb01ned com-\npany policies as well. To study customer ser-\nvice dialogue systems in more realistic set-\ntings, we introduce the Action-Based Con-\nversations Dataset (ABCD), a fully-labeled\ndataset with over 10K human-to-human di-\nalogues containing 55 distinct user intents\nrequiring unique sequences of actions con-\nstrained by policies to achieve task success.\nWe propose two additional dialog tasks, Ac-\ntion State Tracking and Cascading Dialogue\nSuccess, and establish a series of baselines in-\nvolving large-scale, pre-trained language mod-\nels on this dataset. Empirical results demon-\nstrate that while more sophisticated networks\noutperform simpler models, a considerable\ngap (50.8% absolute accuracy) still exists to\nreach human-level performance on ABCD.1",
        "5 Dataset Statistics and Analysis\nWe validate all dialogues to pass quality thresh-\nolds such as including a minimum number of ac-\ntions and avoiding copy/paste behavior. After \ufb01l-\ntering, we end up with 10,042 total conversations\nwith an average of 22.1 turns \u2013 the highest turn\ncount among all compared datasets. Unsurpris-\ningly, ABCD includes more actions per dialogue\nthan other datasets, by at least a factor of two.\nABCD also contains a lower absolute number of\ntokens, but also has the highest variance in the\nnumber of tokens per turn. (See Table 2.)\nSince each sub\ufb02ow represents a unique cus-\ntomer intent, ABCD contains 55 user intents\nevenly distributed through the dataset. By in-\nterpreting buttons as domains, the dataset con-\ntains 30 domains and 231 associated slots, com-\npared to 7 domains and 24 slots within Multi-\nWOZ (Budzianowski et al., 2018).\nBy grounding to the relatable scenario of chat-\nting with customer support of an online retail\ncompany, speakers often showcase various forms\nof natural dialogue, such as offering diverse rea-\nsons for shopping or asking detailed follow-up\nquestions. Furthermore, the unconstrained nature\nof Expert Live Chat allows users to chat with each\nother in a free-form style. Dialogues exhibited\nnormal texting behavior such as users speaking for\nmany turns in a row or \ufb01xing typos with a star in\nthe subsequent line. Other examples of linguistic\nphenomenon can be observed in Table 5.Metric DSTC2 M2M KVRET MultiWOZ SGD MultiDoGO ABCD\nNum of Dialogues 1,612 1,500 2,425 8,438 16,142 40,576 8,034\nNum of Turns 23,354 14,796 12,732 113,556 329,964 813,834 177,407\nNum of Tokens 199,431 121,977 102,077 1,490,615 3,217,369 9,901,235 1,626,160\nAvg. Turns / Dialogue 14.49 9.86 5.25 13.46 20.44 20.06 22.08\nAvg. Tokens / Turn 8.54 8.24 8.02 13.13 9.75 12.16 9.17\nStd Dev. Tokens / Turn 2.95 5.99 6.07 6.19 6.48 \u2013* 6.80\nAvg. Actions / Dialogue 1.0 1.0 1.0 1.81 1.24 \u2013* 3.73\nNo. Unique Tokens 986 1,008 2,842 23,689 30,352 70,003 23,686\nNo. Unique Slots 8 14 13 24 214 73 231\nNo. Slot Values 212 138 1,363 4,510 14,139 55,816 12,047\nNo. Domains 1 2 3 7 16 6 30\nTable 2: Comparison of ABCD to similar dialogue datasets. Numbers reported are for the train split on all datasets,\nwith bold values indicating the top score for each metric. *MultiDoGO is not public, unable to calculate new stats.\n6 ABCD as a Dialogue Benchmark\nThe novel features in ABCD brings two new di-\nalog tasks, Action State Tracking and Cascading\nDialogue Success. We also build baseline systems\nthat are variants of standard dialogue models and\nreport their results on ABCD.\n6.1 Action State Tracking\nAction State Tracking (AST) aims at detecting\nthe pertinent intent by interpreting customer utter-\nances while taking into account constraints from\nthe Agent Guidelines, an aspect not considered in\ntraditional dialog state tracking (DST). For exam-\nple, a conceivable dialogue task might entail help-\ning a customer [Reset Password] once this intent\nhas been identi\ufb01ed. In contrast, the appropriate\nnext step within AST is governed by the Agent\nGuidelines, which might require [Verify Identity]\nof the customer \ufb01rst, or any number of other ac-\ntions, before executing the password reset.\nEach series of actions is considered a unique\nsub\ufb02ow that belongs to a number of high-level\nconversational \ufb02ows. Each individual action in-\ncludes the active button bto click and its corre-\nsponding slots sand valuesv. The task consists\nof executing an action, which constitutes a sin-\ngle agent turn. More speci\ufb01cally, given a context\nCt= [x1,x2,...,x t]wherextcan be a customer\nutterancexc\nt, an agent utterance xa\nt, or a prior ac-\ntionxb\nt, a model should predict the button of the\ncurrent action as well as the relevant slots and val-\nues, if any exist{xb\nt+1= (b,s,v )\u2208B\u00d7S\u00d7V} .\nThis structure is designed to mimic DST where\neach user intent is broken down into domains,\nslots and values (d,s,v ). For both AST and DST,\nthe higher level domain or button can have vary-ing slots. The reverse is also true \u2013 a given slot\ncan be associated with multiple domains or but-\ntons. Lastly, both contain values that can be enu-\nmerable (i.e. payment types or shipping statuses)\nor non-enumerable (phone numbers or email ad-\ndresses). Following the pattern set by Rastogi\net al. (2020b), enumerable values are given in the\nontology to be accessible by a model, whereas the\nnon-enumerable items are not.\nDespite the similar structure, AST deviates\nfrom DST since predicting the right action re-\nquires not only parsing the customer utterance,\nbut also adhering to Agent Guidelines. Suppose\na customer is entitled to a discount which will be\noffered by issuing a [Promo Code] . The customer\nmight request 30% off, but the guidelines stipulate\nonly 15% is permitted, which would make \u201c30\u201d\na reasonable, but ultimately \ufb02awed slot-value. To\nmeasure a model\u2019s ability to comprehend such nu-\nanced situations, we adopt overall accuracy as the\nevaluation metric for AST.\n6.2 Cascading Dialogue Success\nSince the appropriate action often depends on\nthe situation, we propose the Cascading Dialogue\nSuccess (CDS) task to measure a model\u2019s ability\nto understand actions in context. Whereas AST\nassumes an action occurs in the current turn, CDS\ngives an agent the additional options of respond-\ning with an utterance or ending the conversation.\nMoreover, pro\ufb01ciency is no longer measured as\nsuccess over isolated turns but rather as success\nover sequences of consecutive turns.\nFormally, given Ct= [x1,x2,...,x t]as a con-\ntext composed of utterances xc,xa\u2208U and ac-\ntionsxb\u2208A, a model should predict all remain-\ning stepsx>talong with their realized forms. Pos-sible next steps are to take an action, respond with\ntext or end the task. When the next step is an\nactionxb\nt+1, the model should predict the button\nwith its slots and values as in AST. If the agent\nspeaks in the next step xa\nt+1, the model should\nrank the true utterance highest, as measured by re-\ncall metrics.1Finally, the model should recognize\nwhen to end the conversation.\nRewarding the model only when it predicts ev-\nery step correctly is counter-productive because\nminor variations in sentence order do not alter\noverall customer satisfaction. Therefore, CDS is\nscored using a variation on Cascading Evalua-\ntion (Suhr et al., 2019). Rather than receiving a\nsingle score for each conversation, cascaded eval-\nuation allows the model to receive \u201cpartial credit\u201d\nwhenever it successfully predicts each successive\nstep in the chat. This score is calculated on ev-\nery turn, and the model is evaluated based on the\npercent of remaining steps correctly predicted, av-\neraged across all available turns. (See Appendix C\nfor more details.)\n6.3 Baseline Models\nWe also run several baselines on these new tasks.\nThe backbone of all our baseline systems is a\npre-trained Transformer-based model acting as a\ncontext encoder. More speci\ufb01cally, given the di-\nalogue history as a series of utterances, we \ufb01rst\njoin the utterances together with a [SEP] token\nand then tokenize the entire input using Word-\nPiece (Schuster and Nakajima, 2012). Next, we\nfeed the entire input into a BERT model and per-\nform a learned pooling on the hidden states in the\n\ufb01nal layer, which results in a \ufb01xed-length latent\nvectorhenc\u2208R128(Wolf et al., 2019). After-\nwards, we attach a variety of prediction heads con-\nditioned on the hencvector to generate the \ufb01nal\noutput. Details of the prediction heads for the two\nproposed tasks are described next.\nWe break down Action State Tracking (AST)\ninto two sub-problems, button-slot prediction and\nvalue-\ufb01lling. Given the ontology, button predic-\ntion is a straightforward classi\ufb01cation task over\n231 known options, so the prediction head is just a\nlinear classi\ufb01er with a softmax activation for nor-\nmalization:Pb\u00b7slot=Softmax (Wah\u22a4\nenc+ba).\nTo handle value-\ufb01lling, we further decompose\n1Sentences with similar semantics may be formulated in\nseveral ways, so we opt for response retrieval over text gen-\neration since common metrics (i.e. BLEU score) tend to be-\ncome unreliable in these situations (Liu et al., 2016).the task into predicting enumerable and non-\nenumerable values. The ontology lists out all |E|\nenumerable values, so the prediction head penum\nsimply maps the hidden state hencinto the ap-\npropriate dimensions. To handle non-enumerable\nvalues, we follow the insight from (Ma et al.,\n2019) which notes that practically all such values\nare stated by the customer in conversation, so a\nmodel can copy these values from the tokenized\ncontext. During pre-processing, we extract up to\n|N|unique tokens from the natural language cus-\ntomer utterances, where pcopythen represents the\ndistribution over these possible options.2\nWe imitate the TRADE architecture from (Wu\net al., 2019), where conditioned on the action, the\nmodel chooses to either copy from the context\npcopyor select from the enumerable entities penum\nbased on a gating mechanism. The gate is condi-\ntioned on the hidden state hencas well as a learned\ncontext vector ci. Concretely,\npenum=Softmax (Weh\u22a4\nenc+be)\u2208R|E|\npcopy=Softmax (Wch\u22a4\nenc+bc)\u2208R|N|\nci=W\u22a4\nc\u00b7pcopy\u2208Rhid\npgate=\u03c3(Wg\u00b7[henc;ci])\u2208R1\nPval= [pgate\u00d7pcopy; (1\u2212pgate)\u00d7penum]\u2208R|E+N|\nwhere\u03c3represents the Sigmoid function and [\u00b7;\u00b7]\nis the concatenation operation. The \ufb01nal value\npredictions are the argmax of Pvalwhich merge\nthe probabilities of penumandpcopytogether.\nFor Cascading Dialogue Success (CDS), we\nalso tackle next step selection, utterance ranking,\nand intent classi\ufb01cation. Next step selection is a\nchoice between retrieve utterance ,take action and\nend conversation . Intent classi\ufb01cation consists of\nchoosing from the 55 available sub\ufb02ows. Given\nthis basic setting, both tasks use the same setup of\na linear layer followed by a softmax, albeit with\ntheir own respective weights WNS\u2208R3\u00d7hidand\nWIC\u2208R55\u00d7hid. When the next step is to take\naction , the AST model is reused to determine the\nbutton-slot and value. When end conversation is\nselected, all future predictions are ignored, much\nlike an <EOS> symbol signi\ufb01es stopping.\nThis leaves us with utterance ranking, which is\nonly evaluated when retrieve utterance is chosen\nas the next step. Our ranker reproduces the design\n2Choosing larger |N|leads to higher recall, but lower pre-\ncision. We found N= 100 to work well in practice.from (Guu et al., 2020), where the encoded con-\ntexthctxis compared against each encoded candi-\ndate response hcand to produce a ranking score.\nTo embed each jthcandidatedjwe \ufb01rst create\nits inputdinput\nj . Following standard practice, we\nprepend the candidate text djwith [CLS] , sepa-\nrate the individual utterances uiwithin the candi-\ndate response using a [SEP] token, and append\na \ufb01nal [SEP] token afterwards. (Devlin et al.,\n2019). This input dinput\nj is then fed into a static\npretrained BERT model to get an initial hidden\nstate, which is \ufb01nally projected using a learned\nweightWdj\u2208R128\u00d7hidto producehcand. To\nobtainhctxwe start with the hidden state henc\nfrom before and apply a projection matrix WUR\u2208\nR128\u00d7hidto reach the desired dimensionality.\ndinput\nj = [CLS]u1[SEP]u2[SEP]...[SEP]un[SEP]\nhcand=WdjBERT base(dinput\nj)\u22a4\u2208R128\nhctx=WURh\u22a4\nenc\u2208R128\nf(xi,dj) =h\u22a4\nctxhcand\nPrank\nj=exp(f(xi,dj))\n\u03a3d\u2032\njexpf(xi,d\u2032\nj)\nThe \ufb01nal rank is given by normalizing each jth\nscore against all other candidate scores. We use\nthe training objective from (Henderson et al.,\n2019) to calculate the loss:\nJ=M=100\u2211\nj=1P(xi,dj)\u2212M\u2211\ni=1logM\u2211\nj=1expf(xi,dj)\nwhereMis the size of the total candidate set.\n6.4 Experiments\nWe performed experiments on the two newly pro-\nposed tasks, AST and CDS. AST consists of two\nsubtasks, button-slot prediction and value-\ufb01lling,\nwhile CDS builds on this with three additional\nsubtasks of next step selection, utterance ranking,\nand intent classi\ufb01cation. For both tasks, we exper-\nimented with two types of frameworks, a pipeline\nversion and an end-to-end version. The pipeline\nversion trains each subtask separately while the\nend-to-end optimizes all tasks jointly (Liang et al.,\n2020; Rastogi et al., 2020a; Ham et al., 2020).\nThe pipeline model uses a BERT model trained\nwith the RAdam optimizer (Liu et al., 2020).\nTo test the performance of different pretrained\nmodels under the end-to-end framework, weMetric Pipeline BERT AlBERT RoBERTa\nB-Slot 86.7% 89.9% 90.9% 93.6%\nValue 42.1% 61.6% 61.0% 67.2%\nAction 32.3% 59.5% 59.2% 65.8%\nTable 3: Metrics for Action-State Tracking. Pipeline\nvalues come from models trained on individual sub-\ntasks, other models are trained jointly end-to-end.\nexperiment with three additional encoders, Al-\nBERT (Lan et al., 2020), RoBERTa (Liu et al.,\n2019) and RoBERTa-Large. AlBERT model has\nan inter-sentence coherence task and a lighter\nmemory footprint compared to BERT, while\nRoBERTa model has substantially more data and\nhyper-parameter tuning in pretraining than BERT.\nIn the future, we also plan to include GPT-based\nmodels, such as DialoGPT (Zhang et al., 2020) in\nour comparison.\n6.5 Results\nFor both tasks, moving from the pipeline archi-\ntecture to a jointly trained method displayed no-\nticeable improvement in accuracy. As hinted at\nin prior works (Liang et al., 2020), we suspect\nthe group effort gives each subtask extra super-\nvision from other subtasks for more data ef\ufb01cient\ntraining. In the AST task, we found steady im-\nprovements as we move from the older to the\nnewer models with vanilla BERT at 59.5% accu-\nracy and RoBERTa doing the best at 65.8%. For\nthe CDS task, we found a similar trend where\nRoBERTa-Large outperforms BERT, but only by\na mere 0.6%. We hypothesize this small gap be-\ntween models is due to the fact that none were par-\nticularly trained on dialogue data which impacts\ntheir ability to produce a useful encoding (Wu and\nXiong, 2020).\nSeparately, we evaluate CDS subtask dif\ufb01culty\nby asking human volunteers to select the correct\nlabel from a list of possible options. As an ex-\nample, workers would be presented with 55 dif-\nferent classes for Intent Classi\ufb01cation and asked\nto choose the right one. Since humans typically\nstruggle when choosing from large collections of\nitems, \ufb01ne-tuned models performed roughly on\npar or better compared to humans in this unnat-\nural setting. On the other hand, human evaluation\nfor the overall CDS task was judged by measuring\nthe success rate in a standard conversational sce-\nnarios where behavioral instincts are activated, so\nhumans were able to excel on this environment.Model Intent Nextstep B-Slot Value Recall@1/5/10 Cascading Eval\nHuman 85.5% 84.0% 79.0% 77.5% N/A 82.7%\nPipeline 90.4% 83.8% 86.7% 42.1% 26.2/51.7/63.1 18.2%\nBERT-base 89.3% 87.6% 85.9% 73.1% 21.7/46.6/58.7 31.3%\nAlBERT 88.5% 87.2% 86.1% 70.4% 22.1/47.4/58.9 31.2%\nRoBERTa 89.7% 87.8% 87.6% 73.1% 21.6/46.7/58.6 31.5%\nRoBERTa-Large 90.5% 87.5% 88.5% 73.3% 22.0/ 47.8/59.1 31.9%\nBERT-base w/o Action Info 88.4% 76.8% 83.7% 63.4% 18.6/43.0/57.9 29.2%\nBERT-base w/ Guidelines 83.2% 87.5% 85.6% 72.4% 21.8/46.9/58.5 30.6%\nBERT-base w/ Intent Info 100% 88.6% 88.9% 73.8% 22.2/47.6/59.1 32.3%\nBERT-base w/ Intent + Guide 100% 89.2% 89.3% 74.0% 22.6/48.1/59.4 32.7%\nTable 4: Cascading dialogue success task performance with breakdown of all \ufb01ve subtasks. Numbers displayed\nare the average of three seeds. Human evaluation conducted with size of 100 samples per person.\n6.6 Ablation Study\nWe perform an ablation study to test the signif-\nicance of the key features in ABCD. Recall, ac-\ntions are characterized by their dual nature of re-\nquiring signals from both the customer and the\ncompany guidelines. To that end, we provided the\nground truth intent to measure the impact of the\ncustomer side. Conversely, we also test the com-\npany side by masking out invalid buttons based on\nthe insight that the Agent Guidelines are useful for\nnarrowing down the range of possible actions. In\nboth situations, we would expect that providing\nsuch oracle guidance would boost performance.\nLastly, note that the appropriate action depends on\nthe outcomes of prior actions, so for a \ufb01nal exper-\niment we removed prior actions and their explana-\ntions from the context to test their impact on task\nsuccess. (See Appendix E for details.)\nWe observe that supplying the intent informa-\ntion to the BERT model causes a noticeable boost\nin dialog success, bringing the score to 32.3%.\nHowever, augmenting the model with knowledge\nof the guidelines unexpectedly dropped perfor-\nmance down to 30.6%. Further analysis revealed\nthe imperfect intent classi\ufb01er would occasionally\nmask out valid buttons, leaving only incorrect\nones to choose from. As a result, the downstream\naction predictor would be prevented from doing\nits job, causing errors to accumulate. To test this\nhypothesis, we ran another model (Intent+Guide)\nwhich had access to guidelines along with an ora-\ncle intent classi\ufb01er. This model reached the peak\nobserved performance of 32.7%, highlighting the\nimportance of both components. As a \ufb01nal result,\nremoving action information away from action-\nbased conversations unsurprisingly causes a major\nperformance drop (Table 4).7 Conclusion and Future Work\nIn conclusion, we have presented ABCD which\nincludes over 10K dialogues that incorporate pro-\ncedural, dual-constrained actions. Additionally,\nwe established a scalable method for collecting\nlive human conversations with unequal partners.\nWe found that pre-trained models perform decent\non Action State Tracking, but there is a large gap\nbetween humans agents and the top systems for\nCascading Dialogue Success.\nWe plan to incorporate GPT-related mod-\nels (Hosseini-Asl et al., 2020), as alternate forms\nof preprocessing have shown promise in other\nNLP tasks. Other techniques could also be used\nto incorporate speaker info, action semantics and\nother meta-data. Wholly new systems that attend\nto the Agent Guidelines in a fully differentiable\nmanner are also worth exploring. By grounding\ndialogues to in-depth scenarios with explicit poli-\ncies, we hope to have pushed towards a better un-\nderstanding of dialogue success.\nAcknowledgments\nThe authors would like to thank Tao Lei, Felix\nWu and Amnol Kabra for their feedback and sup-\nport. We would also like to thank the anonymous\nNAACL 2021 reviewers for pointing out speci\ufb01c\nareas of confusion in our submission, which we\nhave tried our best to clarify.\nEthical Considerations\nThis paper presents a new dataset which was col-\nlected through the use of crowdworkers. All agent\nworkers were compensated a fair wage based on\ntheir local standard of living, where their loca-\ntion was determined during the vetting process.\n(Please refer to Appendix A for more details.)",
        "4 Data Collection Methodology\nThis section outlines how we collect and annotate\nour dataset with context-dependent actions.\n4.1 Agent Training\nManaging complex guidelines requires \ufb01ltering\nfor top agents, which we do by certifying Mechan-\nical Turk (MTurk) workers through an extensive\n20-question quiz touching on all aspects of task\ncompletion. Keeping the bar high, we set a mini-\nmum threshold of 80% accuracy of the quiz which\nresulted in a low 20% pass rate. After passing the\nexam, we offered the answer key to agents which\nfurther improved understanding. We also created\nshort, 10-minute tutorial videos to showcase how\nto handle the most dif\ufb01cult aspects of the task.\nA group chat app was also deployed to offer live\nfeedback for agents, simulating how supervisors\ncoach customer service representatives in real life.\nFinally, we carefully designed an incentive struc-\nture that rewards agents for correctly identifying\nthe user intent to encourage clari\ufb01cation behavior.\n(Appendix A covers more details.)\n4.2 Expert Live Chat\nRather than utilizing Wizard-of-Oz techniques\n(such as in MultiWOZ), we developed Expert\nLive Chat which contains three unique aspects:(1) Conversations are conducted continuously in\nreal-time. (2) Users involved are not interchange-\nable. (3) Players are informed that all participants\nare human \u2013 no wizard behind the scenes.\n4.2.1 Synchronous Two-person Dialogue\nNormal human conversations occur in real-time,\nbut coordinating multiple users in this manner\nis resource-intensive, so other datasets often em-\nployed workarounds to avoid this dif\ufb01culty. For\nexample, other works have applied rules (Bordes\net al., 2017), templates (Byrne et al., 2019) or\nparaphrasing (Shah et al., 2018) to produce con-\nversations. Wizard-of-Oz (WoZ) techniques in-\ncorporate humans into the mix by allowing one\nof them to play the system role as a wizard\nbehind the scenes (Kelley, 1984). In particu-\nlar, (Budzianowski et al., 2018) decomposed di-\nalogues into individual turns, where for each turn\na new author is responsible for reading the con-\ntext and generating the next plausible response.\nDespite the time-consuming nature, some datasets\nhave produced synchronous dialogues between\ntwo humans (Lewis et al., 2017). However, the\nskill sets of ABCD workers are notably unequal,\nexacerbating the matching problem.\n4.2.2 Pairing Users of Unequal Capability\nExpert Live Chat matches a highly trained agent\nwith a knowledgeable, yet otherwise average cus-\ntomer in real-time. Since the backgrounds are\nuneven, unlike other datasets with concurrent\nusers (Lewis et al., 2017; Zhang et al., 2018; Das\net al., 2017b), incoming Turkers cannot simply be\nrandomly assigned a role. In other words, having\ntwenty participants does not necessarily equate to\nten conversations since it\u2019s possible that only a\nquarter of them are quali\ufb01ed as agents. When such\nan imbalance inevitably arises, one group must\nwait until someone from the other side becomes\navailable. However, leaving either side waiting for\ntoo long leads to serious consequences since idle\ntime directly affects their pay rate.\nTo minimize the likelihood of such an outcome,\nwe \ufb01rst ensure that a reasonable pool of agents\nare always available. Then, we increase the num-\nber of active customers by methodically inviting a\nsubset of customers one batch at a time. To do so,\nwe established a quali\ufb01cation exam for customers\nto ensure their availability during a speci\ufb01ed time\nperiod. Finally, we also redesigned the chat appli-\ncation to make the waiting room experience moreFigure 2: The Agent Dashboard is split into three sec-\ntions. KB Query actions always have system output,\nwhile actions in the Interaction Zone require user input.\nThe FAQ/Policy section is associated with describing\ncompany policies and technical troubleshooting.\npalatable. (See Appendix B for full breakdown.)\nWith these changes, we successfully increased the\npairing rate from 18 out of 80 active users up to\n72 out of 83, an increase of nearly 400%, while\nmaintaining wait times under 10 minutes.\n4.2.3 Interaction Framework\nBesides pairing, we increased the likelihood of\ncollecting rich dialogues without the need for ex-\ntensive instructions by optimizing the chat experi-\nence itself. In particular, we observed the greatest\ngains by grounding the conversation to the relat-\nable scenario of online shopping, which provided\nimmediate context to participants without requir-\ning any extra training.\nFor example, the Agent Dashboard was ar-\nranged to closely re\ufb02ect actual agent workspaces\n(Figure 2). On the customer side, scenarios in the\nCustomer Panel included an image of the product\nbeing discussed, along with other meta-data such\nas the brand or price to match a true shopping ex-\nperience as much as possible (Appendix H). We\nalso explicitly told customers the other speaker\nwas human to encourage natural responses over\ncon\ufb01ned commands meant for machines. Most\nimportantly, customers were given dynamically\ngenerated, natural-language prompts that did not\ninclude information about the values needed to re-\nsolve their issue. As a general framework, Ex-pert Live Chat can be applied in any real-world\nscenario involving an expert and novice. Indeed,\nincreasing the verisimilitude of the experience is\nprecisely what allowed higher quality dialogues to\nbe generated by the workers.\n4.3 Annotation of Actions and Values\nThe \ufb02ows and sub\ufb02ows are automatically anno-\ntated since we have the provenance of each intent\nwhen generating the customer prompt. Addition-\nally, given the ground truth sub\ufb02ow of each con-\nversation, we can deterministically map them to\nthe correct section within the Agent Guidelines\noutlining the correct actions. Calculating accu-\nracy then becomes a simple exercise to align the\npredicted actions with the ones required by the\nmanual. In this way, we capture a key bene\ufb01t of\nmachine-generated text (Shah et al., 2018) without\nsacri\ufb01cing the bene\ufb01t of engaging real users.",
        "1 Introduction\nThe broad adoption of virtual assistants and cus-\ntomer service chatbots in recent years has been\ndriven in no small part by the usefulness of these\ntools, whereby actions are taken on behalf of the\nuser to accomplish their desired targets (Ama-\nzon, 2019; Google, 2019). Research into task-\noriented dialogue has concurrently made tremen-\ndous progress on natural language understanding\nof user needs (Wu et al., 2019; Rastogi et al.,\n2020b; Liang et al., 2020). However, selecting\nactions in real life requires not only obeying user\nrequests, but also following practical policy limi-\ntations which may be at odds with those requests.\nFor example, while a user may ask for a refund on\ntheir purchase, an agent should only honor such a\nrequest if it is valid with regards to the store\u2019s re-\nturn policy. Described in actions, before an agent\n1All code and data will be available at this location.\nFigure 1: An interaction from ABCD (left) starts with\nthe customer receiving a prompt (top right) to ground\nthe dialogue. The agent follows the guidelines (bottom\nright) to identify the customer intent and to assist them\nin resolving the issue through a series of actions.\ncan[O\ufb00er Refund] , they must \ufb01rst [Validate Pur-\nchase] . Furthermore, resolving customer issues\noften concerns multiple actions completed in suc-\ncession with a speci\ufb01c order since prior steps may\nin\ufb02uence future decision states. (See Figure 1)\nTo more closely model real customer service\nagents, we present the Action-Based Conversa-\ntions Dataset (ABCD) consisting of 10,042 con-\nversations containing numerous actions with pre-\ncise procedural requirements. These actions dif-\nfer from typical dialogue acts because tracking\nthem necessitates striking a balance between ex-\nternal user requests and internally-imposed guide-\nlines. Thus, the major difference between\nABCD and other dialogue datasets, such as Mul-\ntiWOZ (Budzianowski et al., 2018), is that it asks\nthe agent to adhere to a set of policies while simul-\ntaneously dealing with customer requests.\nWhile the prevalent data collection paradigm\ninvolves Wizard-of-Oz techniques, our situationarXiv:2104.00783v1  [cs.CL]  1 Apr 2021containing asymmetric speakers compelled the de-\nsign of a novel Expert Live Chat system. Our\ndataset includes asymmetric speakers because, un-\nlike customers, agents must undergo extensive\ntraining to be able to navigate the Agent Guide-\nlines during real-time conversations. This makes\na naive pairing process untenable since arbitrary\nmatching might lead to chats containing two users\nwho share the same role.\nBased on the unique aspects of ABCD, we pro-\npose two new tasks. To start, Action State Track-\ning (AST) closely mirrors the format of Dialogue\nState Tracking where the user intent is inferred\nfrom the dialogue history. AST then differs since\nthe correct state must also be reconciled with the\nrequirements outlined in the Agent Guidelines. As\na second task, Cascading Dialogue Success (CDS)\nextends this notion across the entire conversation.\nAt each turn, the agent decides to take an action,\nrespond with an utterance or end the chat. As\nneeded, the agent should also predict the right ac-\ntion or select the best utterance.\nFor each task, we build various models to es-\ntablish baseline performance and to highlight the\nimportance of each constraint. Experiments show\nthat in addition to conversation history, condition-\ning on the Agent Guidelines further boosts perfor-\nmance, with top models relying on both aspects\nto reach 31.9% accuracy. Additional results show\nremoving action context hurts performance, im-\nplying the importance of taking into account the\nsequential nature of actions. Lastly, human eval-\nuation reaches 82.7%, demonstrating ample room\nfor future improvement.\nThe contribution of this work is three-fold: (1)\nWe provide a novel, large-scale dataset containing\ncontext-dependent, procedural actions along with\ncorresponding Agent Guidelines. (2) We establish\na new technique called Expert Live Chat for cap-\nturing natural dialogue between two unequal inter-\nlocutors. (3) We propose two metrics, Action State\nTracking and Cascading Dialogue Success, for\nmeasuring dialogue comprehension with policy\nconstraints. Finally, we build on pretrained neural\nmodels to serve as baselines for these tasks.",
        "2 Related Work\nThere are numerous open-source tools related to\nthe different aspects of RAG, namely inference,\ntraining and evaluation. LlamaIndex (Liu, 2022),\nLangChain (Chase, 2022) and Haystack (Pietsch\net al., 2019) are well known libraries for composing\nRAG pipelines; however they are not focused on\nevaluation and their training capability is under-\ndeveloped.\nHoshi et al. (2023) proposes a framework for\ndeveloping RAG-based LLMs; while our process-\ning may be similar in the sense of being comprised\nof custom individual steps, they do not introduce\nany form of training. Khattab et al. (2023, 2022)\npresents a different approach, where LLM prompt-\ning is represented as a programming language, to\nbe optimized and compiled; a rather unique and\ngeneral approach that could benefit RAG but has\na high level of complexity due to the abstractions\nintroduced. Saad-Falcon et al. (2024) focuses more\non the evaluation aspect, by creating synthetic data\nand training an LLM critic to evaluate the RAG sys-\ntem. Hsia et al. (2024) studies aspects of retrieval\non the performance of RAG; our RAG Foundry li-\nbrary is general and enables experimentation on all\naspects of RAG: retrieval, text-processing, prompt\ndesign, model selection, inference and evaluations.\nRecently, a concurrent work by Jin et al. (2024)\nproposes a RAG building framework, including\nsome RAG implementations and datasets; we fo-\ncus on extensibility, letting users define custom\ntypes of pipelines with custom components. Rau\net al. (2024) presents a framework, sharing a\nsimilar design-principle of extensibility-through-\nconfiguration as ours; their library imposes a spe-\ncific workflow structure (retriever, ranker, LLM)\nwhile our library is more general and does not im-\nposes any specific paradigm.",
        "Implementing Retrieval-Augmented Genera-\ntion (RAG) systems is inherently complex,\nrequiring deep understanding of data, use\ncases, and intricate design decisions. Addi-\ntionally, evaluating these systems presents sig-\nnificant challenges, necessitating assessment of\nboth retrieval accuracy and generative quality\nthrough a multi-faceted approach. We intro-\nduce RAG F OUNDRY , an open-source frame-\nwork for augmenting large language models\nfor RAG use cases. RAG F OUNDRY inte-\ngrates data creation, training, inference and\nevaluation into a single workflow, facilitating\nthe creation of data-augmented datasets for\ntraining and evaluating large language mod-\nels in RAG settings. This integration en-\nables rapid prototyping and experimentation\nwith various RAG techniques, allowing users\nto easily generate datasets and train RAG\nmodels using internal or specialized knowl-\nedge sources. We demonstrate the frame-\nwork effectiveness by augmenting and fine-\ntuning Llama-3 and Phi-3 models with diverse\nRAG configurations, showcasing consistent im-\nprovements across three knowledge-intensive\ndatasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry .",
        "3 RAG Foundry\nTheRAG F OUNDRY framework facilitates rapid\nprototyping and experimentation with various RAG\nsettings and configurations. The library is com-\nposed of four modules: dataset creation, training,name: my_pipeline\ncache: true\nsteps:\n- _target_: dataset_loaders.loaders.HFLoader\ninputs: main\ndataset_config:\npath: \"Tevatron/wikipedia-trivia\"\nsplit: train\n- _target_: dataset_loaders.loaders.LocalLoader\ninputs: fewshot-data\nfilename: prepared-fewshot-data.jsonl\n- _target_: global_steps.sampling.ShuffleSelect\ninputs: main\nshuffle: 42\nlimit: 10000\n- _target_:\nlocal_steps.retrievers.HaystackRetriever ,\u2192\ninputs: main\npipeline_path: configs/qdrant.yaml\nquery_key: query\ndocs_key: positive_passages\n- _target_: global_steps.sampling.FewShot\ninputs: main\ninput_dataset: fewshot-data\nk:3\noutput_key: fewshot_examples\n- _target_: local_steps.prompter.TextPrompter\ninputs: main\nprompt_file: prompts/basic.txt\noutput_key: my_prompt\nmapping:\nquestion: query\ncontext: positive_passages\nfewshot: fewshot_examples\nanswer: answers\n- _target_: global_steps.output.OutputData\ninputs: main\nfile_name: TQA_train_processed.jsonl\nListing 1: Example of a dataset creation configuration.\nThe example contains data loading, shuffling, sampling,\nretrieval, few-shot collection, prompt building and sav-\ning steps.\ninference, and evaluation. Below, we expand on\neach of the modules and provide example configu-\nrations for running them.\n3.1 Data Creation and Processing\nTheprocessing module facilitates the creation of\ncontext-enhanced datasets by persisting RAG in-\nteractions, which are essential for RAG-oriented\ntraining and inference (Berchansky et al., 2024; Liu\net al., 2024; Yu et al., 2024b). These interactions\nencompass dataset loading, column normalization,\ndata aggregation, information retrieval, template-\nbased prompt creation, and various other forms ofpre-processing. The processed data can be saved\nin a consistent, model-independent format, along\nwith all associated metadata, ensuring compatibil-\nity and reproducibility across different models and\nexperiments.\nThe processing module is comprised of an ab-\nstract pipeline with multiple steps, each defined by\nPython classes that implement specific data pro-\ncessing functionalities. These steps are categorized\ninto two types:\n\u2022Global Steps : Can act on the dataset as a whole,\nmaking them useful for operations such as aggre-\ngations, group-by, examples filtering, join opera-\ntions, and more.\n\u2022Local Steps : Operate on individual examples,\nmaking them suitable for tasks such as retrieval,\ntext processing, and field manipulation.\nThe modular design allows for building flexible\nand efficient data processes, tailored to the needs\nof RAG-oriented training and inference. Steps can\nbe categorized into the following non-exclusive\ncategories:\n\u2022Loaders : Load datasets from the Hugging Face1\nhub or from local sources.\n\u2022Selectors : Filter examples, shuffle datasets, and\nselect subset datasets.\n\u2022Retrievers : Integrate information from external\ndatabases, tools, libraries and pipelines.\n\u2022Samplers : Collect random examples or features\nfrom any dataset to compile few-shot or negative\nexamples.\n\u2022Prompters : Format prompts using custom tem-\nplates and keyword mappings.\nThe processing module supports the handling of\nmultiple datasets at once, through global dataset\nsharing. This feature allows each step of the\npipeline to access any of the loaded datasets, en-\nhancing flexibility and allowing for complex pro-\ncessing procedures. Furthermore, the module in-\ncludes step caching , which caches each pipeline\nstep locally. This improves compute efficiency, and\nfacilitates easy reproduction of results.\n3.1.1 Example: Enhancing a Q&A Dataset\nTo showcase the effectiveness of the process-\ning module, we demonstrate how to enrich a\nquestion-answering dataset with external informa-\n1https://huggingface.co/model:\n_target_: ragfoundry.models.hf.HFTrain\nmodel_name_or_path:\n\"microsoft/Phi-3-mini-128k-instruct\" ,\u2192\nload_in_8bit: true\nlora:\npeft_type: \"LORA\"\nr:16\ntarget_modules: [ \"qkv_proj\" ]\ncompletion_start: \"<|assistant|>\"\ntrain:\ngradient_accumulation_steps: 4\nlearning_rate: 2e-05\nlr_scheduler_type: \"cosine\"\nnum_train_epochs: 1\noptim: \"paged_adamw_8bit\"\ninstruction: prompts/prompt_instructions/qa.txt\ndata_file: TQA_train_processed.jsonl\nListing 2: Example of a training configuration. Model\nand training parameters are specified, in addition to an\ninstruction file containing the system prompt.\ntion fetched using a retrieval pipeline, prepare few-\nshot examples and combine everything together\nusing a prompt template. Listing 1 demonstrates\nhow such a processing pipeline is defined using a\nYAML configuration. The main structure of the file\nis a list of steps, each defined by a _target_ which\npoints to the step implementation. Each step has\ninputs , which is a name or list of dataset names\nto act upon. Other keys in a step relate to specific\nstep logic.\nThe first two steps in listing 1 load datasets from\nHugging Face hub and from a local path. The third\nstep shuffles and selects 10k examples from the\nmain dataset. The forth step runs a Haystack-based\n(Pietsch et al., 2019) retrieval pipeline to retrieve\nrelevant passages using questions from the loaded\ndataset as queries, storing them in docs_key . We\nnote that different retrieval processes or frame-\nworks (Liu, 2022; Chase, 2022; Lin et al., 2021)\ncan be used in retrieval steps. The fifth step selects\n3 few-shot examples from the secondary dataset,\nfollowing a prompt generator step that loads a\nprompt template and replaces all given informa-\ntion according to the defined mapping dictionary.\nLastly, the dataset is saved to a local path.\n3.2 Training\nWe provide a training module to fine-tune models\ngiven the datasets created by the previous process-\ning module. The training module relies on the\nwell established training framework TRL2and sup-\n2https://github.com/huggingface/trlmodel:\n_target_: ragfoundry.models.hf.HFInference\nmodel_name_or_path:\n\"microsoft/Phi-3-mini-128k-instruct\" ,\u2192\nload_in_8bit: true\ninstruction: prompts/prompt_instructions/qa.txt\nlora_path: /path/to/adapter\ngeneration:\ndo_sample: false\nmax_new_tokens: 50\nreturn_full_text: false\ndata_file: my-processed-data.jsnol\ngenerated_file: model-predictions.jsonl\nListing 3: Example of an inference configuration. In ad-\ndition to model and generation options, a system prompt\ncan be defined.\nports advanced and efficient training techniques,\ne.g. LoRA (Hu et al., 2021). An example of a\ntraining configuration is presented in listing 2.\n3.3 Inference\nTheinference module generates predictions given\nthe processed datasets created by the processing\nmodule. Inference is conceptually separated from\nthe evaluation step, since it is more computation-\nally demanding than evaluation. Additionally, one\ncan run multiple evaluations on a single, prepared\ninference results file. An example configuration for\ngenerating predictions given a dataset is presented\nin listing 3.\n3.4 Evaluation\nThe goal of the framework is augmenting LLMs\nfor RAG. The evaluation module allows users to\nrun collections of metrics to evaluate RAG tech-\nniques and tuning processes. The evaluation mod-\nule loads the output of the inference module and\nruns a configurable list of metrics. Metrics are\nclasses implemented in the library. These classes\ncan be as simple as wrappers around other evalua-\ntion libraries, or can be implemented by the user.\nLocal metrics can be run on individual examples,\nlike Exact Match (EM), while Global metrics run\non the entire dataset as a whole, e.g. Recall (for\nclassification-based metrics). Metrics can use any\nfield and metadata in the dataset, not just the input-\noutput pairs. Some of the metrics implemented\nin the library include: a wrapper for the Hugging\nFace evaluate library, EM, F1, classification met-\nrics, BERTScore (Zhang et al., 2019), Semantic\nSimilarity and a wrapper for DeepEval3(for using\n3https://github.com/confident-ai/deepevalanswer_processor:\n_target_: ragfoundry.processing.RegexAnswer\ncapture_pattern: \"Answer: (.*)\"\nstopping_pattern:\nmetrics:\n- _target_: ragfoundry.evaluation.HFEvaluate\nmetric_names: [ \"rouge\" ]\n- _target_: ragfoundry.evaluation.EM\n- _target_: ragfoundry.evaluation.F1\n- _target_: ragfoundry.evaluation.BERTScore\nmodel: \"microsoft/deberta-large-mnli\"\n- _target_: ragfoundry.evaluation.Faithfulness\n- _target_: ragfoundry.evaluation.Relevancy\nembeddings: \"BAAI/bge-small-en-v1.5\"\nresults_file: my-evaluation.yaml\ngenerated_file: model-prediction.jsonl\ndata_file: my-processed-data.jsonl\nListing 4: Example of an evaluation configuration; it\ncontains an answer processor, as well as the list of met-\nrics, with optional parameters, to run.\nthe RAGAS metrics (Es et al., 2024)). After the\nevaluation is completed, a results file is written to\ndisk with the local and global metrics results.\nFurthermore, the evaluation module uses a pro-\ncessing step called an Answer Processor , which\ncan implement custom logic and serve many pur-\nposes, including cleaning and aligning outputs; for\nexample, using regex, one can isolate answers, re-\nmove stop words, chain-of-thought reasoning, de-\nfine a stopping criteria, process citations and attri-\nbutions and any other form of processing needed\nfor a given evaluation.\nSee listing 4 for a configuration example; it con-\ntains an answer processor that extracts an answer\nfrom an output, and a list of metrics to run.",
        "4 Experiments: RAG Tuning\nTo illustrate the usage and usefulness of the\nRAG F OUNDRY library, we experiment with sev-\neral possible RAG improvements to LLMs, and\nevaluate the results on three knowledge-intensive\ntasks.\n4.1 RAG Augmentation Techniques\nWe explore several techniques for RAG augmenta-\ntion, and use RAG F OUNDRY to easily implement\nand evaluate their benefit. As an initial step, we\nevaluate unmodified models; we set Baseline as a\nconfiguration that is defined by running unmodified\nmodels and without any external knowledge. We\ndefine a RAG setting that introduces top-relevant\ndocuments in a consistent prompt template format\nwith a system instruction, and a CoT scheme whichguides the model to use the retrieved context, ex-\nplain the steps, quote relevant parts and produce\na final answer. Complementing that, we explore\nfine-tuning recipes. We fine-tune the model in the\nRAG setup and denote is as RAG-sft . To comple-\nment CoT , we implemented a fine-tuning recipe,\ndenoted as CoT-sft , introduced in (Zhang et al.,\n2024), where gold documents and purely distractor\ndocuments are used in the prompt, determined by\nprobability, in conjunction with a CoT prompt. All\nprompt templates are included in appendix A.1.\n4.2 Datasets\nWe evaluate our models on TriviaQA (Joshi et al.,\n2017), PubmedQA (Jin et al., 2019), and ASQA\n(Stelmakh et al., 2022) which are knowledge in-\ntensive question-answering datasets which ben-\nefit from external sources. The TriviaQA and\nPubmedQA datasets contain relevant context; for\nASQA, retrieval was done over a Wikipedia corpus\nusing a dense retriever4. Dataset sources and sizes\nare included in appendix A.2.\n4.3 Models\nWe experiment with two representative models:\nLlama-35(Touvron et al., 2023; AI@Meta, 2024)\nand Phi-36(Abdin et al., 2024) as they represent\nrobust capabilities and are ideal candidate models\nfor RAG use case deployments.\n4.4 Evaluation\nWe measure and report Exact Match (EM) for\nTriviaQA , STR-EM for ASQA, accuracy and F1\nfor PubmedQA. Additionally, we evaluate two\nRAGAS metrics (Es et al., 2024): Faithfulness and\nRelevancy. Faithfulness measures the relation be-\ntween the generated text and the context. Relevancy\nmeasures the relation between the generated text\nand the query. These two metrics use the context as\ninput for the LLM critic, so are only relevant in the\nRAG settings. The critic LLM used is GPT4-32k,\nversion 0613. An embedder7is required for the\nrelevancy evaluation.\n4.5 Results\nWe present a comparative study of RAG augmenta-\ntion techniques, on the TriviaQA, ASQA and Pub-\nmedQA datasets. Results are presented in table 1:\n4BAAI/llm-embedder\n5meta-llama/Meta-Llama-3-8B-Instruct.\n6microsoft/Phi-3-mini-128k-instruct.\n7BAAI/bge-small-en-v1.5.Model Method TriviaQA ASQA PubmedQA\nEM Faith. Rel. STR-EM Faith. Rel. Acc F1 Faith. Rel.\nPhi-3 3.8BBaseline 0.630 - - 0.109 - - 0.476 0.290 - -\nRAG 0.876 0.821 0.836 0.294 0.685 0.895 0.530 0.281 - -\nRAG-sft 0.878 0.777 0.750 0.252 0.717 0.833 0.720 0.491 - -\nCoT 0.923 0.555 0.741 0.367 0.263 0.826 0.574 0.439 0.477 0.705\nCoT-sft 0.795 0.793 0.749 0.386 0.749 0.839 0.620 0.458 0.631 0.853\nLlama-3 8BBaseline 0.722 - - 0.200 - - 0.560 0.366 - -\nRAG 0.828 0.783 0.746 0.285 0.610 0.861 0.556 0.398 - -\nRAG-sft 0.916 0.704 0.714 0.291 0.653 0.854 0.770 0.537 - -\nCoT 0.896 0.518 0.764 0.395 0.536 0.730 0.684 0.480 0.378 0.732\nCoT-sft 0.851 0.808 0.697 0.422 0.768 0.790 0.694 0.485 0.777 0.883\nTable 1: Evaluation results of baseline and different RAG settings, for the three datasets and two models tested. In\naddition to the main metrics for each dataset, faithfulness and relevancy are reported for the relevant configurations.\nIn bold are the best configurations per dataset, based on the main metrics.\nmain metrics for each dataset are displayed, as well\nas faithfulness and relevancy scores, as defined in\n(Es et al., 2024). For TriviaQA we observe the\nfollowing: retrieved context improves the results,\nfine-tuning the RAG setting improves the results,\nfine-tuning on CoT reasoning (which includes train-\ning on a combination of gold passages and distrac-\ntor passages) decreases performance. Best method\nis model dependent for this dataset. For ASQA,\nwe similarly observe every method improves upon\nthe baseline, CoT reasoning produces consistent\nimprovement in both models, as well as fine-tuning\nof the CoT configuration, which shows to perform\nbest. Finally, for PubmedQA, we observe that al-\nmost all methods improve upon the baseline (with\none exception); CoT reasoning improves upon the\nuntrained RAG setting, but upon fine-tuning, the\nRAG method appears to perform best in both mod-\nels.\nInspecting the faithfulness and relevancy scores,\nnotice that not all configurations are valid to be\nmeasured: these metrics require context, so are\nirrelevant for the baseline method. Additionally,\nin the PubmedQA dataset, the answers are binary\nYes/No; only in the CoT configurations the LLMs\nproduce a reasoning, which can be evaluated. Fi-\nnally, the faithfulness and relevancy scores often\ndo not correlate with the main metrics, neither with\neach other, possibly indicating they capture differ-\nent aspects of the retrieval and generated results,\nand represent a trade-off in performance.\nThe results demonstrate the usefulness of RAG\ntechniques for improving performance, as well as\nthe need to carefully evaluate different aspects of a\nRAG system, on a diverse set of datasets, as effort\non developing generalized techniques is ongoing.5 Conclusion\nWe introduced RAG F OUNDRY , an open-source\nlibrary dedicated to the task of RAG-augmentation\nof LLMs, namely fine-tuning LLMs to become bet-\nter at RAG settings. The library is designed to serve\nas an end-to-end experimentation environment, en-\nabling users to quickly prototype and experiment\nwith different RAG techniques. We demonstrated\nthe usefulness of the library by augmenting two\nmodels with RAG configurations, evaluating on\nthree Q&A datasets and showing the benefit of\nRAG techniques, as well as of using multi-aspect\nmetrics relevant for RAG systems evaluation.\nLimitations and Future Plans\nOur hope is that the library will be useful to as\nmany people and use-cases as possible. However,\ndue to time and resource constraint, we were able to\ndemonstrate its usefulness on a subset of tasks and\ndatasets. Future work can expand the evaluation\nto other tasks, as well as implementing other RAG\ntechniques and evaluations.\nAlthough we designed the library to be general\nand customizable, there might be specific work-\nflows which will be difficult to run as-is and some\ncode changes may be required. The library proved\nuseful for our own research projects on a diverse\nset of datasets and tasks and extending it is easy\nand straightforward.\nFinally, despite our best efforts to offer detailed\ndocumentation in the library, there could be some\nmissing details regarding some functionality or spe-\ncific use-cases. The code repository will accept\nsuggestions, bug-fixes and pull requests.Ethics Statement\nIn conducting our research we strive abiding to\nthe highest ethical standards, including integrity,\nfairness, and societal benefit of our work. We pri-\noritized data privacy and security throughout our\nresearch; any data used in our experiments was\npublicly available and did not contain any private\ninformation. We are committed to the principles of\ntransparency and reproducibility; the methodolo-\ngies, including data pre-processing, model training,\nand evaluation are documented in order to enable\nothers to replicate our findings. Code is made avail-\nable in an open repository. We advocate for the\nresponsible use of LLMs and RAG augmentation.\nIt is essential to exercise caution and verify the ac-\ncuracy and reliability of generated text produced by\nLLMs. Hallucinations can have negative implica-\ntions, and even when RAG methods can ameliorate\nsome of these aspects, verification and inspections\nare needed.",
        "1 Introduction\nLarge Language Models (LLMs) have emerged as\na transformative force in the field of AI, demon-\nstrating an impressive ability to perform a wide\nrange of tasks that traditionally required human in-\ntelligence (Brown et al., 2020; Kojima et al., 2022).\nDespite their impressive capabilities, LLMs have\ninherent limitations. These models can produce\nplausible-sounding but incorrect or nonsensical an-\nswers, struggle with factual accuracy, lack access\nto up-to-date information after their training cutoff\nand struggle in attending to relevant information in\nlarge contexts (Huang et al., 2023; Liu et al., 2023).\nData\nTrainingLoRA\uf085\nInference\uf11c\nLoadersAugmentation\nSelectorsRetrieversSamplersPromptersCachingAPI\nEvaluationEM\uf00cF1FaithfulnessRelevancyAnswer ProcessorROUGE\nFigure 1: An overview of the RAG F OUNDRY frame-\nwork: the Data Augmentation module persists RAG\ninteractions into a dedicated dataset, which is then used\nfor training, inference and evaluation.\nRetrieval-Augmented Generation (RAG) enhances\nLLMs performance by integrating external infor-\nmation using retrieval mechanisms. Combining re-\ntrieval that leverages vast knowledge-bases outside\ntheknowledge of the model, effectively addresses\nknowledge limitations, can reduce hallucinations,\nimprove the relevance of generated content, pro-\nvide interpretability and could be vastly more cost-\nefficient (Lewis et al., 2021; Mallen et al., 2022;\nGao et al., 2023; Asai et al., 2023; Borgeaud et al.,\n2021; Peng et al., 2023; de Jong et al., 2023). Fur-\nthermore, recent research indicates that fine-tuning\nLLMs for RAG can achieve state-of-the-art perfor-\nmance, surpassing that of larger, proprietary mod-\nels (Yu et al., 2024b; Liu et al., 2024).\nHowever, the implementation of RAG systems\nis inherently complex and requires a series of\nintricate decisions that can significantly impact\nthe performance of the system. This process de-arXiv:2408.02545v1  [cs.CL]  5 Aug 2024mands a thorough understanding of the data and\nuse case, and often, solutions do not generalize\nwell to other domains (Barnett et al., 2024; Bala-\nguer et al., 2024). Some key RAG design decisions\ninclude text embedding, indexing parameters, re-\ntrieval algorithms, query building, and prompt de-\nsign, among other considerations beyond the LLM\nconfiguration (Wang et al., 2024). Another issue is\nreproducibility: achieving consistent and compara-\nble results across runs, datasets and tasks. Varia-\ntions in training data, pre-processing steps, model\nconfigurations, and hardware can lead to discrep-\nancies in performance, making it challenging for\nresearchers and practitioners to replicate findings\nand build upon previous work. Additionally, evalu-\nating RAG systems presents a challenge due to the\ndual reliance on retrieval accuracy and generative\nquality. These systems require a sophisticated eval-\nuation suite that accounts for the interplay among\nthe retrieved information, the formalization of data,\nand the generated output (Chen et al., 2023; Yu\net al., 2024a; Es et al., 2024).\nWe introduce RAG F OUNDRY , an open-source\npython framework for developing sophisticated\nretrieval-augmented LLMs for RAG use-cases. The\nlibrary supports researchers and practitioners in the\nnuanced task of enhancing the capabilities of LLMs\nin RAG use cases. It is highly customizable, fa-\ncilitating rapid prototyping and experimentation\nacross all aspects of RAG, including data selec-\ntion, aggregation and filtering, retrieval, text pro-\ncessing, document ranking, few-shot generation,\nprompt design using templates, fine-tuning, infer-\nence, and evaluation. To cater to the specific needs\nof researchers, we designed the framework to func-\ntion as an end-to-end experimentation environment.\nThe backbone of the library consists of four dis-\ntinct modules: data creation, training, inference,\nand evaluation. Each module is encapsulated and\ncontrolled by a configuration file, ensuring compat-\nibility between the output of one module and the\ninput of the next. This modular approach allows\neach step to be isolated and independently experi-\nmented with, enabling the production of multiple\noutputs and the concurrent execution of numerous\nexperiments. Evaluation can be conducted on the\ngenerated outputs as well as on any feature within\nthe data, including retrieval, ranking, and reason-\ning.\nTo illustrate the utility of the framework, we\nconducted experiments involving retrieval, fine-\ntuning, chain-of-thought (CoT) reasoning (Wuet al., 2023) and a negative distractor-documents\ntechnique (Zhang et al., 2024). We compared\ntwo widely accepted baseline models using vari-\nous enhancement methods across three knowledge-\nintensive question-answering tasks, demonstrating\nthe effectiveness of RAG F OUNDRY .",
        "7 Conclusion\nBy comparing the results on four representative\nlanguages on three downstream tasks, we demon-\nstrated the challenges faced in applying natural lan-\nguage processing techniques to ancient logographic\nwriting systems. Our experiments demonstrate,\nhowever, that encoding more readily available vi-\nsual representations of language artifacts can be\na successful strategy for downstream processing\ntasks.8 Limitations\nMore discussion on ancient logographic lan-\nguages. Due to page limits, we do not discuss an-\ncient logographic languages in a critical way. Tech-\nnically, there are no logographic languages, only\nlanguages written in logographic writing systems\n(aka logography ) (Gorman and Sproat, 2023). In\nthis paper, we use the term \u201clogographic languages\u201d\nto denote languages that are quite different from\nthose with alphabetic writing systems especially\nwhen we tried to apply NLP toolkits for compu-\ntational paleography. As mentioned in the related\nwork section, these languages feature glyphs that\nhave multiple transliterations or functional uses. In\nother words, these languages are homophonous or\na glyph can be used as a phonetic value or semantic\nvalue. Therefore, the boundaries between logo-\ngraphic and phonographic is not sharply separated.\nIncluding more logographic writing systems.\nWe selected the four languages because we would\nlike to include at least one language from early civ-\nilization in Ancient China, Ancient Egypt, Indus\nValley Civilization, Mesoamerica, Mesopotamia\nand Minoan Civilization (Woodard, 2004). How-\never, we fail to include Mayan hieroglyphs\n(Mesoamerica) and Oracle Bone script. However,\nMayan is excluded because the collection12is still\nworking in process. Oracle Bone script is primarily\nomitted due to copyright issues.\nTextline images. Most ancient languages remain\nas full-document images. In this paper, we use dig-\nitally rendered text as a surrogate visual feature for\nAkkadian. In reality, much of Cuneiform data is\nstill in hand copies or in photo format. In the future,\nwe look to conduct apples-to-apples comparisons\nfor all languages once the line segmentation anno-\ntations become available.\nAnnotation quality and quantity. The study\nof ancient languages is constantly evolving; hu-\nmanities scholars have not agreed on explanations,\ntransliterations, or even the distinctions between\ncertain glyphs or periods. We try our best to care-\nfully annotated the data without bias; however, fu-\nture editions of the benchmark are needed as things\nchange all the time. A collective platform to cor-\nrect errors and make more data available should be\nconsidered for future development.\n12The Maya Hieroglyphic Text and Image Archive:\nhttps://digitale-sammlungen.ulb.uni-bonn.\nde/mayaLabel imbalance. The classification task in our\nbenchmark is label imbalanced. This is known\nto be a major issue for all machine learning tasks\nrelated to the ancient world (Sommerschield et al.,\n2023; Chen et al., 2024).\nAcknowledgements\nWe thank Professor Wenbo Chen from the Depart-\nment of Humanities at Central South University,\nChina, for his advice on Old Chinese data collec-\ntion and explanation. We thank Professor Edward\nKelting from the Department of Literature at UC\nSan Diego for his advice on Ancient Egyptian data\ncollection and explanation. We thank Jerry You\nfrom http://www.ccamc.org/ for his help\non Unicode and data processing for ancient lan-\nguages.\nWe thank Elizabeth Salesky for her guidance in\nsetting up cross-lingual machine translation experi-\nments for ancient languages using both PIXEL and\nBPE encoders. We thank Chenghao Xiao for the\nhelp setting up the PIXEL + GPT 2 experiment.\nWe thank Kyle Gorman, Alexander Gutkin and\nRichard Sproat for their inspiring work (Sproat and\nGutkin, 2021; Gorman and Sproat, 2023), which\nhas significantly contributed to our understanding\nof logographic writing systems from a computa-\ntional perspective.\nWe thank Nikita Srivatsan, Nikolai V ogler, Ke\nChen, Daniel Spokoyny, David Smith, and the\nanonymous ARR reviewers for their insightful feed-\nback on the paper draft. This work was partially\nsupported by the NSF under grants 2146151 and\n2200333.",
        "6 Related work\nBecause ancient languages are often low-resource,\nthey present challenges that are closely related to\nother domains of NLP, such as low-resource ma-\nchine learning and multi-lingual transfer learning.\nRecent work has explored the application of NLP\ntechniques to ancient languages from the following\nperspectives:\nMultilingual transfer learning and disjoint char-\nacter sets. Muller et al. (2020) studied hard-to-\nprocess living languages such as Uyghur, and re-\nported that a non-contextual baseline outperforms\nall pre-trained LM-based methods. Ancient lan-\nguages also face the same problem, with even less\ndata available. A major challenge that is mostly\nspecific to ancient logographic languages, however,\nis the almost non-existent overlap of their symbol\ninventories with those of high-resource languages.\nVisual representation of languages. Recently,\nseveral works have studied language processing\nbased on images of text. Rust et al. (2023) pre-\ntrained a masked language model on digitally ren-\ndered text and achieved comparable performance\nwith text-based pre-training strategies on down-\nstream tasks. Salesky et al. (2023) found that a\nmulti-lingual translation system with pixel inputs\nwas able to outperform its textual counterpart.\nMachine learning for ancient languages. Som-\nmerschield et al. (2023) surveyed the status of\npipelines for ancient language processing. Notably,\nthe study concludes that applying machine learning\nmethods to ancient languages is bottlenecked by the\ncost of digitization and transcription. According to\nthe Missing Scripts Project,11only 73 of 136 dead\nwriting systems are encoded in Unicode. Ancient\n11https://worldswritingsystems.org/languages, such as Ancient Greek or Latin (Bam-\nman and Burns, 2020), benefit greatly from mul-\ntilingual pre-training techniques, such as mBERT,\nXLM-R (Conneau et al., 2020), and BLOOM (Scao\net al., 2022). The applicability of these techniques\nis limited when it comes to languages that were\nhistorically written in obsolete or extinct writing\nsystems\u2014for instance, languages like Sumerian\nand Elamite were recorded in Cuneiform script and\nancient Chinese was inscribed on oracle bone or\nbamboo. However, observations by existing work\nsupport the potential utility of visual processing\npipelines for ancient languages.\nLogographic writing systems. Logography typi-\ncally denotes a writing system in which each glyph\nrepresents a semantic value rather than a phonetic\none, however, all the languages studied in our paper\nhave at least some phonetic component based on\nthe rebus principle. This paper emphasizes ancient\nlogographies that (i) possess extensive glyph in-\nventories; (ii) have feature glyphs with ambiguous\ntransliterations or functional uses; and (iii) are low-\nresource with much of data remaining in photo for-\nmats (Caplice et al., 1991; Allen, 2000; Woodard,\n2004). Existing research on logographic languages\nhas predominantly focused on those well-resourced\nand still in use, such as Modern Chinese (Zhang\nand Komachi, 2018; Si et al., 2023), or used data\nthat has already been carefully transcribed into\nLatin or annotated with extra semantic informa-\ntion (Wiesenbach and Riezler, 2019; Gutherz et al.,\n2023; Jiang et al., 2024). Our paper aims to ad-\ndress the gap in resources (by proposing new data)\nand methodologies (by adapting visual-only ap-\nproaches) for encoding and analyzing ancient logo-\ngraphic languages, leading to more comprehensive\nunderstanding of historical linguistic landscapes.",
        "4 Experiments and Analysis\nWe describe our general model fine-tuning ap-\nproach in \u00a74.1 and analyze model performance\non the aforementioned tasks in the succeeding sub-\nsections.\n4.1 General Experimental Setup\nWe use the Huggingface Transformers library\n(Wolf et al., 2020) in all experiments, except for ma-\nchine translation, where we use the PIXEL-MT and\nBPE-MT models.9We modified code and model\ncheckpoints provided by Salesky et al. (2023) based\non fairseq (Ott et al., 2019) for the two exceptions.\nWe use Adam (Kingma and Ba, 2015) as the op-\ntimizer for all models, with an initial learning rate\nspecified in Table 3. We use early stopping when\nthe validation loss fails to improve for ten evalua-\ntion intervals (1000 iteration per interval). For data\nwithout a standard test set, we run a fixed number\nof training iterations and report the performance\non the validation set after the last iteration. All\nexperiments are conducted on an NVIDIA-RTX\nA6000 GPU, and the training time ranges from 2\nminutes to 50 hours, depending on the nature of\nthe task and the size of the datasets. Unless other-\nwise specified, all parameters, including those in\npre-trained models, are trainable without freezing.\nWe summarize other configurations in Table 3.\n4.2 Machine Translation\nWe compare the performance of the models on ma-\nchine translation, where we translate ancient Egyp-\ntian (EGY), Akkadian (AKK), and Old Chinese\n(ZHO) into English (Table 4a). We find that the\nPIXEL-MT model consistently achieves the best\n9The prefix PIXEL or BPE also indicates the type of input\nrepresentation the model uses.BLEU score across the three languages, outper-\nforming the second-best method by a large margin.\nModels with pre-training do not always outper-\nform those trained from scratch (Gutherz et al.,\n2023). We find that all models that take textual\n(Unicode or latinized) input achieve worse perfor-\nmance than models trained from scratch with the\nsame type of textual input, suggesting that the lack\nof overlap in symbol inventories poses a serious\nproblem for cross-lingual transfer learning. Our\nresults indicate that choosing the correct input for-\nmat is crucial to achieving the full advantage of\npre-training.\nIn addition, the PIXEL-MT model, pre-trained\non paired data in modern languages (TED59), sig-\nnificantly outperforms PIXEL + GPT2 (pre-trained\nwith masked language modeling) across the board.\nAnother model, BERT-MT, which is further pre-\ntrained on the same parallel text (TED59) with\nBERT initialization, also achieves comparable per-\nformance. These results emphasize the importance\nof pre-training on modern paired data, empirically\nsuggesting that the PIXEL encoder with parallel\ntext pretraining is an effective combination for an-\ncient logographic language translation.\n[Pred] Confucius said: Those who lead the people will be good at holding on to the superior.[Ref ]  Confucius said: Those above are fond of \"benevolence\", \u2026[Pred] At the beginning of my kingship, in my first regnal year, in the fifth month when I sat on the royal throne, (the god) Assur, my lord, encouraged me and I gave (them) to the Hamranu, the Luhutu, Hatalu, Rapiqu, Rapiqu, Rapiqu, Nasiru, Gulasi, Nabatu, \u2026[Ref ] At the beginning of my reign, in my first palu, in the fifth month after I sat in greatness on the throne of kingship, (the god) Assur, my lord, encouraged me and I marched against (the Aramean tribes) Hamaranu, Luhu`atu, Hatallu, Rubbu, Rapiqu, Hiranu, (5) Rabi-ilu, Nasiru, Gulusu, Nabatu,[Pred] after Hes Majesty had as to the Shesmet who satisfies this August, Sopu, the Lord of the East. [Ref ] after His Majesty had come to Shesmet while satisfying this august god, Sopdu, the lord of the EastZHO-ENAKK-ENEGY-EN\nFigure 4: Case study for machine translation using the\nPIXEL-MT model. Notably, there are many spelling\nerrors in the predictions, particularly with uncommon\nnamed entities.\nQualitative analysis. As shown in Figure 4, the\nlow BLEU scores for ZHO-EN translation is a re-\nsult of the translation model failing to capture the\nmeaning of the input, instead focusing on repeated\nformatting queues: e.g., \u201c Confucius said:(a) Machine translation (BLEU score)\nModality Tokenization Input ModelPre-trained? Source Language\nMLM MT EGY AKK ZHO\nDataset size (# lines) 2,337 8,056 500\nVisual token-free textline PIXEL + GPT21! \u2717 2.83 7.51 1.14\nVisual token-free textline PIXEL-MT \u2717! 29.16 44.15 5.45\nTextual BPE w/ ext vocab2Unicode T5 ! \u2717 n/a 12.42 0.28\nTextual byte-level Unicode ByT5 ! \u2717 n/a 4.51 0.53\nTextual char-level Unicode Conv-s2s \u2717 \u2717 - 36.52\u2217-\nTextual BPE Unicode BPE-MT \u2717! 23.26 36.18 1.32\nTextual BPE Latin T5 ! \u2717 21.18 10.67 n/a\nTextual char-level Latin Conv-s2s \u2717 \u2717 - 37.47\u2217-\n(b) Attribute prediction (F 1accuracy)\nModality Tokenization Input Model LNA AKK EGY ZHO\ngeo time genre geo time genre\nNumber of classes 7 16 12 24 14 3\nDataset size (# examples) 772 36,454 36,454 36,454 1,320 302\nMajority 14.28 6.25 8.33 4.17 7.14 33.33\nVisual token-free photo ResNet 8.24 75.02 45.45 62.99 n/a n/a\nVisual token-free textline PIXEL 16.56 72.91 50.84 61.44 16.24 52.17\nTextual BPE w/ ext vocab2Unicode BERT n/a 0\u2217\u22170\u2217\u22170\u2217\u2217n/a 74.85\nTextual BPE Unicode BERT n/a 72.40 50.85 63.70 n/a 90.30\nTextual byte-level Unicode CANINE n/a 82.83 47.88 56.42 n/a 96.43\nTextual BPE Latin BERT 32.92 80.91 53.45 65.10 34.71 n/a\nTextual BPE Latin mBERT 50.52 83.08 56.71 66.33 36.25 n/a\nTable 4: (a) Results on machine translation (from each of the source languages to English), in terms of BLEU scores.\nMLM denotes models pretrained on unsupervised data with the masked language model (MLM) loss, while MT\ndenotes models pretrained with supervised parallel data (TED59). (b) Macro F 1scores for attribute prediction.\n\u2217: numbers taken from Gutherz et al. (2023), where their models are trained from scratch, i.e., without pretraining.\n\u2217\u2217: The character set is 100% disjointed without extending the vocabulary of the model, resulting in zero F 1\nscores.1: This model is trained using PIXEL as the encoder and GPT2 as the decoder, with linear projection\nlayers to convert the final layer of PIXEL into a prefix input for GPT2.2: This model is the only one experiencing\nout-of-vocabulary (OOV) issues with Unicode input. To address this, we extended the vocabulary with random\ninitialization. n/a: indicates the representation of a specific language does not exist in our benchmark.\nThose who... \u201d Indeed, given that the topical\ndomain of the ZHO-EN translation data is philo-\nsophical writing, achieving an accurate translation\nwould be challenging even with a much larger set\nof parallel translations. For AKK-EN, we found\nthat the overall quality to be quite good, despite the\nfact that errors in translating named entities appear\nmore often than in standard MT tasks. This case\nstudy suggests that translation performance could\nimprove further if we training using a custom target\nlanguage (English) vocabulary. We also show more\ngenerated examples from the PIXEL-MT model in\nAppendix C.\n4.3 Attribute Classification\nTable 4b summarizes the performance of attribute\nclassification with different features and models.\nAs expected, the image features can work fairlywell for some of these attribute classification tasks\nas many of the relevant features are visual (e.g., for\ntime and location); but, are not generally as effec-\ntive as textual input representations. By comparing\nBERT with latinized input and CANINE on Uni-\ncode, we find that when both accurate latinization\nand Unicode representations are available, latiniza-\ntion is the most informative feature\u2014with the ex-\nception of time period classification for Akkadian.\nThis exception is aligned with our understanding\nof Akkadian, as different Cuneiform characters are\nused across different time periods. Thus, in this\ncase, Unicode can provide more clues for deter-\nmining the time period of a sample. Note that the\nlabel distribution is not balanced for most ancient\nlanguage attribution tasks. For more details, refer\nto Chen et al. (2024).Modality Model Input RIAO MCONG\nDataset Size (# tokens) 5k 130k\nVisual PIXEL Image 92.74 85.22\nTextual BERT Latin 92.13 83.88\nTable 5: Dependency parsing result on Akkadian (eval-\nuated on the UD corpora RIAO and MCONG), in terms\nof labeled attachment scores (LAS). Note that the num-\nber of tokens are reported.\n4.4 Dependency Parsing\nWe compare the dependency parsing performance\nof models with visual and textual encoders (Ta-\nble 5).10While all models achieve quite high pars-\ning accuracy, we find that models with visual en-\ncoders perform the best on both investigated cor-\npora (RIAO and MCONG). During training, mod-\nels taking visual input generally converge faster\nthan their textual counterparts, which is in line with\nprior work (Salesky et al., 2023) that uses visual\nfeatures for machine translation.\n5 Ablation Study on OCR and Image\nQuality\nAs mentioned earlier, the majority of data from\nancient times remain in the form of photographs.\nWe first closely examine two different visual input\nrepresentations for the ZHO-EN translation task,\nhandcopied figure andphotograph (\u00a75.1). Next,\nwe examine OCR performance on ancient logo-\ngraphic languages to gain better understanding of\nthis bottleneck for current NLP pipelines (\u00a75.2).\n5.1 Handcopy v.s. Raw Image\nInput representation BLEU\nphotograph 2.09\nhandcopied figure 5.45\nTable 6: Performance on ZHO-EN translation using the\nPIXEL-MT model with different visual input features.\nFor the ZHO-EN translation data, we have ac-\ncess to both photographs of the bamboo slips and\nhandcopied textline figures (see the Bamboo script\nexample in Figure 3 for reference). As shown in Ta-\nble 6, the quality of the visual features significantly\ninfluences the translation accuracy\u2014translations\nderived from photographs yield a low BLEU score\n10We only conduct experiments on Akkadian since it is the\nonly language with off-the-shelf dependency annotations.of 2.09, whereas handcopied figures, which typ-\nically provide clearer and more consistent visual\ndata, result in a higher BLEU score of 5.45. This re-\nsult suggests that for models that perform implicit\nOCR as part of the translation process, the clarity\nof the source material is paramount.\n5.2 Text Recognition Study\nWe simplify the task of transcribing ancient texts\nby starting with lines of text that have been accu-\nrately segmented. For datasets that include glyph-\nlevel annotations, we employ glyph classification\nto recognize the text. Details on models and config-\nuration of line-level OCR andglyph classification\ncan be found in Appendix B.\nMethod Output LNA EGY AKK ZHO\nOCR Unicode 57.17 N/A 5.72 71.85\nOCR Latin 63.44 65.88 21.98 N/A\nTable 7: Line-level OCR results with the best valida-\ntion character error rate (CER) reported. The study\nincludes various writing systems using Kraken trained\nfrom scratch on segmented text lines. N/A: either the\nUnicode or Latin version of the text is not available.\n0 250 500 750 1000 1250\nNumber of glyphs |G|20406080Error Rate (%)\nglyph classification\nCTC loss101102103\nFreq count of glyph (log)\nFigure 5: Glyph classification on Old Chinese (ZHO).\nLeft axis : we plot the error rate of glyph classification.\nThe data point at |G| = 50 shows the classification error\ncalculated using the top 50 most frequent glyphs in the\ndataset. The purple horizontal line (71.85%) represents\nthe line-level text recognition CER for ZHO, provided\nfor reference. Right axis : The frequency count (in\norange bars) of each glyph in the dataset. Note that the\ncounts are in logarithmic scale, illustrating the long tail\ndistribution of glyph counts.\nResults. The line-level OCR performance for the\nfour languages is presented in Table 7. When com-\nparing digital renderings of text to handwritten sam-\nples, it is evident that Old Chinese (ZHO) achieves\na CER of 71.85, while Linear A has a CER of\n57.17. As shown in Figure 5, glyph classificationfor ZHO is approximately 20% less accurate than\nline-level OCR, indicating that contextual features\nsignificantly aid in recognizing glyphs. Further-\nmore, there is a rapid increase in error rate as the\nnumber of glyphs increases, highlighting the intrin-\nsic challenge of processing logographic languages,\nwhich typically have a large symbol inventories,\nand their frequency distribution often follows a\nlong-tail pattern (see the orange bars in Figure 5).\nTherefore, developing robust visual models that can\neffectively leverage visual features is crucial for im-\nproving NLP on ancient logographic languages.",
        "3 Methods\nIn this section, we will describe feature encoding\nmethods (\u00a73.1) for both visual and textual inputs,\nas well as task-specific layers (\u00a73.2) for each task\nwe consider.\n3.1 Feature Encoding\nNLP for Low-resource languages has benefitted\na lot from pre-trained models. However, modern\npre-trained models do not cover the character in-\nventories of the considered ancient logographic\nlanguages. To overcome this shortage, we summa-\nrize solutions to the problem into four categories\nand describe them as follows.\nExtending vocabulary. In this line of approach\n(Wang et al., 2020; Imamura and Sumita, 2022),\nthe vocabulary is extended by adding the unseen to-\nkens. The embeddings of new tokens can be either\ninitialized randomly or calculated by a function.! #   $ \nBamboo script (photo)\nEgyptian hierograph (digital figure)\nLinear A (montage)\n! # $ % & &Linear A (tablet)Cuneiform (digital rendering)Bamboo script (handcopy)\nNot in UnicodeStacking\nU1202d U121a0 U12038 U12197 U12363 U123631. raw image\n2. montage3. renderingconcatenate glyphs in a rowFigure 3: Image features of four ancient writing systems. (1) Egyptian hieroglyphs and Bamboo scripts are already\nmanually segmented into images of lines. In the handcopy version of the Bamboo script, the word within parentheses\nindicates the corresponding modern Chinese glyph. Although both the Egyptian and Bamboo script images appear\nto be in a digital font, they are only accessible as images without underlying codepoint mappings to Unicode. (2)\nLinear A tablets are believed to be written in horizontal lines running from left to right (Salgarella, 2020); therefore,\nwe use the montage concatenation of each glyph as the representation. (3) We digitally render Cuneiform Unicode\nusing computer font as the visual representation.\nIn the fine-tuning stage, the embeddings of new\ntokens are updated together with the rest of the\nmodel.\nLatin transliteration as a proxy. The majority\nof past work on cross-lingual transfer has focused\non using Latin transliteration as the proxy to trans-\nfer knowledge from high-resource to low-resource\nlanguages (Pires et al., 2019; Fang et al., 2020).\nFollowing this line of work, we input latinization\nrepresentations to mBERT (Devlin et al., 2018) to\nobtain the embeddings of the ancient languages.\nTokenization-free. The idea of the tokenization-\nfree approach is to view tokens as a sequence of\nbytes and directly operate on UTF-8 codepoints\nwithout an extra mapping step. As representa-\ntive models, ByT5 (Xue et al., 2022) and CA-\nNINE (Clark et al., 2022) use Unicode encod-\ning of a string to resolve the cross-lingual out-of-\nvocabulary issues. This work uses ByT5 for ma-\nchine translation and CANINE for classification.\nPixel Encoder for Text. Recently, there has been\na novel approach (Rust et al., 2023) that aims to re-\nsolve the disjoint-character-set problem by render-\ning text into images and then applying a standard\nimage encoder, such as the Vision Transformer\nwith Masked Autoencoder (ViT-MAE) (He et al.,\n2022), to encode the features. In this work, we use\nPIXEL (Rust et al., 2023), a pixel-based language\nmodel pre-trained on the Common Crawl dataset\nwith a masked image modeling objective, to encode\nthe visual text lines for ancient languages. Addi-\ntionally, we use PIXEL-MT (Salesky et al., 2023), a\npixel-based machine translation model pre-trained\non 59 languages, for the machine translation task.\nFull Document Image Encoding. When the im-\nages of ancient artifacts are available (e.g., for Lin-ear A and Cuneiform), we can encode the full-\ndocument images directly. We use ResNet-50\n(He et al., 2016) as the backbone model for full-\ndocument image inputs.\n3.2 Task-Specific Layers\nMachine translation. After encoding the input\nto vectors, machine translation requires a decoder\nto generate sequential outputs. Encoder-decoder\nmodels, such as T5 (Raffel et al., 2020), ByT5,\nPIXEL-MT, and BPE-MT (Salesky et al., 2023),\nuse 3/6/12 layers of Transformer blocks as the\ndecoders. For Encoder-only models, such as\n(m)BERT or PIXEL, we attach a GPT2 model (Rad-\nford et al., 2019) as the decoder to produce sequen-\ntial output. Among the aforementioned models, T5,\nByT5, and PIXEL are pre-trained on large-scale\ntext corpora such as the Common Crawl; PIXEL-\nMT and BPE-MT are pre-trained on 1.5M pairs\nof sentences of 59 modern languages; PIXEL-MT\nis an encoder-decoder model with a 6-layer Trans-\nformer encoder and a 4-layer Transformer decoder.\nClassification. We attach a two-layer ReLU-\nactivated perceptron (MLP) with a hidden size of\n512 to the encoder for all classification tasks. The\nMLP outputs the predicted distribution over the\ncandidate classes.\nDependency Parsing. After encoding, we use\nthe deep bi-affine parser (Dozat and Manning,\n2017) for dependency parsing, which assigns a\nscore to each possible dependency arc between two\nwords. We use the minimum spanning tree (MST)\nalgorithm during inference to find the best depen-\ndency tree for each sentence.Task Model BSZ Steps LR\ntranslation visual 64 30,000 5e-4\ntranslation textual 56 30,000 5e-4\ntranslation byT5 64 100,000 1e-3\nclassification visual/textual 256 30,000 5e-4\nparsing visual/textual 256 1,000 8e-5\nTable 3: Hyperparameter configuration. Note that,\nbyT5 is particularly hard to converge compared to other\ntransformer-based models. For the parsing task, due to\nthe low-resource nature of the parsing data, 1,000 steps\nare sufficient to achieve model convergence.",
        "Standard natural language processing (NLP)\npipelines operate on symbolic representations\nof language, which typically consist of se-\nquences of discrete tokens. However, creating\nan analogous representation for ancient logo-\ngraphic writing systems is an extremely labor-\nintensive process that requires expert knowl-\nedge. At present, a large portion of logographic\ndata persists in a purely visual form due to\nthe absence of transcription\u2014this issue poses\na bottleneck for researchers seeking to apply\nNLP toolkits to study ancient logographic lan-\nguages: most of the relevant data are images\nof writing . This paper investigates whether di-\nrect processing of visual representations of lan-\nguage offers a potential solution. We introduce\nLogogramNLP , the first benchmark enabling\nNLP analysis of ancient logographic languages,\nfeaturing both transcribed and visual datasets\nfor four writing systems along with annotations\nfor tasks like classification, translation, and\nparsing. Our experiments compare systems that\nemploy recent visual and text encoding strate-\ngies as backbones. The results demonstrate that\nvisual representations outperform textual rep-\nresentations for some investigated tasks, sug-\ngesting that visual processing pipelines may\nunlock a large amount of cultural heritage data\nof logographic languages for NLP-based anal-\nyses. Data and code are available at https:\n//logogramNLP.github.io/ .",
        "1 Introduction\nThe application of computational techniques to the\nstudy of ancient language artifacts has yielded ex-\nciting results that would have been difficult to un-\ncover with manual analysis alone (Assael et al.,\n2022). Unsurprisingly, one of the biggest chal-\nlenges in this domain is data scarcity, which, in\nturn, means that transferring from pre-trained sys-\ntems on well-resourced languages is paramount.\nHowever, it is more challenging to adopt similar\ntechniques for ancient logographic writing systems,\nTranslation\n     ? ? ? ? ? ? ?OCRUnicodeLatinPhotoHandcopyVisualTextualInputOutput\n?\u5f03\u616e???Parse TreeDating\u2026MConventionalVisual (novel)?Figure 1: Illustration of the processing flow of Old\nChinese (in Bamboo Script), an ancient logographic\nlanguage, best viewed in color. M denotes the pre-\ntrained model used in the pipeline. Vision-based models\ndirectly process visual representations (violet; dashed\nlines). Conventional NLP pipelines (blue; solid lines)\nfirst convert visual representations into symbolic text\u2014\neither automatically, which is quite noisy, or manually,\nwhich is labor-intensive. However, as shown, some\nancient logographic writing systems have symbol in-\nventories that have not yet been fully mapped into Uni-\ncode. Even when Unicode codepoints exist, they are\noften mutually exclusive with the symbol inventories of\nhigh-resource languages, reducing the effectiveness of\ntransferring from pre-trained models. Finally, latiniza-\ntion(a potential solution for finding common ground\nwith pre-training languages) loses information from the\noriginal input, is not fully standardized, and is difficult\nto automate.\nin which individual symbols represent entire se-\nmantic units like morphemes or words.\nThe challenges associated with NLP for ancient\nlogographic languages mainly come from two as-\npects. First, for many ancient languages, most\navailable data sources are in visual forms, consist-\ning of untranscribed photographs or hand-drawn\ncopies (i.e., lineart ). Adopting the conventional\nNLP pipeline, which requires converting visual\nrepresentations into symbolic text, is therefore not\nstraightforward: automatic transcriptions are often\nnoisy due to data scarcity, while manual transcrip-\ntions are labor-intensive and require domain exper-\ntise. Some logographic writing systems, such asarXiv:2408.04628v1  [cs.CL]  8 Aug 2024Old Chinese, even include symbol inventories that\nremain not fully mapped to Unicode (depicted in\nFigure 1).\nSecond, even when perfect Unicode transcrip-\ntions are available, their symbol inventories are of-\nten mutually exclusive with those of high-resource\nlanguages, which can substantially reduce the effec-\ntiveness of transfer from pre-trained multilingual\nencoders, such as mBERT (Devlin et al., 2018).\nOne processing step that might be used to mitigate\nthis issue is latinization of the Unicode transcripts\n(Rust et al., 2021; Muller et al., 2020). However,\nit is challenging to Latinize logographic languages\ndue to uncertain pronunciations (Sproat and Gutkin,\n2021) and the resulting inconsistent latinization\nschemes across artifacts from the same language\nand writing system. Such a process is laborious\u2014\nhumanists may devote months or even years to de-\ntermine the correct transliteration. In contrast, once\na correct transliteration is determined, translation\ninto another language may only take minutes.\nFortunately, advances in visual encoding strate-\ngies for NLP tasks offer an alternative solution.\nRecent studies have investigated NLP systems\nthat model text in the pixel space (Rust et al.,\n2023; Tschannen et al., 2023; Salesky et al., 2023),\nthereby opening new possibilities for the direct use\nof visual representations of ancient logographic\nwriting systems. These approaches, to date, have\nprimarily been applied to digitally rendered texts.\nThey have not yet been extensively evaluated on\nhandwritten texts, such as lineart , i.e., neatly hand-\ncopied versions of texts by scholars.\nIn this paper, we attempt to answer the following\nquestions: (1) Can we effectively apply NLP toolk-\nits, such as classifiers, machine translation systems,\nand syntactic parsers, to visual representations of\nlogographic writing systems? (2) Does this strategy\nallow for better transfer from pre-trained models\nand lead to better performance? Additionally, as\nshown in Figure 1, many logographic languages\nhave multiple partially processed representations,\nincluding artifact photographs, hand-copied lineart,\nUnicode, Latin transliteration, and normalization\u2014\nwe also aim to empirically investigate the extent to\nwhich various representations at each stage, includ-\ning textual and visual modalities, facilitate effective\nfine-tuning of downstream NLP systems.\nWe have curated LogogramNLP , a benchmark\nconsisting of four representative ancient logo-\ngraphic writing systems (Linear A, Egyptian hiero-glyphic, Cuneiform, and Bamboo Script), along\nwith annotations for fine-tuning and evaluating\ndownstream NLP systems on three tasks, including\nthree attribute classification tasks, machine transla-\ntion, and dependency parsing.\nWe conduct experiments on these languages and\ntasks with a suite of popular textual and visual\nencoding strategies. Surprisingly, visual repre-\nsentations perform better than conventional text\nrepresentations for some tasks (including machine\ntranslation), likely due to visual encoding allowing\nfor better transfer from cross-lingual pre-training.\nThese results highlight the potential of visual rep-\nresentation processing, a novel approach to ancient\nlanguage processing, which can be directly applied\nto a larger portion of existing data.\n2 Dataset: Languages, Tasks and\nChallenges\nOur benchmark consists of four representative an-\ncient languages\u2014Linear A, Egyptian hieroglyphic,\nCuneiform, and Bamboo script (\u00a72.1).1Each lan-\nguage is associated with a unique writing system\nand unique challenges. We refer the readers to\nAppendix A for data collection and cleaning de-\ntails. Our benchmark covers three tasks: machine\ntranslation, dependency parsing, and attribute clas-\nsification (\u00a72.2).\n2.1 Logographic Languages\nA major characteristic of logographic languages is\nthat the size of symbol inventories is significantly\nlarger than that in alphabetic languages such as\nAncient Greek (24 letters) or Modern English (26\nletters). A summary of different representations of\nthe languages of our interest is shown in Figure 2,\nand Table 2 summarizes the current status of each\nlanguage.\nLinear A. Linear A is an undeciphered language\nused by the Minoan at Crete and is believed to\nbe not related to ancient Greek. Scholars have\ndifferentiated the glyphs and carefully hand-copied\nthem into linearts. We collected a dataset of 772\ntablets (i.e., manually drawing) from SigLA.2Each\ntablet also has a separable glyph with annotated\nUnicode.\n1Bamboo scripts usually combine Seal scripts and Clerical\nscripts.\n2https://sigla.phis.me/browse.htmlWriting system Language abbr.Visual Feature Textual Feature Task\nFull Doc Textline Unicode latinization Translation UD Parsing Attribute\nLinear A Unknown LNA Y Y Y Y\nEgyptian hieroglyph Ancient Egyptian EGY Y Y Y Y\nCuneiform Akkadian & Sumerian AKK Y Y Y Y Y\u2217Y\nBamboo script Ancient Chinese ZHO Y Y Y Y\u2217Y\u2217\nTable 1: A summary of the task availability across four ancient languages with unique writing systems. The\nunderlined Yindicates that the data has not previously been used in a machine learning setup, which demonstrates\nthe novelty of our benchmark; and asterisks (\u2217) indicate that we conducted extra manual labeling.\nPhotographImage Textline \nlabor-intensity: \n!\n!\n!\nexpertise level: \n\u2b50\n\u2b50\n Unicode\n!\"#$%%&'O(\u7d55)-\u667a(\u77e5)-\u68c4-O(\u8fa9)Latinization_igi_ ka-bi-sum2sDm=f hnw m r n tA-wr hAkr grH n sDr.tCuneifromEgyptian HeiroglyphBamboo ScriptLinear A\nlabor-intensity: \n!\n expertise level: \n\u2b50\n\u2b50\n\u2b50\nlabor-intensity: \n!\n expertise level: \n\u2b50\nTranslationParse TreeDating\u2026\n! #   $ !\"#N/Aqe ra2 u ki ro *79 su Not within our proposed dataset\nFigure 2: Example of four logographic languages with different representation formats. The arrow shows the typical\nprocessing flow of ancient languages by humanists. The workload and expertise required to transcribe the text from\nimages is even greater than that of downstream tasks such as machine translation. The red circle O (in Bamboo\nScript) indicates the character is not digitized as Unicode yet. Green dashed boxes note that Unicode exists for\nEgyptian hieroglyphics and Linear A, but the alignment to documents is unavailable; the same goes for Egyptian\nand Linear A photographs.\nstatus LNA AKK EGY ZHO GRC\ndeciphered None Most Most Most All\ndifferentiated Most Most Most Most All\nencoded Most Most Some Some All\nLatinized All All All None All\nTable 2: Summary of the status of the ancient logo-\ngraphic languages presented in our paper. The status\nis measured from the perspective of paleography. We\nput Ancient Greek (GRC), a well-known ancient non-\nlogographic language, here for comparison.\nAkkadian (Cuneiform). CuneiML (Chen et al.,\n2023) is a dataset that contains 36k entries of\ncuneiform tablets. Each tablet consists of Unicode\nCuneiform, lineart, and transliteration. We also\nuse the Akkadian Universal Dependencies (UD)\ndataset (Luukko et al., 2020), which contains 1,845\nsentences with dependency annotations. Since the\nUD annotation of Akkadian only keeps the normal-\nization form of the language, we obtain the Uni-\ncode by running a dynamic programming-based\nmatching algorithm.\nAncient Egyptian (Hieroglyph). We segmented\nthe St Andrews Corpus (Nederhof and Berti,2015)3using a rule-based segmenter, and obtained\n891 examples of parallel data. Additionally, we col-\nlected data from the Thot Sign List (TSL; English\ntranslation)4and BBAW (German translation)5for\n2,337 and 100,736 samples of parallel data, respec-\ntively. However, the transliteration standards differ\namong these three sources of data, and BBAW does\nnot include hieroglyph image features. Therefore,\nwe only used TSL\u2019s data.\nOld Chinese (Bamboo script). We collected\n13,770 pieces of bamboo slips from Kaom,6which\ncome with the photograph of each line of the text.\nThe Baoshan collection covers three genres: Wen-\nshu (Document), Zhanbu (Divine), and Book. The\nGuodian collection contains parallel data translated\ninto modern Chinese. The vocabulary size is 1,303.\nNotably, about 40% of the characters do not have a\nUnicode codepoint and are, therefore, represented\nas place-holder triangles or circles. This dataset\n3https://mjn.host.cs.st-andrews.ac.uk/\negyptian/texts/corpus/pdf/\n4https://thotsignlist.org/\n5https://aaew.bbaw.de/tla/servlet/\nTlaLogin\n6http://www.kaom.netdoes not come with human-labeled latinization due\nto the lack of transliteration standards.\n2.1.1 Visual Representations\nSince ancient scripts did not consistently adhere to\na left-to-right writing order, breaking down multi-\nline documents into images of single-line text is\nnontrivial. These historical data, therefore, need\nadditional processing to be machine-readable. Fig-\nure 3 shows examples of different processing strate-\ngies. We summarize the approaches we used in\nbuilding the dataset as follows:\n1.Raw image (no processing) : the raw images\nare already manually labeled and cut into text\nlines of images, and no extra processing is re-\nquired.\n2.Montage : we generate a row of thumbnails of\neach glyph using the montage tool in ImageMag-\nick.7This strategy is used for Linear A, as the\noriginal texts are written on a stone tablet, and\nscholars have not determined the reading order-\ning of this unknown script.\n3.Digital rendering : we digitally render the text\nusing computer fonts when the language is al-\nready encoded in Unicode. Given that most\nancient logographic scripts are still undergoing\nthe digitization process, this option is currently\nunavailable except for Cuneiform.\n2.1.2 Textual Representations\nThe processing of textual features for ancient logo-\ngraphic scripts also requires special attention. Un-\nlike modern languages, ancient logographic writing\nsystems can have multiple latinization standards\nor lack universally agreed-upon transcription stan-\ndards. For example, the cuneiform parsing data\nis not in standard transliteration (ATF)8form, but\nrather, in the UD normalized form. This mismatch\nintroduces extra difficulty to downstream tasks, es-\npecially in low-resource settings.\nA similar issue also exists for Old Chinese: most\nancient characters do not even exist in the current\nUnicode alphabet. While we may find some mod-\nern Chinese characters that look similar to the an-\ncient glyphs, they are usually not identical, and\nsuch a representation loses information from the\noriginal text.\nFor Egyptian hieroglyphs, most characters are\n7https://imagemagick.org\n8ATF is a format used to represent cuneiform text.\nMore details can be found at http://oracc.ub.\nuni-muenchen.de/doc/help/encoded in Unicode, but there is no standard en-\ncoding for \u201cstacking\u201d multiple glyphs vertically\n(Figure 3). Therefore, we do not include the Uni-\ncode text for our ancient Egyptian data as they are\nnot available.\n2.2 Tasks\nOur benchmark covers three tasks (Table 1): trans-\nlation, dependency parsing, and attribute classi-\nfication. The model performance on these tasks\nreflects various aspects of ancient language un-\nderstanding. To better understand the information\nloss when using a pipeline approach, we also re-\nport performance using this method: predicting the\ntransliteration first and using the noisy predicted\ntransliteration for downstream tasks.\nMachine translation. The task is to translate the\nancient languages, represented by either text or im-\nages, into modern languages, such as English. In all\nof our experiments, we translate ancient languages\ninto English.\nDependency parsing. Given a sentence in the\nancient language, the task is to predict the depen-\ndency parse tree (Tesni\u00e8re, 1959) of the sentence.\nIn the dependency parse tree, the parent of each\nword is its grammatical head.\nAttribute classification. The task is to predict an\nattribute of the given artifact, for example, prove-\nnience (found place), time period, or genre.",
        "2 Related Work\nGenerative models. Recent advances in generative models, largely powered by diffusion mod-\nels [23\u201325], have enabled photo-realistic synthesis of images [ 26\u201328] and videos [ 29\u201331,20], and\nbeen extended to various other modalities [ 32,33]. The generation is mainly controlled by a text\nor image prompt. Recent works have explored ways to leverage these models\u2019 prior knowledge,\nvia either score distillation sampling [ 34\u201337] or fine-tuning on specialized data for downstream\napplications, such as multi-view images for 3D asset generation [38\u201343].\nVideo generation for motion. Attempts to model object motion often resort to pre-defined shape\nmodels, e.g., SMPL [ 1] for humans and SMAL [ 2] for quadrupeds, which are constrained to a single\nor only a few categories. Videos have been considered as a unified representation that can capture\ngeneral object dynamics [ 5]. However, existing video generators pre-trained on Internet videos\noften suffer from incoherent or minimal motion. Researchers have considered explicitly controlling\nvideo generation with motion trajectories. Drag-A-Video [ 44] extends the framework proposed by\nDragGAN [ 8] to videos. This method is training-free, relying on the motion prior captured by the\npre-trained video generator, which is often not strong enough to produce high-quality videos. Hence,\nother works focus on training-based methods, which learn drag-based control using ad-hoc training\ndata for this task. Early efforts such as iPoke [ 6] and YODA [ 45] train variational autoencoders\nor diffusion models to synthesize videos with objects in motion, conditioned on sparse motion\ntrajectories sampled from optical flow. Generative Image Dynamics [ 10] uses a Fourier-based motion\nrepresentation suitable for natural, oscillatory dynamics such as those of trees and candles, and\ngenerates motion for these categories with a diffusion model. DragNUWA [ 9] and others [ 11,16\u201318]\nfine-tune pre-trained video generators on large-scale curated datasets, enabling drag-based control\nin open-domain video generation. However, these methods do notallow controlling motion at the\nlevel of object parts, as their training data entangles multiple factors, including camera viewpoint and\nobject scaling and re-positioning, making it hard to obtain a model of part-level motion. Concurrent\nworks leverage the motion prior captured by video generative models for the related 4D generation\ntask [ 46\u201349]. These models, however, lack the capability of explicit dragging control, which we\ntackle in this work.",
        "We present Puppet-Master, an interactive video generative model that can serve as\na motion prior for part-level dynamics. At test time, given a single image and a\nsparse set of motion trajectories ( i.e.,drags ), Puppet-Master can synthesize a video\ndepicting realistic part-level motion faithful to the given drag interactions. This is\nachieved by fine-tuning a large-scale pre-trained video diffusion model, for which\nwe propose a new conditioning architecture to inject the dragging control effectively.\nMore importantly, we introduce the all-to-first attention mechanism, a drop-in\nreplacement for the widely adopted spatial attention modules, which significantly\nimproves generation quality by addressing the appearance and background issues\nin existing models. Unlike other motion-conditioned video generators that are\ntrained on in-the-wild videos and mostly move an entire object, Puppet-Master is\nlearned from Objaverse-Animation-HQ, a new dataset of curated part-level motion\nclips. We propose a strategy to automatically filter out sub-optimal animations\nand augment the synthetic renderings with meaningful motion trajectories. Puppet-\nMaster generalizes well to real images across various categories and outperforms\nexisting methods in a zero-shot manner on a real-world benchmark. See our project\npage for more results: vgg-puppetmaster.github.io .",
        "6 Conclusion\nWe have introduced Puppet-Master, a model that can synthesize nuanced part-level motion in the\nform of a video, conditioned on sparse motion trajectories or drags. Fine-tuned from a large-scale\npre-trained video generator on a carefully curated synthetic part-level motion dataset Objaverse-\nAnimation-HQ, which we have contributed, our model demonstrates excellent zero-shot generalization\nto real-world cases. Thanks to the proposed adaptive layer normalization modules, the cross-attention\nmodules with drag tokens and, perhaps more importantly, the all-to-first spatial attention modules, we\nhave shown superior results compared to previous works on multiple benchmarks. Ablation studies\nverify the importance of the various components that contributed to this improvement.\nAcknowledgments. This work is in part supported by a Toshiba Research Studentship, EPSRC\nSYN3D EP/Z001811/1, and ERC-CoG UNION 101001212. We thank Luke Melas-Kyriazi, Jinghao\nZhou, Minghao Chen and Junyu Xie for useful discussions, Dejia Xu for sharing his experience\ndeveloping CamCo [ 74], and RigManic, Inc. for providing the OpenAI credits essential for our\nresearch.",
        "1 Introduction\nWe consider learning an open-ended model of the motion of natural objects, which can understand\ntheir internal dynamics. Most models of dynamic objects are ad-hoc and only work for a specific\nfamily of related objects, such as humans or quadrupeds [ 1,2], severely limiting their generality.\nMore open-ended models like [ 3] do not use such constrained shape priors but are difficult to scale\ndue to the lack of suitable training data ( i.e., vertex-aligned 3D meshes). Therefore, we require a\nmore general framework to learn a universal model of motion. This framework must be flexible\nenough to model very different types of internal dynamics ( e.g., part articulation, sliding of parts, and\nsoft deformations). Furthermore, it must be able to tap substantial quantities of training data.\nRecently, video generators learned from millions of videos have been proposed as proxies of world\nmodels, i.e., models of any kind of natural phenomena, including motion. Such models may implicitly\nunderstand object dynamics; however, generating videos is insufficient: a useful model of object\ndynamics must be able to make predictions about the motion of given objects.\nInspired by DragAPart [ 4] and [ 5], we thus consider performing such predictions by learning a\nconditional video generator. This generator takes as input a single image of an object and one or more\ndrags which specify the motion of selected physical points of the object; it then outputs a plausible\nvideo of the entire object motion consistent with the drags (Fig. 1).\nSeveral authors have already considered incorporating drag-like motion prompts in image or video\ngeneration [ 6\u201318]. Many such works utilize techniques like ControlNet [ 19] to inject motion control\nin a pre-trained generator. However, these models tend to respond to drags by shifting or scaling anarXiv:2408.04631v1  [cs.CV]  8 Aug 2024DragNUWAOurs: Puppet-Master\n(a)(b)(c)(d)\nDragAnything\n(f)\n(e)\nOurs: Puppet-MasterFigure 1: Part-level dynamics vs.shifting or scaling an entire object. Puppet-Master generates\nvideos depicting part-level motion, prompted by one or more drags.\nentire object and fail to capture their internal dynamics (Fig. 1), such as a drawer sliding out of a\ncabinet or a fish swinging its tail. The challenge is encouraging generative models to synthesize such\ninternal, part-level dynamics.\nWhile DragAPart has already considered this challenge, its results are limited for two reasons. First,\nthe diversity of its training data is poor, as it primarily focuses on renderings of 3D furniture. Second,\nit starts from an image generator instead of a video generator. Consequently, it cannot benefit from\nthe motion prior that a video generator trained on a large scale may already have captured, and can\nonly capture the end frame of the motion.\nIn this work, we thus explore the benefits of learning a motion model from a pre-trained video\ngenerator while also significantly scaling the necessary training data to larger, more diverse sources.\nTo do so, we start from Stable Video Diffusion (SVD) [ 20] and show how to re-purpose it for motion\nprediction. We make the following contributions.\nFirst, we propose new conditioning modules to inject the dragging control into the video generation\npipeline effectively. In particular, we find that adaptive layer normalization [21] is much more\neffective than the shift-based modulation proposed by [ 4]. We further observe that the cross-attention\nmodules of the image-conditioned SVD model lack spatial awareness, and propose to add drag tokens\nto these modules for better conditioning. We also address the degradation in appearance quality\nthat often arises when fine-tuning video generators on out-of-distribution datasets by introducing\nall-to-first attention, where all generated frames attend the first one via modified self-attention. This\ndesign creates a shortcut that allows information to propagate from the conditioning frame to the\nothers directly, significantly improving generation quality.\nOur second contribution is data curation: we provide two datasets to learn part-level object motion.\nBoth datasets comprise subsets of the more than 40k animated assets from Objaverse [ 22]. These\nanimations vary in quality: some display realistic object dynamics, while others feature objects\n2that (i) are static, (ii) exhibit simple translations, rotations, or scaling, or (iii) move in a physically\nimplausible way. We introduce a systematic approach to curate the animations at scale. The resulting\ndatasets, Objaverse-Animation and Objaverse-Animation-HQ, contain progressively fewer animations\nof higher quality. We show that Objaverse-Animation-HQ, which contains fewer but higher-quality\nanimations, leads to a better model than Objaverse-Animation, demonstrating the effectiveness of the\ndata curation.\nWith this, we train Puppet-Master , a video generative model that, given as input a single image of an\nobject and corresponding drags, generates an animation of the object. These animations are faithful\nto both the input image and the drags while containing physically plausible motions at the level of\nthe individual object parts . The same model works for a diverse set of object categories. Empirically,\nit outperforms prior works on multiple benchmarks. Notably, while our model is fine-tuned using\nonly synthetic data, it generalizes well to real data, outperforming prior models that were fine-tuned\non real videos. It does so in a zero-shot manner by generalizing to out-of-distribution, real-world data\nwithout further tuning.",
        "3 Method\nGiven a single image yof an object and one or more drags D={dk}K\nk=1, our goal is to synthesize a\nvideoX={xi}N\ni=1sampled from the distribution\nX\u223c P(x1, x2, . . . , x N|y,D) (1)\nwhere Nis the number of video frames. The distribution Pshould reflect physics and generate a\npart-level animation of the object that responds to the drags. To learn it, we capitalize on a pre-trained\nvideo generator, i.e., Stable Video Diffusion (SVD, Section 3.1) [ 20]. Such video generators are\nexpected to acquire an implicit, general-purpose understanding of motion through their pre-training\n3Conv BlockAll-to-FirstSpatial Attn.Temporal Attn.Conv BlockAll-to-First Spatial Attn.Temporal Attn.\nCLIP\n\ud835\udc51!\ud835\udc51\"MLPX-Attn.Tokens\nConv &Modulate\u22ef\n\ud835\udc44\ud835\udc3e\ud835\udc49attend\n\u00d7\ud835\udc46\n,MLPConditioning(\ud835\udc66,\ud835\udc9f)\ud835\udc66\nX-Attn.X-Attn.X-Attn.Conv &Modulate\nDrag Tokens(A)\n(B)(C)Figure 2: Architectural Overview of Puppet-Master . To enable precise drag conditioning, we first\nmodify the original latent video diffusion architecture (Section 3.1) by ( A) adding adaptive layer\nnormalization modules to modulate the internal diffusion features and ( B) adding cross attention\nwith drag tokens (Section 3.2). Furthermore, to ensure high-quality appearance and background, we\nintroduce ( C)all-to-first spatial attention, a drop-in replacement for the spatial self-attention modules,\nwhich attends every noised video frame with the first frame (Section 3.3).\non Internet videos. This prior is particularly important to us, since we require data representative of\npart-level motions for our purposes, which are relatively scarce comparing to Internet videos.\nIn this section, we show how to tame the pre-trained video generator for part-level motion control.\nThere are two main challenges. First, the drag conditioning must be injected into the video generation\npipeline to facilitate efficient learning and accurate and time-consistent motion control while avoiding\ntoo much modifying the pre-trained video generator\u2019s internal representation. Second, na\u00efvely fine-\ntuning a pre-trained video diffusion model can result in artifacts such as cluttered backgrounds [ 39].\nTo address these challenges, in Section 3.2, we first introduce a novel mechanism to inject the drag\ncondition Din the video diffusion model. Then, in Section 3.3, we improve the video generation\nquality by introducing all-to-first attention mechanism, which reduces artifacts like the background\nclutter. Note that while we build on SVD, these techniques should be easily portable to other video\ngenerators based on diffusion.\n3.1 Preliminaries: Stable Video Diffusion\nSVD is an image-conditioned video generator based on diffusion, implementing a denoising process\nin latent space. This utilizes a variational autoencoder (V AE) (E, D), where the encoder Emaps\nthe video frames to the latent space, and the decoder Dreconstructs the video from the latent codes.\nDuring training, given a pair (X=x1:N, y)formed by a video and the corresponding image prompt,\none first obtains the latent code as z1:N\n0=E(x1:N), and then adds to the latter Gaussian noise\n\u03f5\u223cN(0,I), obtaining the progressively more noised codes\nz1:N\nt=\u221a\u00af\u03b1tz1:N\n0+\u221a\n1\u2212\u00af\u03b1t\u03f51:N, t= 1, . . . , T. (2)\nThis uses a pre-defined noising schedule \u00af\u03b10= 1, . . . , \u00af\u03b1T= 0. The denoising network \u03f5\u03b8is trained\nto reverse this noising process by optimizing the objective function:\nmin\n\u03b8E(x1:N,y),t,\u03f51:N\u223cN(0,I)\u0002\n\u2225\u03f51:N\u2212\u03f5\u03b8(z1:N\nt, t, y)\u22252\n2\u0003\n. (3)\nHere, \u03f5\u03b8uses the same U-Net architecture of VideoLDM [ 30], inserting temporal convolution and\ntemporal attention modules after the spatial modules used in Stable Diffusion [ 27]. The image\nconditioning is achieved via (1) cross attention with the CLIP [ 50] embedding of the reference frame\ny; and (2) concatenating the encoded reference image E(y)channel-wise to z1:N\ntas the input of the\nnetwork. After \u03f5\u03b8is trained, the model generates a video \u02c6Xprompted by yvia iterative denoising\nfrom pure Gaussian noise z1:N\nT\u223c N(0,I), followed by V AE decoding \u02c6X= \u02c6x1:N=D(z1:N\n0).\n43.2 Adding Drag Control to Video Diffusion Models\nNext, we show how to add the drags Das an additional input to the denoiser \u03f5\u03b8for motion control.\nWe do so by introducing an encoding function for the drags Dand by extending the SVD architecture\nto inject the resulting code into the network. The model is then fine-tuned using videos combined\nwith corresponding drag prompts in the form of training triplets (X, y,D). We summarize the key\ncomponents of the model below and refer the reader to Appendix A for more details.\nDrag encoding. Let\u2126be the spatial grid {1, . . . , H }\u00d7{1, . . . , W }where H\u00d7Wis the resolution\nof a video. A drag dkis a tuple (uk, v1:N\nk)specifying that the drag starts at location uk\u2208\u2126in\nthe reference image yand lands at locations vn\nk\u2208\u2126in subsequent frames. To encode a set of\nK\u2264Kmax= 5dragsD={dk}K\nk=1we use the multi-resolution encoding of [ 4]. Each drag dk1,\nis input to a hand-crafted encoding function enc(\u00b7, s) : \u2126N7\u2192RN\u00d7s\u00d7s\u00d7c, where sis the desired\nencoding resolution. The encoding function captures the state of the drag in each frame; specifically,\neach slice enc(dk, s)[n]encodes (1) the drag\u2019s starting location ukin the reference image, (2) its\nintermediate location vn\nkin the n-th frame, and (3) its final location vN\nkin the final frame. The\ns\u00d7smapenc(dk, s)[n]is filled with values \u22121except in correspondence of the 3locations, where\nwe store uk,vn\nkandvN\nkrespectively, utilizing c= 6 channels. Finally, we obtain the encoding\nDs\nenc\u2208RN\u00d7s\u00d7s\u00d7cKmaxofDby concatenating the encodings of the Kindividual drags, filling extra\nchannels with value \u22121ifK < K max. The encoding function is further detailed in Appendix A.\nDrag modulation. The SVD denoiser comprises a sequence of U-Net blocks operating at different\nresolutions s. We inject the drag encoding Ds\nencin each block, matching the block\u2019s resolution s. We\ndo so via modulation using an adaptive normalization layer [21, 51\u201356]. Namely,\nfs\u2190fs\u2297(1+\u03b3s) +\u03b2s, (4)\nwhere fs\u2208RB\u00d7N\u00d7s\u00d7s\u00d7Cis the U-Net features of resolution s, and\u2297denotes element-wise\nmultiplication. \u03b3s, \u03b2s\u2208RB\u00d7N\u00d7s\u00d7s\u00d7Care the scale andshift terms regressed from the drag\nencoding Ds\nenc. We use convolutional layers to embed Ds\nencfrom the dimension cKmaxto the target\ndimension C. We empirically find that this mechanism provides better conditioning than using only a\nsingle shift term with noscaling as in [4].\nDrag tokens. In addition to conditioning the network via drag modulation, we also do so via cross-\nattention by exploiting SVD\u2019s cross-attention modules. These modules attend a single key-value\nobtained from the CLIP [ 50] encoding of the reference image y. Thus, they degenerate to a global\nbias term with nospatial awareness [ 57]. In contrast, we concatenate to the CLIP token additional\ndrag tokens so that cross-attention is non-trivial. We use multi-layer perceptrons (MLPs) to regress\nan additional key-value pair from each dragdk. The MLPs take the origin ukand terminations vn\nk\nandvN\nkofdkalong with the internal diffusion features sampled at these locations, which are shown\nto contain semantic information [ 58], as inputs. Overall, the cross-attention modules have 1 +Kmax\nkey-value pairs ( 1is the original image CLIP embedding), with extra pairs set to 0ifK < K max.\n3.3 Attention with the Reference Image Comes to Rescue\nIn preliminary experiments utilizing the Drag-a-Move [ 4] dataset, we noted that the generated videos\ntend to have cluttered/gray backgrounds. Instant3D [ 39] reported a similar problem when generating\nmultiple views of a 3D object, which they addressed via careful noise initialization. VideoMV [ 59]\nand Vivid-ZOO [ 60] directly constructed training videos with a gray background, which might help\nthem offset a similar problem.\nThe culprit is that SVD, which was trained on 576\u00d7320videos, fails to generalize to very different\nresolutions. Indeed, when prompted by a 256\u00d7256image, SVD cannot generate reasonable videos.\nAs a consequence, fine-tuning SVD on 256\u00d7256videos (as we do for Puppet-Master) is prone to\nlocal optima, yielding sub-optimal appearance details. Importantly, we noticed that the first frame\nof each generated video is spared from the appearance degradation (Fig. 6), as the model learns to\ndirectly copy the reference image. Inspired by this, we propose to create a \u201cshortcut\u201d from each\nnoised frame to the first frame with all-to-first spatial attention, which significantly mitigates, if not\ncompletely resolves, the problem.\n1With a slight abuse of notation, we assume dk\u2208\u2126N, asuk=v1\nkand hence v1:N\nk\u2208\u2126Nfully describes dk.\n5All Objaverse Animated Assets (40k)Objaverse-Animation (16k)Objaverse-AnimationHQ (10k)\nFeature extractionRandom forest classificationRenderingGPT-4V prompting & queryingTraining Data\nDrastic ChangeStatic\nGlobal Change Only\nSudden Appearance Change\nUnrealisticAnimation\n\u22ef\n\u22ef\n\u22efFigure 3: Data Curation . We propose two strategies to filter the animated assets in Objaverse,\nresulting in Objaverse-Animation ( 16k) and Objaverse-Animation-HQ ( 10k) of varying levels of\ncuration, from which we construct the training data of Puppet-Master by sampling sparse motion\ntrajectories and projecting them to 2D as drags.\nAll-to-first spatial attention. Previous works [ 61\u201363] have shown that attention between the noised\nbranch and the reference branch improves the generation quality of image editing and novel view\nsynthesis tasks. Here, we design an all-to-first spatial attention that enables each noised frame to\nattend to the first (reference) frame. Inspired by [ 63], we implement this attention by having each\nframe query the key and value of the first frame in all self-attention layers within the denoising U-Net.\nMore specifically, denoting the query, key, and value tensors as Q, K andV\u2208RB\u00d7N\u00d7s\u00d7s\u00d7C, we\ndiscard the key and value tensors of non-first frames, i.e.,K[:,1:]andV[:,1:], and compute the\nspatial attention Aiof the i-th frame as follows:\nAi= softmax \nflat (Q[:,i]) flat ( K[:,0])T\n\u221a\nD!\nflat (V[:,0]), (5)\nwhere flat(\u00b7) :RB\u00d7s\u00d7s\u00d7C7\u2192RB\u00d7L\u00d7Cflattens the spatial dimensions to get L=s\u00d7stokens for\nattention. The benefit is two-fold: first, this \u201cshortcut\u201d to the first frame allows each non-first frame\nto directly access non-degraded appearance details of the reference image, effectively alleviating\nlocal minima during optimization. Second, combined with the proposed drag encoding (Section 3.2),\nwhich specifies, for every frame, the origin ukat the first frame, all-to-first attention enables the latent\npixel containing the drag termination ( i.e.,vn\nk) to more easily attend to the latent pixel containing the\ndrag origin on the first frame, potentially facilitating learning.\n4 Curating Data to Learn Part-Level Object Motion\nTo train our model we require a video dataset that captures the motion of objects at the level of parts.\nCreating such a dataset in the real world means capturing a large number of videos of moving objects\nwhile controlling for camera and background motion. This is difficult to do for many categories ( e.g.,\nanimals) and unfeasible at scale. DragAPart [ 4] proposed to use instead renderings of synthetic 3D\nobjects, and their corresponding part annotations, obtained from GAPartNet [ 64]. Unfortunately,\nthis dataset still requires to manually annotate and animate 3D object parts semi-manually, which\nlimits its scale. We instead turn to Objaverse [ 22], a large-scale 3D dataset of 800k models created by\n3D artists, among which about 40k are animated. In this section, we introduce a pipeline to extract\nsuitable training videos from these animated 3D assets, together with corresponding drags D.\nIdentifying animations. While Objaverse [ 22] has more than 40k assets labeled as animated, not all\nanimations are useful for our purposes (Fig. 3). Notably, some are \u201cfake\u201d, with the objects remaining\nstatic throughout the sequence, while others feature drastic changes in the objects\u2019 positions or even\n6their appearances. Therefore, our initial step is to filter out these unsuitable animations. To do so, we\nextract a sequence of aligned point clouds from each animated model and calculate several metrics\nfor each sequence, including: (1) the dimensions and location of the bounding box encompassing the\nentire motion clip, (2) the size of the largest bounding box for the point cloud at any single timestamp\nand (3) the mean and maximal total displacement of all points throughout the sequence. Using these\nmetrics, we fit a random forest classifier, which decides whether an animation should be included\nin the training videos or not, on a subset of Objaverse animations where the decision is manually\nlabeled. The filtering excludes many assets that exhibit imperceptibly little or over-dramatic motions\nand results in a subset of 16k animations, which we dub Objaverse-Animation.\nFurther investigation reveals that this subset still contains assets whose motions are artificially\nconceived and therefore do not accurately mimic real-world dynamics (Fig. 3). To avoid such\nimaginary dynamics leaking into our synthesized videos, we employ the multi-modal understanding\ncapability of GPT-4V [ 65] to assess the realism of each motion clip. Specifically, for each animated 3D\nasset in Objaverse-Animation, we fix the camera at the front view and render 4images at timestamps\ncorresponding to the 4quarters of the animation and prompt GPT-4V to determine if the motion\ndepicted is sufficiently realistic to qualify for the training videos. This filtering mechanism excludes\nanother 6k animations, yielding a subset of 10k animations which we dub Objaverse-Animation-HQ.\nSampling drags. The goal of drag sampling is to produce a sparse set of drags D={dk}K\nk=1\nwhere each drag dk:= (uk, v1:N\nk)tracks a point ukon the asset in pixel coordinates throughout\ntheNframes of rendered videos. To encourage the video generator to learn a meaningful motion\nprior, ideally, the set should be both minimal andsufficient : each group of independently moving\nparts should have oneandonly one drag corresponding to its motion trajectory, similar to Drag-a-\nMove [ 4]. For instance, there should be separate drags for different drawers of the same furniture,\nas their motions are independent, but not for a drawer and its handle, as in this case, the motion of\noneimplies that of the other. However, Objaverse [ 22] lacks the part-level annotation to enforce\nthis property. To partially overcome this, we find that some Objaverse assets are constructed in\na bottom-up manner, consisting of multiple sub-models that align well with semantic parts. For\nthese assets, we sample 1drag per sub-model; for the rest, we sample a random number of drags\nin total. For each drag, we first sample a 3D point on the visible part of the model (or sub-model)\nwith probabilities proportional to the point\u2019s total displacement across Nframes and then project\nits ground-truth motion trajectory p1, . . . , p N\u2208R3to pixel space to obtain dk. Once all Kdrags\nare sampled, we apply a post-processing procedure to ensure that each pair of drags is sufficiently\ndistinct, i.e., fori\u0338=j, we randomly remove one of dianddjif\u2225v1:N\ni\u2212v1:N\nj\u22252\n2\u2264\u03b4where \u03b4is a\nthreshold we empirically set to 20Nfor256\u00d7256renderings.",
        "5 Experiments\nThe final model, Puppet-Master, is trained on a combined dataset of Drag-a-Move [ 4] and Objaverse-\nAnimation-HQ (Section 4). We evaluate the performance of the final checkpoint on multiple bench-\nmarks, including the test split of Drag-a-Move and real-world cases from Human3.6M [ 66], Amazon-\nBerkeley Objects [ 67], Fauna Dataset [ 68,69], and CC-licensed web images in a zero-shot manner,\ndemonstrating qualitative and quantitative improvements over prior works and excellent general-\nization to real cases (Section 5.1). The design choices that led to Puppet-Master are ablated and\ndiscussed further in Section 5.2. In Section 5.3, we show the effectiveness of our data curation\nstrategy (Section 4). We refer the reader to Appendix C for the implementation details.\n5.1 Main Results\nQuantitative comparison. We compare Puppet-Master to DragNUWA [ 9] and DragAnything [ 16],\nboth of which are trained on real-world videos to support open-domain motion control, on the\npart-level motion-conditioned video synthesis task in Table 1. On the in-domain test set ( i.e., Drag-\na-Move), Puppet-Master outperforms both methods on all standard metrics, including pixel-level\nPSNR, patch-level SSIM, and feature-level LPIPS and FVD, by a significant margin.\nAdditionally, to better demonstrate our model\u2019s superiority in generating part-level object dynamics,\nwe introduce a flow-based metric dubbed flow error . Specifically, we first track points on the object\nthroughout the generated and ground-truth videos using CoTracker [ 70], and then compute flow error\n7Table 1: Comparisons with DragNUWA [ 9], DragAnything [ 16] and DragAPart [ 4] on the in-domain\nDrag-a-Move and out-of-domain Human3.6M datasets. The best method is bolded and second best\nunderlined. Our model has notbeen trained on the Human3.6M dataset, or any real video datasets.\nMethodDrag-a-Move [4] Human3.6M [66]\nPSNR\u2191SSIM\u2191LPIPS\u2193FVD\u2193flow error \u2193PSNR\u2191SSIM\u2191LPIPS\u2193FVD\u2193\nDragNUWA 20.09 0.874 0.172 281.49 17.55 / 15.41 17.52 0.878 0.158 466.91\nDragAnything 16.71 0.799 0.296 468.46 16.09 / 23.21 13.29 0.767 0.305 768.63\nDragAPart\n\u2014Original 23.41 0.925 0.085 180.27 14.17 / 3.71 15.14 0.852 0.197 683.40\n\u2014Re-Trained 23.78 0.927 0.082 189.10 14.34 / 3.73 15.25 0.860 0.188 549.64\nPuppet-Master 24.41 0.927 0.085 246.99 12.21 /3.53 17.59 0.872 0.155 454.76\nDragAPartOurs: Puppet-Master\n(a)(b)\nFigure 4: Qualitative Comparison with DragAPart [ 4]. The videos generated by DragAPart lack\ntemporal consistency: (a) the door initially opens to the left, but later it is switched to open to the\nright, and it partially closes between the second and third frames visualized here; (b) DragAPart fails\nto generalize to out-of-domain cases, resulting in distorted motion.\nas the root mean square error (RMSE) between the two trajectories. We report two RMSEs in Table 1\nfor this flow error metric. The former value ( i.e., before the slash) is averaged among the origins\nof all conditioning drags only, i.e.,{uk}K\nk=1, while the latter value ( i.e., after the slash) is averaged\namong all foreground points. While Puppet-Master has lower values on both, it obtains a significantly\nsmaller value when the error is averaged among all foreground points. This indicates Puppet-Master\ncan better model nuanced part-level dynamics, thanks to which the parts that do not necessarily\nmove along with the dragged parts stay static in the generated videos, reducing the overall error. By\ncontrast, DragNUWA and DragAnything move the whole object, so every point incurs a large error.\nTo assess the cross-domain generalizability, we directly evaluate Puppet-Master on an unseen dataset\ncaptured in the real world ( i.e., Human3.6M). On this out-of-domain test set, Puppet-Master outper-\nforms prior models on most metrics, despite not being fine-tuned on any real videos.\nFor completeness, we also include the metrics of DragAPart [ 4], a drag-conditioned image generator.\nThe original DragAPart was trained on Drag-a-Move only. For fairness, we fine-tune it from Stable\nDiffusion [ 27] with the identical data setting as Puppet-Master, and evaluate the performance of both\ncheckpoints ( Original2andRe-Trained in Table 1). The videos are obtained from Nindependently\ngenerated frames conditioned on gradually extending drags. While its samples exhibit high visual\nquality in individual frames, they lack temporal smoothness, characterized by abrupt transitions and\ndiscontinuities in movement, resulting in a larger flow error3(Fig. 4a). This justifies starting from\na video generator to improve temporal consistency. Furthermore, DragAPart fails to generalize to\nout-of-domain cases ( e.g., Fig. 4b and Table 1).\n2Original is not ranked as it is trained on single-category data only and hence not an open-domain generator.\n3FVD is not an informative metric for motion quality. Prior works [ 71,72] have noted that FVD is biased\ntowards the quality of individual frames and does notsufficiently account for motion in generated videos. Good\nFVD scores can still be obtained with static videos or videos with severe temporal corruption.\n8Figure 5: Qualitative Results onreal-world cases spanning diverse categories.\nQualitative comparison. We show samples generated by Puppet-Master and prior models side by\nside in Fig. 1. The dynamics generated by Puppet-Master are physically plausible and faithful to the\ninput image and drags. By contrast, the videos generated by DragNUWA [ 9] and DragAnything [ 16]\nscale (d, e, f) or shift (b) the object as a whole at best, or even show distorted motion (a, c). Even\nthough Puppet-Master is fine-tuned solely on renderings of synthetic 3D models, it does generalize to\nreal cases, and is capable of preserving fine-grained texture details.\nQualitative results on real data. In Fig. 5, we show more real examples generated by Puppet-\nMaster. The synthesized videos exhibit realistic dynamics that are typical of the underlying categories,\nincluding humans, animals, and several man-made categories.\nTable 2: Ablation studies of various model components. In addition to the standard metrics, we\nreport a flow-based metric dubbed flow error . A lower flow error indicates the generated videos\nfollow the drag control better. We also manually count the frequency of generated videos whose\nmotion directions are opposite to the intention of their drag inputs. Here, \u2265indicates there are video\nsamples whose motion directions are hard to distinguish. When ablating attention with the reference\nimage, we use Cas the base drag conditioning architecture.\nSetting PSNR \u2191SSIM\u2191LPIPS\u2193FVD\u2193flow error \u2193% wrong dir. \u2193\nDrag conditioning\nA Shift only w/o end loc. 13.23 0.816 0.446 975.16 15.60 px \u22655\nB Shift+scale w/o end loc. 22.98 0.917 0.093 223.20 9.33 px 4\nC Shift+scale w/ end loc. 23.67 0.926 0.080 205.40 10.48 px 4\nD C + x-attn. w/ drag tok. 24.00 0.929 0.069 170.43 9.80 px 1\nAttn. w/ ref. image\nNo attn. 11.96 0.771 0.391 823.00 12.35 px \u22653\nAttn. w/ static ref. video 17.51 0.874 0.233 483.18 13.57 px \u22658\nAll-to-first attn. 23.67 0.926 0.080 205.40 10.48 px 4\n5.2 Ablations\nWe conduct several ablation studies to analyze the introduced components of Puppet-Master. For\neach design choice, we train a model using the training split of the Drag-a-Move [ 4] dataset with\nbatch size 8for30,000iterations and evaluate on 100videos from its test split without classifier-free\nguidance [73]. Results are shown in Table 2 and Fig. 6 and discussed in detail next.\n9No drag tok. x-attn.(No attn.)No drag tok. x-attn.(Attn. w/ static ref. vid.)No drag tok. x-attn.(All-to-first attn.)\nFull modelFigure 6: Visualization of samples generated by different model designs, where we show the last\nframe and the first 3frames. While all designs produce nearly perfect first frames, our proposed\nall-to-first attention module significantly enhances sample quality. Without this module, the generated\nsamples often exhibit sub-optimal appearances and backgrounds. The cross-attention module with\ndrag tokens further improves the appearance details.\nDrag conditioning. Table 2 compares Puppet-Master with multiple variants of conditioning mecha-\nnisms (Section 3.2). Adaptive normalization layers ( Avs.B), drag encoding with final termination\nlocation vN\nk(Bvs.C), and cross attention with drag tokens ( Cvs.D) are all beneficial. Notably,\nby combining these ( i.e., rowD), the model achieves a negligible rate of generated samples with\nincorrect motion directions (see Table 2 caption for details).\nAttention with the reference image. We find that all-to-first attention (Section 3.3) is essential for\nhigh generation quality. We also compare all-to-first attention with an alternative implementation\nstrategy inspired by the X-UNet design in 3DiM [ 61], where we pass a static video consisting of\nthe reference image copied Ntimes to the same network architecture and implement cross attention\nbetween the clean (static) reference video branch and the noised video branch. The latter strategy\nperforms worse. We hypothesize that this is due to the distribution drift between the two branches,\nwhich forces the optimization to modify the pre-trained SVD\u2019s internal representations too much.\nFigure 7: Data curation helps stabilize training.Setting PSNR \u2191 SSIM\u2191\nw/o Data Curation 6.04 0.411\nw/ Data Curation 19.87 0.884\nSetting LPIPS \u2193 FVD\u2193\nw/o Data Curation 0.703 1475.35\nw/ Data Curation 0.181 624.47\nTable 3: Training on more abundant but lower-\nquality data leads to lower generation quality. Here,\n\u2018w/o Data Curation\u2019 model is trained on Objaverse-\nAnimation while \u2018w/ Data Curation\u2019 model is\ntrained on Objaverse-Animation-HQ. Both models\nare trained for 7K iterations. Evaluation is per-\nformed on the test split of Drag-a-Move [4].\n105.3 Less is More: Data Curation Helps at Scale\nTo verify that our data curation strategy from Section 4 is effective, we compare two models trained\non Objaverse-Animation and Objaverse-Animation-HQ respectively under the same hyper-parameter\nsetting. The training dynamics are visualized in Fig. 7. The optimization collapses towards 7k\niterations when the model is trained on a less curated dataset, resulting in much lower-quality video\nsamples (Table 3). This suggests that the data\u2019s quality matters more than quantity at scale.",
        "4 Proposed Method\nAccording to the sensor fusion literature for conventional cameras, the main\nstrategies for combining stereo images with sparse depth measurements from\nactive sensors consist of i) concatenating the two modalities and processing them\nas joint inputs with a stereo network [12,46,69,81], ii) modulating the internal\ncost volume computed by the backbone itself [25,49,69,82] or, more recently, iii)\nprojecting distinctive patterns on images according to depth hints [4].\nWe follow the latter path, since it is more effective and flexible than the\nalternatives \u2013 which can indeed be applied to white box frameworks only. For\nthis purpose, we design two alternative strategies suited even for gray and black\nbox frameworks, respectively, as depicted in Fig. 3.\n4.1 Virtual Stack Hallucination \u2013 VSH\nGiven left and right stacks SL,SRof size W \u00d7H\u00d7C and a set Zof depth measure-\nments z(x, y)by a sensor, we perform a Virtual Stack Hallucination (VSH), by\naugmenting each channel c\u2208C, to increase the distinctiveness of local patterns6 L. Bartolomei et al.\n(a) (b) (c)\nFig.3: Overview of a generic event-based stereo network and our hallucina-\ntion strategies. State-of-the-art event-stereo frameworks (a) pre-process raw events\nto obtain event stacks fed to a deep network. In case the stacks are accessible, we\ndefine the model as a gray box, otherwise as a black box . In the former case (b), we can\nhallucinate patterns directly on it (VSH). When dealing with a black box (c), we can\nhallucinate raw events that will be processed to obtain the stacks (BTH).\nand thus ease matching. This is carried out by injecting the same virtual stack\nA(x, y, x\u2032, c)intoSL,SRrespectively at coordinates (x, y)and(x\u2032, y).\nSL(x, y, c )\u2190 A (x, y, x\u2032, c)\nSR(x\u2032, y, c)\u2190 A (x, y, x\u2032, c)(1)\nwith x\u2032obtained as x\u2212d(x, y), with disparity d(x, y)triangulated back from\ndepth z(x, y)asbf\nz(x,y), according to the baseline and focal lengths b, fof the\nstereo system. We deploy a generalized version of the random pattern operator\nAproposed in [4], agnostic to the stacked representation:\nA(x, y, x\u2032, c)\u223c U(S\u2212,S+) (2)\nwithS\u2212andS+the minimum and maximum values appearing across stacks\nSL,SRandUa uniform random distribution. Following [4], the pattern can\neither cover a single pixel or a local window. This strategy alone is sufficient al-\nready to ensure distinctiveness and to dramatically ease matching across stacks,\neven more than with color images [4], since acting on semi-dense structures \u2013\ni.e., stacks are uninformative in the absence of events. It also ensures a straight-\nforward application of the same principles used on RGB images, e.g., to combine\ntheoriginalcontent(color)withthevirtualprojection(pattern)employingalpha\nblending [4]. Nevertheless, we argue that acting at this level i) requires direct\naccess to the stacks, i.e., a gray-box deep event-stereo network, and ii) might be\nsub-optimal as stacks encode only part of the information from streams.\n4.2 Back-in-Time Hallucination \u2013 BTH\nA higher distinctiveness to ease correspondence can be induced by hallucinating\npatterns directly in the continuous events domain. Specifically, we act in the\nso-called event history : given a timestamp tdat which we want to estimateLiDAR-Event Stereo Fusion 7\n(densifiedfor visualizationpurpose)\n(a)(b)tz= tdtz= td-15\ntdtdtdtdtdtdtdtdtdtdtdtd\nFig.4: Overview of Back-in-Time Hallucination (BTH). To estimate disparity\nattd, if LiDAR data is available \u2013 e.g., at timestamp tz=td(green) or tz=td\u221215\n(yellow) \u2013 we can na\u00efvely inject events of random polarities at the same timestamp tz\n(a). More advanced injection strategies can be used \u2013 e.g. by hallucinating multiple\nevents, starting from td, back-in-time at regular intervals (b).\ndisparity, raw events are sampled from the left and right streams starting from\ntdand going backward, according to either SBN or SBT stacking approaches, to\nobtain a pair of event histories EL=\b\neL\nk\tN\nk=1andER=\b\neR\nk\tM\nk=1, where eL\nk, eR\nk\nare the k-th left and right events. Events in the history are sorted according to\ntheir timestamp \u2013 i.e., inequality tk\u2264tk+1holds for every two adjacent ek, ek+1.\nAt this point, we intervene to hallucinate novel events: given a depth mea-\nsurement z(\u02c6x,\u02c6y), triangulated back into disparity d(\u02c6x,\u02c6y), we inject a pair of\nfictitious events \u02c6eL= (\u02c6x,\u02c6y,\u02c6p,\u02c6t)and\u02c6eR= (\u02c6x\u2032,\u02c6y,\u02c6p,\u02c6t)respectively inside ELand\nER, producing \u02c6EL=\b\neL\n1, . . . , \u02c6eL, . . . , eL\nN\t\nand \u02c6ER=\b\neR\n1, . . . , \u02c6eR, . . . , eR\nM\t\n. By\nconstruction, \u02c6eLand\u02c6eRadhere to i) the time ordering constraint, ii) the geom-\netry constraint \u02c6x\u2032= \u02c6x\u2212d(\u02c6x,\u02c6y)and iii) a similarity constraint \u2013 i.e.,\u02c6p,\u02c6tare the\nsame for \u02c6eLand\u02c6eR. Fictitious polarity \u02c6pand fictitious timestamp \u02c6tare two de-\ngrees of freedom useful to ensure distinctiveness along the epipolar line and ease\nmatching, according to which we can implement different strategies summarized\nin Fig. 4, and detailed in the remainder.\nSingle-timestamp injection. The simplest way to increase distinctiveness\nis to insert synchronized events at a fixed timestamp. Accordingly, for each depth\nmeasurement d(\u02c6x,\u02c6y), a total of K\u02c6x,\u02c6ypairs of fictitious events are inserted in\nEL,ER, having polarity \u02c6pkrandomly chosen from the discrete set {\u22121,1}. Times-\ntamp \u02c6tis fixed and can be, for instance, tzat which the sensor infers depth, that\ncan coincide with timestamp tdat which we want to estimate disparity \u2013 e.g.,\ntz=td= 0in the case depicted in Fig. 4 (a). Inspired by [4], events might be\noptionally hallucinated in patches rather than single pixels. However, as depth\nsensors usually work at a fixed acquisition frequency \u2013 e.g., 10Hz for LiDARs \u2013\nsparse points might be unavailable at any specific timestamp. Nonetheless, since\nEL,ERencode a time interval, we can hallucinate events even if derived from\ndepth scans performed in the past \u2013e.g., at tz< td, \u2013 by placing them in the\nproper position inside EL,ER.\nRepeatedinjection. Thepreviousstrategydoesnotexploitoneofthemain\nadvantages of events over color images, i.e. the temporal dimension, at its best.\nPurposely, we design a more advanced hallucination strategy based on repeated\nna\u00efve injections performed along the time interval sampled by EL,ER. As long\nas we are interested in recovering depth at tdonly, we can hallucinate as many8 L. Bartolomei et al.\nevents as we want in the time interval before t\u2013i.e., for tz=td= 0, over the en-\ntire interval as shown in Fig. 4 (b) \u2013 consistent with the depth measurements at\ntditself,whichwillincreasethedistinctivenessintheeventhistoriesandwillease\nthe match by hinting the correct disparity. Inspired by the stacked representa-\ntions introduced in Sec. 3, we can design a strategy for injecting multiple events\nalong the stream. Accordingly, we define the conservative time range [t\u2212, t+]of\nthe events histories EL,ER, with t\u2212= min\b\ntL\n0, tR\n0\t\nandt+= max\b\ntL\nN, tR\nM\t\nand\ndivide it into Bequal temporal bins. Then, inspired by MDES [43], we run B\nsingle-timestamp injections at \u02c6tb=2b\u22121\n2b(t+\u2212t\u2212) +t\u2212, with b\u2208 {1, . . . , B }.\nAdditionally, each depth measurement is used only once \u2013 i.e., the number of\nfictitious events Kb,\u02c6x,\u02c6yin the b-th injection is set as Kb,\u02c6x,\u02c6y\u2190K\u02c6x,\u02c6y\u03b4(b, D \u02c6x,\u02c6y)\nwhere \u03b4(\u00b7,\u00b7)is the Kronecker delta and D\u02c6x,\u02c6y\u2190round (XU(B\u22121) + 1)is a ran-\ndom slot assignment. We will show in our experiment how this simple strategy\ncan improve the results of BTH, in particular increasing its robustness against\nmisaligned LiDAR data \u2013 i.e., measurements retrieved at a timestamp tz< td.",
        "2 Related Work\nStereo Matching on color images. It is a longstanding open problem, with\na large body of literature spanning from traditional approaches grounded on\nhandcrafted features and priors [5,24,31,36,62,68,75,76,78] to contemporary\ndeep learning approaches that brought significant improvements over previous\nmethods,startingwith[79].Nowadays,themosteffectivesolutionshaveemerged\nas end-to-end deep stereo networks [51], replacing the whole stereo pipeline with\nadeepneuralnetworkarchitecturethrough2Dand3Darchitectures.Theformer,\ninspiredbytheU-Netmodel[53],adoptsanencoder-decoderdesign[37,42,45,50,\n54,59,63,64,74,77]. In contrast, the latter constructs a feature cost volume from\nimage pair features and estimates the disparity map through 3D convolutions at\nthe cost of substantially higher memory and runtime demands [10,11,13,16,23,\n27,28,57,70,73,80]. A recent trend in this field [34,38,65,71,83,84] introduced\ninnovative deep stereo networks that embrace an iterative refinement paradigm\nor use Vision Transformers [22,35].\nStereo Matching with event cameras. This topic attracted significant\nattention due to the unique advantages of event sensors over traditional frame-\nbased cameras. Similarly to conventional stereo matching, the first approaches\nfocused on developing traditional algorithms by building structured represen-\ntations, such as voxel grids [56], matched through handcrafted similarity func-\ntions [30,56,60,85]. However, pseudo-images lose the high temporal resolution\nof the stream: to face this problem, [8,52] handle events without an intermedi-\nate representation using an event-to-event matching approach, where for each\nreference event, a set of possible matches is given. Camu\u00f1as-Mesa et al. [7]4 L. Bartolomei et al.\nFig.2: Event cameras vs LiDARs \u2013 strengths and weaknesses. Event cameras\nprovide rich cues at object boundaries where LiDARs cannot (cyan), yet LiDARs can\nmeasure depth where the lack of texture makes event cameras uninformative (green).\nadd filters to exploit orientation cues and increase matching distinctiveness. In-\nstead, [47] revisited the cooperative network from [41]. Neural networks also\nshowed promising results on event stereo matching with models directly pro-\ncessing raw events or using structured representation. The former are often in-\nspired by [41] and typically employ Spiking Neural Networks (SNN) [1,15,44].\nThe latter adopts data-driven Convolutional Neural Networks (CNNs) to infer\ndense depth maps [43,66,67]. A detailed review of different event-based stereo\ntechniques can be found in [17].\nSensor fusion for stereo. Recent research has delved into the fusion of\ncolor-cameras stereo vision with active sensors, starting with handcrafted algo-\nrithms: Badino et al. [2] integrated LiDAR data directly into the stereo algo-\nrithm using dynamic programming, Gandhi et al. [19] proposed an efficient seed-\ngrowingalgorithmtofusetime-of-flight(ToF)depthdatawithstereopairs,while\nMarin et al. [40] and Poggi et al. [48] exploited confidence measures. Eventu-\nally, contemporary approaches integrated depth from sensors with modern stereo\nnetworks, either by concatenating them to images as input [12,46,69,81] or by\nusing them to guide the cost optimization process by modulating existing cost\nvolumes [25,49,69,82]. More recently, Bartolomei et al. [4] followed a different\npathwithVirtualPatternProjection(VPP).AlthoughLiDARsensorsandevent\ncamerashavebeendeployedtogetherforsomeapplications[6,14,20,33,55,58,61],\nthis paper represents the first attempt at combining LiDAR with an event stereo\nframework. We argue that the two modalities are complementary, as shown in\nFig. 2 \u2013 e.g., the lack of texture and motion makes an event camera uninforma-\ntive, whereas this does not affect LiDAR systems.\n3 Preliminaries: Event-based Deep Stereo\nEvent cameras measure brightness changes as an asynchronous stream of events.\nAccordingly, an event ek= (xk, yk, pk, tk)is triggered at time tkif the intensity\nsensed by pixel (xk, yk)on the W \u00d7H sensor grid changes and surpasses a specific\ncontrast threshold. Depending on the sign of this change, it will have polarity\npk\u2208 {\u2212 1,1}. Since this unstructured flow is not suitable for standard CNNs \u2013 as\nthose proposed in the classical stereo literature [51] \u2013 converting it into W \u00d7H\u00d7C\nstructured representations is necessary if we are interested in obtaining a dense\ndisparitymap[21].Purposely,givenatimestamp tdatwhichwewanttoestimateLiDAR-Event Stereo Fusion 5\na disparity map, events are sampled backward in time from the stream, either\nbased on a time interval (SBT) or a maximum number of events (SBN), and\nstackedaccording to various strategies \u2013 among them:\nHistogram [39]. Events of the two polarities are counted into per-pixel his-\ntograms, yielding a W \u00d7H\u00d72 stack.\nVoxel grid [86]. The timelapse from which events are sampled is split into\nBuniform bins: polarities are accumulated in each bin of a W \u00d7H\u00d7B stack.\nMixed-DensityEventStack(MDES) [43].Similartothevoxelgridstrat-\negy, the timelapse is split into bins covering 1,1\n2,1\n4, ...,1\n2N\u22122,1\n2N\u22121of the total\ninterval. The latest event in each bin is kept, yielding a W \u00d7H\u00d7N binary stack.\nConcentrated stack [43]. A shallow CNN is trained to process a pre-\ncomputed stack ( e.g., an MDES) and aggregate it to a W \u00d7H\u00d71 data structure.\nTime-Ordered Recent Event (TORE) [3]. It stores event timestamps\ninto Q per-pixel queues for each polarity, yielding a W \u00d7H\u00d72Q stack.\nTime Surface [32]. A surface is derived from the timestamp distributions\nof the two polarities. S values are sampled for each, yielding a W \u00d7H\u00d72S stack.\nERGO-12 [87]. An optimized representation of 12 channels, each built ac-\ncording to different strategies from the previous. It yields a W \u00d7H\u00d712 stack.\nTencode [26]. A color image representation in which R and B channels\nencode positive and negative polarities, with G encoding the timestamp relative\nto the total timelapse. It produces an RGB image, i.e. a W\u00d7H\u00d73 stack.\nWe can broadly classify stereo frameworks using these representations into\nthree categories: i) white boxes , for which we have full access to the implemen-\ntation of both the stereo backbone and the stacked event construction; ii) gray\nboxes, in case we do not have access to the stereo backbone; iii) black boxes , when\nthe stacked event representation is not accessible neither.",
        ". Event stereo matching is an emerging technique to estimate\ndepth from neuromorphic cameras; however, events are unlikely to trig-\nger in the absence of motion or the presence of large, untextured regions,\nmaking the correspondence problem extremely challenging. Purposely,\nwe propose integrating a stereo event camera with a fixed-frequency ac-\ntive sensor \u2013 e.g., a LiDAR \u2013 collecting sparse depth measurements,\novercoming the aforementioned limitations. Such depth hints are used\nby hallucinating \u2013 i.e., inserting fictitious events \u2013 the stacks or raw in-\nput streams, compensating for the lack of information in the absence of\nbrightness changes. Our techniques are general, can be adapted to any\nstructured representation to stack events and outperform state-of-the-art\nfusion methods applied to event-based stereo.",
        "6 Conclusion\nThis paper proposes a novel framework for implementing event stereo and Li-\nDAR fusion. It works by hallucinating fictitious events either in the stacked\nrepresentation processed by stereo backbones or the continuous streams sensed\nby event cameras, easing the matching process to the downstream stereo model\nestimating disparity. Our exhaustive experiments prove that our solutions, VSH\nand BTH, dramatically outperform alternative fusion strategies from the RGB\nstereo literature, retaining the microsecond resolution typical of event cameras\ndespite the discrete frame rate of LiDARs and depth sensors in general.\nLimitations. Despite the robustness shown with misaligned LiDAR data, a\nmarginal drop in accuracy compared to the case of having LiDAR measurements\nat the very same timestamp at which we aim to infer disparity maps occurs.\nFuture work will focus on studying new design mechanisms to deal with it.LiDAR-Event Stereo Fusion 15\nAcknowledgement. This study was carried out within the MOST \u2013 Sus-\ntainable Mobility National Research Center and received funding from the Eu-\nropean Union Next-GenerationEU \u2013 PIANO NAZIONALE DI RIPRESA E RE-\nSILIENZA (PNRR) \u2013 MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.4 \u2013\nD.D. 1033 17/06/2022, CN00000023. This manuscript reflects only the authors\u2019\nviews and opinions, neither the European Union nor the European Commission\ncan be considered responsible for them.\nWe acknowledge the CINECA award under the ISCRA initiative, for the\navailability of high-performance computing resources and support.",
        "1 Introduction\nDepth estimation is a fundamental task with many applications ranging from\nrobotics, 3D reconstruction, and augmented/virtual reality to autonomous ve-\nhicles. Accurate, prompt, and high-resolution depth information is crucial for\nmost of these tasks, but obtaining it remains an open challenge. Among the\nmany possibilities, depth-from-stereo is one of the longest-standing approachesarXiv:2408.04633v1  [cs.CV]  8 Aug 20242 L. Bartolomei et al.\nto deal with it, with a large literature of deep architecture [51] proposed in the\nlast decade for processing rectified color images.\nEventcameras[18](orneuromorphiccameras)arerecentlyemergingasanal-\nternative to overcome thelimitationsof traditionalimagingdevices, suchas their\nlow dynamic range or the motion blur caused by fast movements. Unlike their\ntraditional counterparts, event cameras do not capture frames at synchronous\nintervals. Instead, they mimic the dynamic nature of human vision by reporting\npixel intensity changes, which can have positiveornegative polarities, as soon as\nthey happen. This peculiarity endows them with unparalleled features \u2013 notably\nmicrosecond temporal resolution, and an exceptionally high dynamic range \u2013\nmaking them perfectly suited for applications where fast motion and challeng-\ning light conditions are persistent issues ( e.g. autonomous driving). The events\nstreams are often encoded in W \u00d7H\u00d7C tensors, thus being fully compatible with\nthe CNNs used for classical stereo [43], capable of estimating dense disparity\nmaps driven by data, despite the sparse nature of events.\nHowever, as the events trigger only with brightness changes any derived data\nissemi-dense and uninformative, for instance, when facing large untextured\nregions or in the absence of any motion \u2013 e.g., as in the example in Fig. 1. This\nmakes the downstream stereo network struggle to match events across left and\nright cameras, as shown in Fig. 1 (a). According to the RGB stereo literature,\nfusing color information with sparse depth measurements from an active sensor\n[4,12,49,82]( e.g.,aLiDAR)considerablysoftenstheweaknessesofpassivedepth\nsensing, despite the much lower resolution at which depth points are provided.\nWe argue that such a strategy would counter the aforementioned issues even if\napplied to the event stereo paradigm, yet with a notable nodus caused by the\nfixed rate at which depth sensors work \u2013 usually, 10Hz for LiDARs \u2013 being in\ncontrast with the asynchronous acquisition rate of event cameras. This would\ncause to either i) use depth points only when available, harming the accuracy of\nmost fusion strategies known from the classical stereo literature [12,49,82], or ii)\nlimiting processing to the LiDAR pace, nullifying one of the greatest strength of\nevent cameras \u2013 i.e., microseconds resolution. Nonetheless, this track on event\nstereo/active sensors fusion has remained unexplored so far.\nIn this paper, starting from the RGB literature [4,12,49,82], we embark on\na comprehensive investigation into the fusion of event-based stereo with sparse\ndepth hints from active sensors. Inspired by [4], which projects distinctive color\npatterns on the images consistently with measured depth, we design a halluci-\nnation mechanism to generate fictitious events over time to densify the stream\ncollected by the event cameras. Purposely, we propose two different strategies,\nrespectively consisting of i) creating distinctive patterns directly at the stack\nlevel, i.e. a Virtual Stack Hallucination (VSH), just before the deep net-\nwork processing, or ii) generating raw events directly in the stream, starting\nfrom the time instant tdfor which we aim to estimate a disparity map and\nperforming Back-in-Time Hallucination (BTH). Both strategies, despite the\ndifferent constraints \u2013 VSH requires explicit access to the stacked representation,\nwhereas BTH does not \u2013 dramatically improve the accuracy of pure event-basedLiDAR-Event Stereo Fusion 3\nstereo systems, overcoming some of their harshest limitations as shown in Fig.\n1 (c,d). Furthermore, despite depth sensors having a fixed acquisition rate that\nis in contrast with the asynchronous capture rate of event cameras, VSH and\nBTH can leverage depth measurements not synchronized with td(thus collected\nattz< td) with marginal drops in accuracy compared to the case of perfectly\nsynchronized depth and event sensors ( tz=td). This strategy allows for exploit-\ning both VSH and BTH while preserving the microsecond resolution peculiar of\nevent cameras. Exhaustive experiments support the following claims:\n\u2013We prove that LiDAR-stereo fusion frameworks can effectively be adapted\nto the event stereo domain\n\u2013Our VSH and BTH frameworks are general and work effectively with any\nstructured representation among the eight we surveyed\n\u2013Our strategies outperform existing alternatives inherited from RGB stereo\nliterature on DSEC [21] and M3ED [9] datasets\n\u2013VSH and BTH can exploit even outdated LiDAR data to increase the event\nstream distinctiveness and ease matching, preserving the microsecond reso-\nlution of event cameras and eliminating the need for synchronous processing\ndictated by the constant framerate of the depth sensor",
        "5 Experiments\n5.1 Implementation and Experimental Settings\nWe implement VSH and BTH in Python, using the Numba package.\nGeneral framework. We build our code base starting from SE-CFF [43] \u2013\nstate-of-the-art for event-based stereo \u2013 assuming the same stereo backbone as\nin their experiments, i.e. derived from AANet [72], and run SBN to generate the\nevent history to be stacked. While we select a single architecture, we implement\na variety of stacked representations: purposely, we implement a single instance\nof the stereo backbone for any stacked representations introduced in Sec. 3,\ntaking the opportunity to evaluate their performance with the event stereo task.\nFor Concentration representation, we use MDES as the prior stacking function\nfollowing [43] and avoid considering future events during training. Furthermore,\nin this case, VSH is applied before the concentration network since it would\ninterfere with gradient back-propagation during training \u2013 while this cannot\noccur with BTH. From [4], we adapt occlusion handling and hallucination on\nuniform/not uniform patches. We also implement alpha-blending, for VSH only\n\u2013 as it loses its purpose when acting on the raw streams. For all our methods,\nwe inherit the same hyper-parameters from [4]; yet, we discard occluded points\nas the occlusion handling strategy for BTH since an equivalent strategy to deal\nwith sparse event histories is not trivial. For VSH on Voxel Grids, we use the\n5-th and 95-th percentile to calculate S\u2212andS+due to the frequent presence of\nextreme values in the stack. For BTH, we perform 12 injections ( i.e.,B= 12).\nExisting fusion methodologies. We compare our proposal with existing\nmethods from the RGB stereo literature, consisting of i) modulating the cost\nvolume built by the backbone \u2013 Guided Stereo Matching [49], ii) concatenating\nthe sparse depth values to the inputs to the stereo network \u2013 e.g., as done\nby LidarStereoNet [12], iii) a combination of both the previous strategies \u2013 inLiDAR-Event Stereo Fusion 9\nDSEC [21] M3ED [9]\nFig.5: Qualitative comparison \u2013 DSEC vs M3ED. DSEC features 640\u00d7480\nevent cameras and a 16-line LiDAR, M3ED has 1280\u00d7720event cameras and a 64-line\nLiDAR. LiDAR scans have been dilated with a 7\u00d77kernel to ease visualization.\nanalogy to CCVNorm [69]. Any strategy is adapted to the same common stereo\nbackbone [43] (see supplementary material ). Running BTH and VSH adds\nrespectively 10ms and 2-15ms (depending on representations) on the CPU.\nTraining protocol. Any model we train \u2013 either the original event stereo\nbackbones or those implementing fusion strategies \u2013 runs for 25 epochs with a\nbatch size of 4 and a maximum disparity set to 192. We use Adam [29] with beta\n(0.9, 0.999) and weight decay set to 10\u22124. The learning rate starts at 5\u00b710\u22124\nand decays with cosine annealing. We apply random crops and vertical flips to\naugment data during training.\n5.2 Evaluation Datasets & Protocol\nWe introduce datasets and metrics used in our experiments.\nDSEC [21]. An outdoor event stereo dataset, captured using wide-baseline\n(50 cm) stereo event cameras at 640\u00d7480resolution. Ground-truth disparity\nis obtained by accumulating 16-line LiDAR scans, for a total of 26384 maps\norganized into 41 sequences. We split them into train/test sets following [43].\nFrom the training set, we retain a further searchsplit for hyper-parameters\ntuning and ablation experiments. Sparse LiDAR measurements are obtained by\naligning the raw scans with the ground-truth \u2013 both provided by the authors\n\u2013 by running a LiDAR inertial odometry pipeline followed by ICP registration\n(see thesupplementary material for details).\nM3ED [9]. This dataset provides 57 indoor/outdoor scenes collected with\na compact multi-sensor block mounted on three different vehicles \u2013 i.e., a car,\na UAV, and a quadruped robot. A 64-line LiDAR generates semi-dense ground-\ntruth depth, while the event stereo camera has a shorter baseline (12 cm) and\na higher resolution ( 1280\u00d7720). We use 5 sequences from this dataset for eval-\nuation purposes only \u2013 some of which contain several frames acquired with the\ncameras being static \u2013 to evaluate the generalization capacity of the models both\nto different domains and the density of the LiDAR sensor. Similarly to DSEC,\nwe derived sparse LiDAR depth maps from the raw scans. Thanks to the SDK\nmade available by the authors, we could derive LiDAR measurements aligned to\nany desired temporal offset according to linear interpolation of the ground-truth\nposes (see the supplementary material for details). This allows us to run ded-10 L. Bartolomei et al.\nFig.6: Hyperparameters search. Results on DSEC search split. On top, we study\nthe impact of (a) patch size, (b) uniform patches, and (c) alpha blending on VSH. At\nthe bottom, we consider (d) single vs repeated injection, (e) patch size, (f) uniform\npatches, (g) number of fictitious events, and (h) uniform polarities on BTH.\nicated experiments to assess the effect of time-misaligned depth measurements.\nFig. 5 shows a qualitative comparison between the two datasets.\nEvaluation Metrics. We compute the percentage of pixels with an error\ngreater than 1 or 2 pixels (1PE, 2PE), and the mean absolute error (MAE). We\nhighlight the bestandsecond best methods per row on each metric.\n5.3 Ablation Study\nWe ran hyper-parameters search and ablation experiments for VSH and BTH on\ntheDSECsearchsplit,reportingthe1PEerror.Weconductedtheseexperiments\nusing any representation listed in Sec. 3 \u2013 except for Concentration [43], which\nstarts from pre-computed MDES stacks \u2013 and report the average results.\nVSH.Fig. 6 (top) shows the impact of different hyper-parameters on VSH\nstrategy. In (a), we can observe how VSH is improved by using 3\u00d73patches,\nwhile 5\u00d75cannot yield further benefits. Consequently, we select it as the default\nconfiguration from now on. In (b), we show that uniform patterns are more\neffective than random ones, and in (c) alpha equal to 0.5 works the best.\nBTH.Fig. 6 (bottom) focuses on our second strategy. In (d) we show how\nrepeated injection can improve the results; thus, we select it as the default con-\nfiguration from now on. In the remainder, we will better appreciate how this\nsetting is much more robust when dealing with misaligned LiDAR data. Next,\n(e) outlines how hallucinating events with 3\u00d73patches lead to the best results.\nApplying a uniform patch of events following (f) yields, again, better results.\nIn (g), we tested different numbers K\u02c6x,\u02c6yof injected fictitious events. Injecting\nmore than one event is beneficial, yet saturating with two. Finally, (h) shows\nthat using uniform polarities yields lower errors.\n5.4 Experiments on DSEC\nWe now report experiments on the DSEC testing split, either when applying\nfusion strategies to pre-trained stereo models without retraining them or when\ntraining the networks from scratch to exploit LiDAR data.LiDAR-Event Stereo Fusion 11\nTable 1: Results on DSEC [21] \u2013 pre-trained. We test the different stacked repre-\nsentations (rows) with several fusion strategies applied to pre-trained stereo backbones.\nStacked Baseline Guided [49] VSH (ours) BTH (ours)\nrepresentation 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE\n(A) Histogram [39] 16.21 4.73 0.74 16.07 4.68 0.73 13.71 4.20 0.6913.32 3.92 0.66\n(B) MDES [43] 15.32 4.40 0.70 15.13 4.34 0.70 12.94 3.52 0.6312.61 3.50 0.62\n(C) Concentration [43] 15.97 4.33 0.70 15.79 4.27 0.70 13.70 3.60 0.6514.66 3.77 0.66\n(D) Voxelgrid [86] 16.49 4.56 0.72 16.29 4.50 0.71 13.12 3.69 0.6512.44 3.60 0.62\n(E) TORE [3] 15.91 4.57 0.71 15.72 4.50 0.71 12.53 3.65 0.6312.27 3.68 0.62\n(F) Time Surface [32] 15.33 4.29 0.70 15.18 4.24 0.69 12.16 3.38 0.6212.28 3.45 0.62\n(G) ERGO-12 [87] 15.02 4.20 0.68 14.87 4.14 0.68 12.02 3.40 0.6111.98 3.42 0.61\n(H) Tencode [26] 14.46 4.17 0.68 14.29 4.11 0.67 12.12 3.37 0.6111.86 3.45 0.61\nAvg. Rank. - 3.00 3.00 3.00 1.751.38 1.50 1.251.63 1.13\nTable 2: Results on DSEC [21] \u2013 retrained. We test different stacked represen-\ntations (rows) with several fusion strategies applied during training.\nConcat [12] Guided+Concat [69] Guided [49] VSH (ours) BTH (ours)\n1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE\n(A) 12.57 3.37 0.62 12.81 3.41 0.63 15.57 4.58 0.72 9.903.26 0.5310.91 3.41 0.59\n(B) 12.37 3.17 0.61 12.40 3.25 0.61 14.66 4.36 0.70 9.313.01 0.51 9.623.01 0.54\n(C) 12.38 3.41 0.63 12.74 3.44 0.66 15.15 4.45 0.71 9.703.04 0.53 9.662.98 0.55\n(D) 12.23 3.18 0.60 11.90 3.10 0.60 14.52 4.21 0.68 10.16 3.20 0.56 9.682.90 0.54\n(E) 12.99 3.33 0.62 12.62 3.25 0.61 16.00 4.56 0.73 9.913.05 0.53 9.832.98 0.54\n(F) 12.18 3.09 0.61 12.47 3.17 0.61 14.40 4.21 0.68 9.472.90 0.52 9.582.92 0.54\n(G) 12.43 3.14 0.61 12.82 3.19 0.62 13.85 3.97 0.66 9.252.88 0.50 9.372.87 0.54\n(H) 11.95 3.08 0.60 11.75 3.10 0.60 14.72 4.21 0.69 9.393.00 0.52 9.592.97 0.55\n3.38 3.00 3.13 3.63 3.50 3.38 5.00 5.00 5.00 1.381.88 1.13 1.631.38 1.88\nPre-trained models. Tab. 1 reports, on each row, the results yielded by\nusing a specific stacked representation. In the columns, we report the different\nfusion strategies involved in our experiments, starting with the baseline \u2013 i.e.,\na stereo backbone processing events only. In the last row, we report the average\nranking \u2013 for the three metrics \u2013 achieved by any fusion strategy over the eight\nrepresentations. Starting from baseline models, we can notice how the different\nrepresentations have an impact on the accuracy of the stereo backbone, with\nthose modeling complex behaviors \u2013 e.g., Time Surface [32] or ERGO-12 [87]\n\u2013 yielding up to 2% lower 1PE than simpler ones such as Histogram [39]. The\nGuided framework [49] can improve the results only moderately: this is caused\nby the very sparse measurements retrieved from the 16-line LiDAR sensor used\nin DSEC, as well as by the limited effect of the cost volume modulation in\nregions where events are not available for matching. Nonetheless, VSH and BTH\nconsistently outperform Guided, always improving the baseline by 2-3% points\non 1PE. In general, BTH achieves the best 1PE and MAE metrics in most cases;\nthis strategy is the best when re-training the stereo backbone is not feasible.\nTraining from scratch. Tab. 2 reports the results obtained by training the\nstereobackbonesfromscratchtoperformLiDAR-eventstereofusion.Thisallows\neitherthedeploymentofstrategiesthatprocesstheLiDARdatadirectlyasinput\n[12,69] or those not requiring it, i.e., [49] and ours. Specifically, Concat [12] and\nGuided+Concat [69] strategy achieve results comparable to those by VSH and\nBTH observed before, thus outperforming Guided [49] which, on the contrary,\ncannot benefit much from the training process. When deploying our solutions\nduring training, their effectiveness dramatically increases, often dropping 1PE12 L. Bartolomei et al.\nTable 3: Results on M3ED [9] \u2013 pre-trained. We test the different stacked repre-\nsentations (rows) with several fusion strategies applied to pre-trained stereo backbones.\nStacked Baseline Guided [49] VSH (ours) BTH (ours)\nrepresentation 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE\n(A) Histogram [39] 37.70 19.49 1.76 37.18 19.29 1.75 20.19 11.19 1.1922.32 12.37 1.27\n(B) MDES [43] 43.17 19.50 1.85 42.27 19.16 1.83 29.42 14.80 1.5222.58 12.20 1.30\n(C) Concentration [43] 45.78 20.84 1.82 45.06 20.57 1.80 33.63 16.19 1.5325.22 12.68 1.28\n(D) Voxelgrid [86] 37.33 17.66 1.70 36.64 17.38 1.68 20.40 11.41 1.2220.94 11.72 1.23\n(E) TORE [3] 41.70 19.09 1.81 41.00 18.78 1.80 28.25 14.01 1.4721.91 12.34 1.30\n(F) Time Surface [32] 38.58 18.52 1.72 37.91 18.23 1.70 24.89 13.34 1.3722.60 12.77 1.31\n(G) ERGO-12 [87] 36.33 17.81 1.66 35.61 17.50 1.64 22.53 12.33 1.2620.41 11.69 1.21\n(H) Tencode [26] 43.56 20.07 1.82 42.66 19.76 1.80 28.24 14.46 1.4322.61 12.75 1.26\nAvg. Rank. - 3.00 3.00 3.00 1.75 1.75 1.75 1.25 1.25 1.25\nTable 4: Results on M3ED [9] \u2013 retrained. We test different stacked representa-\ntions (rows) with several fusion strategies applied during training.\nConcat [12] Guided+Concat [69] Guided [49] VSH (ours) BTH (ours)\n1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE 1PE 2PE MAE\n(A) 34.67 15.21 1.92 38.65 17.00 1.94 37.45 18.98 1.76 19.34 12.93 1.4619.83 13.20 1.39\n(B) 37.72 16.91 1.85 37.32 17.16 2.14 37.00 18.66 1.76 19.24 13.17 1.4418.70 11.79 1.24\n(C) 39.88 19.01 2.33 38.45 17.76 2.47 38.14 19.62 1.8019.84 13.68 1.90 19.46 12.29 1.35\n(D) 33.89 16.21 1.89 33.54 15.85 1.75 37.85 18.81 1.7418.56 11.76 1.3221.02 14.30 1.80\n(E) 38.83 18.38 2.27 35.80 16.63 2.05 40.51 19.96 1.95 20.03 13.97 1.8620.04 12.65 1.39\n(F) 40.26 18.44 2.19 35.48 17.74 2.15 38.77 18.41 1.75 19.61 13.01 1.5521.91 14.33 1.72\n(G) 42.43 19.31 2.31 42.24 18.42 2.34 37.95 17.83 1.76 18.45 12.31 1.5519.12 11.60 1.24\n(H) 37.46 17.87 2.15 33.69 16.47 1.95 39.78 19.42 1.82 19.49 12.21 1.3819.28 11.68 1.33\n4.38 4.00 4.50 3.63 3.38 4.38 4.00 4.63 2.75 1.38 1.63 1.88 1.63 1.38 1.50\nerror below 10%. VSH often yields the best 1PE and MAE overall, nominating\nit as the most effective \u2013 yet intrusive \u2013 among our solutions.\n5.5 Experiments on M3ED\nWe test the effectiveness of BTH and alternative approaches on M3ED, using\nthe backbones trained on DSEC without any fine-tuning on M3ED itself.\nPre-trained models. Tab. 3 collects the outcome of this experiment by ap-\nplying Guided, VSH, and BTH to pre-trained models. Looking at the baselines,\nwe can appreciate how M3ED is very challenging for models trained in a different\ndomain, with 1PE errors higher than 30%. This is caused by both the domain\nshift and the higher resolution of the event cameras used. Even so, complex event\nrepresentations \u2013 e.g., ERGO-12 [87] \u2013 can better generalize. Guided confirms\nits limited impact, this time mainly because of the ineffectiveness of the cost\nvolume modulation in the absence of any information from the events domain.\nOn the contrary, we can appreciate even further the impact of VSH and BTH,\nalmost halving the 1PE error. Specifically, BTH is the absolute winner with 6\nout of 8 representations, and the best choice for pre-trained frameworks.\nTraining from scratch. Tab. 4 resumes the results obtained when train-\ning on DSEC the backbones implementing LiDAR-event stereo fusion strategies.\nThe very different distribution of depth points observed across the two datasets\n\u2013 sourced respectively from 16 and 64-line LiDARs \u2013 yields mixed results for\nexisting methods [12,49,69], with rare cases for which they fail to improve\nthe baseline model (e.g., Concat and Guided with Time Surface and ERGO-LiDAR-Event Stereo Fusion 13\nEvents & LiDAR Baseline Guided [49] BTH (ours) BTH (ours, retrain)\n1PE: 50.84%\n 1PE: 48.94%\n 1PE: 32.16%\n 1PE: 22.83%\nEvents & LiDAR Baseline Guided [49] VSH (ours) BTH (ours)\n1PE: 53.19%\n 1PE: 50.26%\n 1PE: 10.94%\n 1PE: 13.50%\nFig.7: Qualitative results. Results on DSEC zurich_10_b with Voxelgrid [86] (top)\nand M3ED spot_indoor_obstacles with Histogram [39] (bottom).\n12, Guided+Concat with Histogram). On the contrary, backbones trained with\nVSH and BTH consistently improve over the baseline, often with larger gains\ncompared to their use with pre-trained models. Overall, BTH is the best on\n2PE and MAE, confirming it is better suited for robustness across domains and\ndifferent LiDAR sensors.\nFig. 7 shows qualitative results. On DSEC (top), BTH dramatically improves\nresults over the baseline and Guided, yet cannot fully recover some details in the\nscene except when retraining the stereo backbone. On M3ED (bottom), both\nVSH and BTH with pre-trained models reduce the error by 5 \u00d7.\n5.6 Experiments on M3ED \u2013 Time-misaligned LiDAR\nWe conclude by assessing the robustness of the considered strategies against the\nuse of LiDAR not synchronized with the timestamp at which we wish to estimate\ndisparity \u2013 occurring if we wish to maintain the microsecond resolution of the\nevent cameras. Purposely, we extract raw LiDAR measurements collected 3, 13,\n32, 61, and 100 ms in the past with the M3ED SDK.\nFig. 8 shows the trend of the 1PE metric achieved by Guided (red), VSH\n(yellow) and BTH (black and green) on pre-trained backbones. Not surprisingly,\nthe error rates arise at the increase of the temporal distance: while this is less\nevident with Guided because of its limited impact, this becomes clear with VSH\nandBTH.Nonetheless,bothcanalwaysretainasignificantgainoverthebaseline\nmodel (blue) \u2013 i.e., the stereo backbone processing events only \u2013 even with the\nfarthest possible misalignment with a 10Hz LiDAR (100ms). We can appreciate\nhow BTH is often better than VSH (coherently with Tab. 3), yet only when\nrepeatedinjectionsareperformed(green).Indeed,usingasingleinjection(black)14 L. Bartolomei et al.\nFig.8: Experiments with time-misaligned LiDAR on M3ED [9] \u2013 pre-\ntrained. We measure the robustness of different fusion strategies against the use of\nout-of-sync LiDAR data, without retraining the stereo backbone.\nFig.9: Experiments with time-misaligned LiDAR on M3ED [9] \u2013 retrained.\nWe measure the robustness of different fusion strategies against the use of out-of-sync\nLiDAR data when training the stereo backbone from scratch.\nrapidlyleadsBTHtoanaccuracydropwhenincreasingthemisalignment,except\nwhen using Histogram representation. Overall, BTH with ERGO-12 is the most\nrobust solution. Fig. 9 shows the results achieved by VSH (yellow) and BTH\n(green) after retraining, against the best competitor according to average ranks\nin Tab. 4 \u2013 i.e., Guided+Concat (red). The impact of this latter is limited and\nsometimes fails to improve the baseline (see Histogram and ERGO-12). On the\ncontrary, our solutions confirm their robustness and effectiveness even when\ndealing with time-misaligned LiDAR data."
    ],
    "draft_summary": "This paper introduces Arctic-TILT, a novel method for synthesizing data using an external data corpus to improve the diversity of the training data. The method is built upon the TILT encoder-decoder model and extends its sequential positional bias with an attention bias based on relative horizontal and vertical distances between each pair of tokens. Additionally, contextualized image embeddings are added to cover the token image region semantics in the context of its entire visual neighborhood. The model uses a U-Net network as an image encoder and a Multi-Head Attention mechanism to combine visual and textual features. The authors also propose a novel modality fusion mechanism inspired by Tensor Product representations, which is used to fuse text and vision modalities.",
    "date": null,
    "title": null,
    "beginner_result": "Arctic-TILT is a new way to create diverse training data for computers by combining text and images. It uses a special model that looks at how words and images are related to each other in a scene, and combines them in a way that helps the computer understand the context better. This method can help computers learn more accurately and make better decisions.",
    "intermediate_result": "Arctic-TILT is a new method for generating diverse training data by leveraging external data sources. It builds on the TILT model, adding attention-based biases to account for spatial relationships between tokens and incorporating contextual image embeddings to capture visual semantics. The model combines visual and textual features using a U-Net network and Multi-Head Attention, and introduces a novel modality fusion mechanism inspired by Tensor Product representations to integrate text and vision modalities.",
    "advanced_result": "This paper introduces Arctic-TILT, a novel method for synthesizing data using an external data corpus to improve the diversity of the training data. The method is built upon the TILT encoder-decoder model and extends its sequential positional bias with an attention bias based on relative horizontal and vertical distances between each pair of tokens. Additionally, contextualized image embeddings are added to cover the token image region semantics in the context of its entire visual neighborhood. The model uses a U-Net network as an image encoder and a Multi-Head Attention mechanism to combine visual and textual features. The authors also propose a novel modality fusion mechanism inspired by Tensor Product representations, which is used to fuse text and vision modalities.",
    "image_path": null,
    "link": null,
    "keep_refining": false,
    "entities": [
        "TILT encoder-decoder",
        "Arctic-TILT",
        "U-Net network",
        "U-Net network",
        "Tensor Product",
        "Multi-Head Attention"
    ],
    "draft_version": 2
}