5 Experiments
The final model, Puppet-Master, is trained on a combined dataset of Drag-a-Move [ 4] and Objaverse-
Animation-HQ (Section 4). We evaluate the performance of the final checkpoint on multiple bench-
marks, including the test split of Drag-a-Move and real-world cases from Human3.6M [ 66], Amazon-
Berkeley Objects [ 67], Fauna Dataset [ 68,69], and CC-licensed web images in a zero-shot manner,
demonstrating qualitative and quantitative improvements over prior works and excellent general-
ization to real cases (Section 5.1). The design choices that led to Puppet-Master are ablated and
discussed further in Section 5.2. In Section 5.3, we show the effectiveness of our data curation
strategy (Section 4). We refer the reader to Appendix C for the implementation details.
5.1 Main Results
Quantitative comparison. We compare Puppet-Master to DragNUWA [ 9] and DragAnything [ 16],
both of which are trained on real-world videos to support open-domain motion control, on the
part-level motion-conditioned video synthesis task in Table 1. On the in-domain test set ( i.e., Drag-
a-Move), Puppet-Master outperforms both methods on all standard metrics, including pixel-level
PSNR, patch-level SSIM, and feature-level LPIPS and FVD, by a significant margin.
Additionally, to better demonstrate our model’s superiority in generating part-level object dynamics,
we introduce a flow-based metric dubbed flow error . Specifically, we first track points on the object
throughout the generated and ground-truth videos using CoTracker [ 70], and then compute flow error
7Table 1: Comparisons with DragNUWA [ 9], DragAnything [ 16] and DragAPart [ 4] on the in-domain
Drag-a-Move and out-of-domain Human3.6M datasets. The best method is bolded and second best
underlined. Our model has notbeen trained on the Human3.6M dataset, or any real video datasets.
MethodDrag-a-Move [4] Human3.6M [66]
PSNR↑SSIM↑LPIPS↓FVD↓flow error ↓PSNR↑SSIM↑LPIPS↓FVD↓
DragNUWA 20.09 0.874 0.172 281.49 17.55 / 15.41 17.52 0.878 0.158 466.91
DragAnything 16.71 0.799 0.296 468.46 16.09 / 23.21 13.29 0.767 0.305 768.63
DragAPart
—Original 23.41 0.925 0.085 180.27 14.17 / 3.71 15.14 0.852 0.197 683.40
—Re-Trained 23.78 0.927 0.082 189.10 14.34 / 3.73 15.25 0.860 0.188 549.64
Puppet-Master 24.41 0.927 0.085 246.99 12.21 /3.53 17.59 0.872 0.155 454.76
DragAPartOurs: Puppet-Master
(a)(b)
Figure 4: Qualitative Comparison with DragAPart [ 4]. The videos generated by DragAPart lack
temporal consistency: (a) the door initially opens to the left, but later it is switched to open to the
right, and it partially closes between the second and third frames visualized here; (b) DragAPart fails
to generalize to out-of-domain cases, resulting in distorted motion.
as the root mean square error (RMSE) between the two trajectories. We report two RMSEs in Table 1
for this flow error metric. The former value ( i.e., before the slash) is averaged among the origins
of all conditioning drags only, i.e.,{uk}K
k=1, while the latter value ( i.e., after the slash) is averaged
among all foreground points. While Puppet-Master has lower values on both, it obtains a significantly
smaller value when the error is averaged among all foreground points. This indicates Puppet-Master
can better model nuanced part-level dynamics, thanks to which the parts that do not necessarily
move along with the dragged parts stay static in the generated videos, reducing the overall error. By
contrast, DragNUWA and DragAnything move the whole object, so every point incurs a large error.
To assess the cross-domain generalizability, we directly evaluate Puppet-Master on an unseen dataset
captured in the real world ( i.e., Human3.6M). On this out-of-domain test set, Puppet-Master outper-
forms prior models on most metrics, despite not being fine-tuned on any real videos.
For completeness, we also include the metrics of DragAPart [ 4], a drag-conditioned image generator.
The original DragAPart was trained on Drag-a-Move only. For fairness, we fine-tune it from Stable
Diffusion [ 27] with the identical data setting as Puppet-Master, and evaluate the performance of both
checkpoints ( Original2andRe-Trained in Table 1). The videos are obtained from Nindependently
generated frames conditioned on gradually extending drags. While its samples exhibit high visual
quality in individual frames, they lack temporal smoothness, characterized by abrupt transitions and
discontinuities in movement, resulting in a larger flow error3(Fig. 4a). This justifies starting from
a video generator to improve temporal consistency. Furthermore, DragAPart fails to generalize to
out-of-domain cases ( e.g., Fig. 4b and Table 1).
2Original is not ranked as it is trained on single-category data only and hence not an open-domain generator.
3FVD is not an informative metric for motion quality. Prior works [ 71,72] have noted that FVD is biased
towards the quality of individual frames and does notsufficiently account for motion in generated videos. Good
FVD scores can still be obtained with static videos or videos with severe temporal corruption.
8Figure 5: Qualitative Results onreal-world cases spanning diverse categories.
Qualitative comparison. We show samples generated by Puppet-Master and prior models side by
side in Fig. 1. The dynamics generated by Puppet-Master are physically plausible and faithful to the
input image and drags. By contrast, the videos generated by DragNUWA [ 9] and DragAnything [ 16]
scale (d, e, f) or shift (b) the object as a whole at best, or even show distorted motion (a, c). Even
though Puppet-Master is fine-tuned solely on renderings of synthetic 3D models, it does generalize to
real cases, and is capable of preserving fine-grained texture details.
Qualitative results on real data. In Fig. 5, we show more real examples generated by Puppet-
Master. The synthesized videos exhibit realistic dynamics that are typical of the underlying categories,
including humans, animals, and several man-made categories.
Table 2: Ablation studies of various model components. In addition to the standard metrics, we
report a flow-based metric dubbed flow error . A lower flow error indicates the generated videos
follow the drag control better. We also manually count the frequency of generated videos whose
motion directions are opposite to the intention of their drag inputs. Here, ≥indicates there are video
samples whose motion directions are hard to distinguish. When ablating attention with the reference
image, we use Cas the base drag conditioning architecture.
Setting PSNR ↑SSIM↑LPIPS↓FVD↓flow error ↓% wrong dir. ↓
Drag conditioning
A Shift only w/o end loc. 13.23 0.816 0.446 975.16 15.60 px ≥5
B Shift+scale w/o end loc. 22.98 0.917 0.093 223.20 9.33 px 4
C Shift+scale w/ end loc. 23.67 0.926 0.080 205.40 10.48 px 4
D C + x-attn. w/ drag tok. 24.00 0.929 0.069 170.43 9.80 px 1
Attn. w/ ref. image
No attn. 11.96 0.771 0.391 823.00 12.35 px ≥3
Attn. w/ static ref. video 17.51 0.874 0.233 483.18 13.57 px ≥8
All-to-first attn. 23.67 0.926 0.080 205.40 10.48 px 4
5.2 Ablations
We conduct several ablation studies to analyze the introduced components of Puppet-Master. For
each design choice, we train a model using the training split of the Drag-a-Move [ 4] dataset with
batch size 8for30,000iterations and evaluate on 100videos from its test split without classifier-free
guidance [73]. Results are shown in Table 2 and Fig. 6 and discussed in detail next.
9No drag tok. x-attn.(No attn.)No drag tok. x-attn.(Attn. w/ static ref. vid.)No drag tok. x-attn.(All-to-first attn.)
Full modelFigure 6: Visualization of samples generated by different model designs, where we show the last
frame and the first 3frames. While all designs produce nearly perfect first frames, our proposed
all-to-first attention module significantly enhances sample quality. Without this module, the generated
samples often exhibit sub-optimal appearances and backgrounds. The cross-attention module with
drag tokens further improves the appearance details.
Drag conditioning. Table 2 compares Puppet-Master with multiple variants of conditioning mecha-
nisms (Section 3.2). Adaptive normalization layers ( Avs.B), drag encoding with final termination
location vN
k(Bvs.C), and cross attention with drag tokens ( Cvs.D) are all beneficial. Notably,
by combining these ( i.e., rowD), the model achieves a negligible rate of generated samples with
incorrect motion directions (see Table 2 caption for details).
Attention with the reference image. We find that all-to-first attention (Section 3.3) is essential for
high generation quality. We also compare all-to-first attention with an alternative implementation
strategy inspired by the X-UNet design in 3DiM [ 61], where we pass a static video consisting of
the reference image copied Ntimes to the same network architecture and implement cross attention
between the clean (static) reference video branch and the noised video branch. The latter strategy
performs worse. We hypothesize that this is due to the distribution drift between the two branches,
which forces the optimization to modify the pre-trained SVD’s internal representations too much.
Figure 7: Data curation helps stabilize training.Setting PSNR ↑ SSIM↑
w/o Data Curation 6.04 0.411
w/ Data Curation 19.87 0.884
Setting LPIPS ↓ FVD↓
w/o Data Curation 0.703 1475.35
w/ Data Curation 0.181 624.47
Table 3: Training on more abundant but lower-
quality data leads to lower generation quality. Here,
‘w/o Data Curation’ model is trained on Objaverse-
Animation while ‘w/ Data Curation’ model is
trained on Objaverse-Animation-HQ. Both models
are trained for 7K iterations. Evaluation is per-
formed on the test split of Drag-a-Move [4].
105.3 Less is More: Data Curation Helps at Scale
To verify that our data curation strategy from Section 4 is effective, we compare two models trained
on Objaverse-Animation and Objaverse-Animation-HQ respectively under the same hyper-parameter
setting. The training dynamics are visualized in Fig. 7. The optimization collapses towards 7k
iterations when the model is trained on a less curated dataset, resulting in much lower-quality video
samples (Table 3). This suggests that the data’s quality matters more than quantity at scale.